{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c6e7fc",
   "metadata": {},
   "source": [
    "# ğŸ¯ MAP Competition: Gemma-2-2b-it Submission Notebook\n",
    "\n",
    "## Overview\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€äº‹å‰ã«è¨“ç·´ãƒ»ä¿å­˜ã•ã‚ŒãŸGemma-2-2b-itãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€MAP - Charting Student Math Misunderstandingsã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®æå‡ºã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "### Model Details\n",
    "- **Pre-trained Model**: google/gemma-2-2b-it\n",
    "- **Parameters**: ~2.6B\n",
    "- **Task**: 6-class text classification\n",
    "- **Evaluation Metric**: MAP@3\n",
    "\n",
    "### Target Classes (6åˆ†é¡)\n",
    "- **True_Correct**: æ­£è§£ã§æ­£ã—ã„èª¬æ˜\n",
    "- **True_Neither**: æ­£è§£ã ãŒæ›–æ˜§ãªèª¬æ˜\n",
    "- **True_Misconception**: æ­£è§£ã ãŒèª¤ã£ãŸæ¦‚å¿µã®èª¬æ˜\n",
    "- **False_Correct**: ä¸æ­£è§£ã ãŒæ­£ã—ã„æ¦‚å¿µã®èª¬æ˜\n",
    "- **False_Neither**: ä¸æ­£è§£ã§æ›–æ˜§ãªèª¬æ˜\n",
    "- **False_Misconception**: ä¸æ­£è§£ã§èª¤ã£ãŸæ¦‚å¿µã®èª¬æ˜\n",
    "\n",
    "### Strategy\n",
    "1. Kaggleã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸäº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
    "2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†\n",
    "3. æ¨è«–å®Ÿè¡Œã§MAP@3å½¢å¼ã®äºˆæ¸¬ç”Ÿæˆ\n",
    "4. submission.csvä½œæˆãƒ»æå‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27de544",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c331fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggleç’°å¢ƒã§å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹é–¢æ•°\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆKaggleã§é€šå¸¸ä¸è¶³ã™ã‚‹ã‚‚ã®ï¼‰\n",
    "required_packages = [\n",
    "    \"transformers>=4.35.0\",\n",
    "    \"accelerate>=0.26.0\", \n",
    "    \"sentencepiece>=0.1.99\"\n",
    "]\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«çŠ¶æ³ç¢ºèª\n",
    "        if \"transformers\" in package:\n",
    "            import transformers\n",
    "            print(f\"âœ… transformers already installed: {transformers.__version__}\")\n",
    "        elif \"accelerate\" in package:\n",
    "            import accelerate\n",
    "            print(f\"âœ… accelerate already installed: {accelerate.__version__}\")\n",
    "        elif \"sentencepiece\" in package:\n",
    "            import sentencepiece\n",
    "            print(f\"âœ… sentencepiece already installed: {sentencepiece.__version__}\")\n",
    "    except ImportError:\n",
    "        print(f\"ğŸ“¦ Installing {package}...\")\n",
    "        install_package(package)\n",
    "\n",
    "print(\"ğŸ‰ All required libraries are ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a165f27d",
   "metadata": {},
   "source": [
    "## ğŸ“š Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997bfcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "# è­¦å‘Šã‚’éè¡¨ç¤º\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º\n",
    "print(\"ğŸ”§ Environment Information:\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Kaggleã®ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹è¨­å®š\n",
    "KAGGLE_INPUT_PATH = \"/kaggle/input\"\n",
    "if os.path.exists(KAGGLE_INPUT_PATH):\n",
    "    print(f\"ğŸ“ Kaggle environment detected: {KAGGLE_INPUT_PATH}\")\n",
    "    # ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹\n",
    "    COMP_DATA_PATH = \"/kaggle/input/map-charting-student-math-misunderstandings\"\n",
    "    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã«å¤‰æ›´ã—ã¦ãã ã•ã„ï¼‰\n",
    "    MODEL_DATA_PATH = \"/kaggle/input/gemma-2-2b-math-misconception-model\"\n",
    "    print(f\"ğŸ¯ Competition data path: {COMP_DATA_PATH}\")\n",
    "    print(f\"ğŸ¤– Model data path: {MODEL_DATA_PATH}\")\n",
    "else:\n",
    "    print(\"ğŸ“ Local environment detected\")\n",
    "    COMP_DATA_PATH = \"./map_data\"\n",
    "    MODEL_DATA_PATH = \"./saved_models/gemma-2-2b-math-misconception\"\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c08eb0",
   "metadata": {},
   "source": [
    "## ğŸ”§ Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathMisconceptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Math Misconception Dataset for PyTorch\n",
    "    æ¨è«–å°‚ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list): ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ\n",
    "            tokenizer: Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
    "            max_length (int): æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "\n",
    "        # Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "        }\n",
    "\n",
    "print(\"âœ… MathMisconceptionDataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7223a",
   "metadata": {},
   "source": [
    "## ğŸ¤– Load Pre-trained Gemma Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9979e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_gemma_model():\n",
    "    \"\"\"äº‹å‰è¨“ç·´æ¸ˆã¿Gemmaãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ¤– äº‹å‰è¨“ç·´æ¸ˆã¿Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ç¢ºèª\n",
    "        print(f\"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {MODEL_DATA_PATH}\")\n",
    "        \n",
    "        if not os.path.exists(MODEL_DATA_PATH):\n",
    "            print(f\"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {MODEL_DATA_PATH}\")\n",
    "            print(\"ğŸ’¡ Kaggleç’°å¢ƒã§ã¯æ­£ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’ç¢ºèªã—ã¦ãã ã•ã„\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã®èª­ã¿è¾¼ã¿\n",
    "        label_file = os.path.join(MODEL_DATA_PATH, \"label_mapping.json\")\n",
    "        print(f\"ğŸ“‹ ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿: {label_file}\")\n",
    "        \n",
    "        with open(label_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            label_mapping = json.load(f)\n",
    "        \n",
    "        print(\"âœ… ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿æˆåŠŸ\")\n",
    "        for idx, label in label_mapping.items():\n",
    "            print(f\"   {idx}: {label}\")\n",
    "        \n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\n",
    "        print(\"\\nğŸ“ Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_DATA_PATH)\n",
    "        print(f\"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿æˆåŠŸ\")\n",
    "        print(f\"ğŸ”– ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³: {tokenizer.pad_token}\")\n",
    "        print(f\"ğŸ“ èªå½™ã‚µã‚¤ã‚º: {tokenizer.vocab_size:,}\")\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "        print(\"\\nğŸ§  Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_DATA_PATH,\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\" if device.type == \"cuda\" else None,\n",
    "        )\n",
    "        \n",
    "        # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\n",
    "        if device.type == \"cpu\":\n",
    "            model = model.to(device)\n",
    "        \n",
    "        print(f\"âœ… Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†!\")\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"ğŸ“Š åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°: {model.config.num_labels}\")\n",
    "        print(f\"ğŸ“ˆ ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}\")\n",
    "        print(f\"ğŸ’¡ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: ~{total_params / 1e9:.2f}B parameters\")\n",
    "        \n",
    "        # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\n",
    "        model.eval()\n",
    "        \n",
    "        return model, tokenizer, label_mapping\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        print(\"\\nğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:\")\n",
    "        print(\"1. Kaggleã§ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒæ­£ã—ãã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\")\n",
    "        print(\"2. MODEL_DATA_PATHãŒæ­£ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’æŒ‡ã—ã¦ã„ã‚‹ã‹ç¢ºèª\")\n",
    "        print(\"3. å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒå…¨ã¦å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\")\n",
    "        raise e\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Ÿè¡Œ\n",
    "model, tokenizer, label_mapping = load_pretrained_gemma_model()\n",
    "print(\"\\nğŸ‰ äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æº–å‚™å®Œäº†!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0449de47",
   "metadata": {},
   "source": [
    "## ğŸ“Š Load and Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf08611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_test_data():\n",
    "    \"\"\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "        test_path = os.path.join(COMP_DATA_PATH, \"test.csv\")\n",
    "        print(f\"ğŸ“ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹: {test_path}\")\n",
    "        \n",
    "        if not os.path.exists(test_path):\n",
    "            print(f\"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {test_path}\")\n",
    "            return None\n",
    "        \n",
    "        test_df = pd.read_csv(test_path)\n",
    "        print(f\"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ!\")\n",
    "        print(f\"ğŸ“ˆ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test_df.shape}\")\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ç¢ºèª\n",
    "        print(\"\\nğŸ“‹ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ—:\")\n",
    "        print(test_df.columns.tolist())\n",
    "        \n",
    "        print(\"\\nğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:\")\n",
    "        print(test_df.head(3))\n",
    "        \n",
    "        # å¼·åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã®ä½œæˆ\n",
    "        def create_enhanced_text(row):\n",
    "            \"\"\"Question + MC_Answer + Explanation ã‚’çµåˆã—ãŸå¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆ\"\"\"\n",
    "            question = str(row.get(\"QuestionText\", \"\")) if pd.notna(row.get(\"QuestionText\")) else \"\"\n",
    "            mc_answer = str(row.get(\"MC_Answer\", \"\")) if pd.notna(row.get(\"MC_Answer\")) else \"\"\n",
    "            explanation = str(row.get(\"StudentExplanation\", \"\")) if pd.notna(row.get(\"StudentExplanation\")) else \"\"\n",
    "            \n",
    "            # Gemmaç”¨ã®æ§‹é€ åŒ–ãƒ†ã‚­ã‚¹ãƒˆ\n",
    "            enhanced_text = f\"Question: {question} Selected Answer: {mc_answer} Explanation: {explanation}\"\n",
    "            return enhanced_text\n",
    "\n",
    "        print(\"\\nğŸ”§ å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆä¸­...\")\n",
    "        test_df[\"enhanced_text\"] = test_df.apply(create_enhanced_text, axis=1)\n",
    "        \n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆé•·ã®çµ±è¨ˆ\n",
    "        text_lengths = test_df[\"enhanced_text\"].str.len()\n",
    "        print(f\"\\nğŸ“Š ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ:\")\n",
    "        print(f\"   å¹³å‡: {text_lengths.mean():.0f} æ–‡å­—\")\n",
    "        print(f\"   æœ€å°: {text_lengths.min()} æ–‡å­—\")\n",
    "        print(f\"   æœ€å¤§: {text_lengths.max()} æ–‡å­—\")\n",
    "        print(f\"   ä¸­å¤®å€¤: {text_lengths.median():.0f} æ–‡å­—\")\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¤º\n",
    "        print(f\"\\nğŸ“ å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«:\")\n",
    "        sample_text = test_df[\"enhanced_text\"].iloc[0]\n",
    "        print(f\"Length: {len(sample_text)} characters\")\n",
    "        print(f\"Sample: {sample_text[:300]}...\")\n",
    "        \n",
    "        return test_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Ÿè¡Œ\n",
    "test_df = load_and_prepare_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814efd5e",
   "metadata": {},
   "source": [
    "## ğŸ”® Generate Predictions for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f83497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_predictions(model, tokenizer, test_df, label_mapping, batch_size=8):\n",
    "    \"\"\"ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹äºˆæ¸¬ç”Ÿæˆï¼ˆMAP@3å½¢å¼ï¼‰\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ”® ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆäºˆæ¸¬ç”Ÿæˆï¼ˆMAP@3ï¼‰\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if test_df is None or len(test_df) == 0:\n",
    "        print(\"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_df):,}ä»¶\")\n",
    "    print(f\"ğŸ”§ ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}\")\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\n",
    "    test_texts = test_df[\"enhanced_text\"].tolist()\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
    "    print(\"ğŸ”§ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­...\")\n",
    "    test_dataset = MathMisconceptionDataset(\n",
    "        test_texts, tokenizer, max_length=512\n",
    "    )\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ: {len(test_dataloader)}ãƒãƒƒãƒ\")\n",
    "    \n",
    "    # äºˆæ¸¬å®Ÿè¡Œ\n",
    "    print(\"ğŸ”® äºˆæ¸¬å®Ÿè¡Œä¸­...\")\n",
    "    all_predictions = []\n",
    "    \n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(test_dataloader):\n",
    "                # ãƒãƒƒãƒã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                # æ¨è«–å®Ÿè¡Œ\n",
    "                outputs = model(**batch)\n",
    "                predictions = outputs.logits\n",
    "                \n",
    "                # CPU ã«ç§»å‹•ã—ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "                batch_predictions = predictions.cpu().numpy()\n",
    "                all_predictions.append(batch_predictions)\n",
    "                \n",
    "                # é€²æ—è¡¨ç¤º\n",
    "                if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(test_dataloader):\n",
    "                    processed = min((batch_idx + 1) * batch_size, len(test_df))\n",
    "                    print(f\"   é€²æ—: {processed:,}/{len(test_df):,} ({processed/len(test_df)*100:.1f}%)\")\n",
    "        \n",
    "        print(\"âœ… äºˆæ¸¬å®Œäº†!\")\n",
    "        \n",
    "        # äºˆæ¸¬çµæœã‚’çµåˆ\n",
    "        all_predictions = np.vstack(all_predictions)\n",
    "        print(f\"ğŸ“Š äºˆæ¸¬çµæœå½¢çŠ¶: {all_predictions.shape}\")\n",
    "        \n",
    "        # ç¢ºç‡ã«å¤‰æ›\n",
    "        probs = torch.softmax(torch.tensor(all_predictions), dim=-1).numpy()\n",
    "        \n",
    "        # å„ã‚µãƒ³ãƒ—ãƒ«ã§ä¸Šä½3ã¤ã®äºˆæ¸¬ã‚’å–å¾—\n",
    "        print(\"ğŸ¯ TOP-3äºˆæ¸¬æŠ½å‡ºä¸­...\")\n",
    "        submission_predictions = []\n",
    "        \n",
    "        # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã®é€†å¤‰æ›ç”¨\n",
    "        idx_to_label = {int(k): v for k, v in label_mapping.items()}\n",
    "        \n",
    "        for i, prob in enumerate(probs):\n",
    "            # ç¢ºç‡ã®é«˜ã„é †ã«ä¸Šä½3ã¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
    "            top3_indices = np.argsort(prob)[::-1][:3]\n",
    "            \n",
    "            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«åã«å¤‰æ›\n",
    "            top3_labels = [idx_to_label[idx] for idx in top3_indices]\n",
    "            \n",
    "            # ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§çµåˆï¼ˆã‚³ãƒ³ãƒšè¦æ±‚å½¢å¼ï¼‰\n",
    "            prediction_string = \" \".join(top3_labels)\n",
    "            submission_predictions.append(prediction_string)\n",
    "            \n",
    "            # é€²æ—è¡¨ç¤ºï¼ˆæœ€åˆã®5ä»¶ï¼‰\n",
    "            if i < 5:\n",
    "                top3_probs = [prob[idx] for idx in top3_indices]\n",
    "                print(f\"  ã‚µãƒ³ãƒ—ãƒ« {i+1}: {prediction_string}\")\n",
    "                print(f\"    ç¢ºç‡: {[f'{p:.3f}' for p in top3_probs]}\")\n",
    "        \n",
    "        print(f\"âœ… TOP-3äºˆæ¸¬æŠ½å‡ºå®Œäº†: {len(submission_predictions)}ä»¶\")\n",
    "        \n",
    "        # äºˆæ¸¬ã®çµ±è¨ˆæƒ…å ±\n",
    "        all_pred_labels = \" \".join(submission_predictions).split()\n",
    "        from collections import Counter\n",
    "        pred_counts = Counter(all_pred_labels)\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ äºˆæ¸¬çµ±è¨ˆ:\")\n",
    "        print(f\"  äºˆæ¸¬ã«ä½¿ç”¨ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªæ•°: {len(pred_counts)}\")\n",
    "        for category, count in pred_counts.most_common():\n",
    "            percentage = count / (len(submission_predictions) * 3) * 100\n",
    "            print(f\"    {category}: {count}å› ({percentage:.1f}%)\")\n",
    "        \n",
    "        return submission_predictions\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ äºˆæ¸¬å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"ğŸ–¥ï¸ ç¾åœ¨ã®GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "        raise e\n",
    "\n",
    "# äºˆæ¸¬å®Ÿè¡Œ\n",
    "if model is not None and tokenizer is not None and test_df is not None:\n",
    "    print(\"ğŸ”® ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆäºˆæ¸¬ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ç’°å¢ƒã«å¿œã˜ã¦èª¿æ•´\n",
    "    batch_size = 4 if device.type == \"cpu\" else 8\n",
    "    test_predictions = generate_test_predictions(model, tokenizer, test_df, label_mapping, batch_size)\n",
    "    print(\"ğŸ‰ ãƒ†ã‚¹ãƒˆäºˆæ¸¬å®Œäº†!\")\n",
    "else:\n",
    "    print(\"âŒ å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    test_predictions = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ccd930",
   "metadata": {},
   "source": [
    "## ğŸ“¤ Create Submission File\n",
    "\n",
    "æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«æå‡ºã™ã‚‹ãŸã‚ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5cb096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_file(test_df, predictions, output_path=\"submission.csv\"):\n",
    "    \"\"\"æå‡ºç”¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ“¤ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if test_df is None or predictions is None:\n",
    "        print(\"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯äºˆæ¸¬çµæœãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ“Š æå‡ºãƒ‡ãƒ¼ã‚¿: {len(predictions):,}ä»¶\")\n",
    "    \n",
    "    # æå‡ºç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ\n",
    "    # test_dfã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¾ãŸã¯é©åˆ‡ãªIDåˆ—ã‚’ä½¿ç”¨\n",
    "    if 'QuestionId_Answer' in test_df.columns:\n",
    "        submission_df = pd.DataFrame({\n",
    "            'QuestionId_Answer': test_df['QuestionId_Answer'],\n",
    "            'Correct': predictions\n",
    "        })\n",
    "    else:\n",
    "        # IDåˆ—ãŒãªã„å ´åˆã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨\n",
    "        submission_df = pd.DataFrame({\n",
    "            'QuestionId_Answer': test_df.index,\n",
    "            'Correct': predictions\n",
    "        })\n",
    "    \n",
    "    print(f\"âœ… æå‡ºãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆå®Œäº†: {submission_df.shape}\")\n",
    "    print(f\"ğŸ“ åˆ—: {list(submission_df.columns)}\")\n",
    "    \n",
    "    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\n",
    "    print(f\"\\nğŸ“‹ æå‡ºãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:\")\n",
    "    print(submission_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜\n",
    "    try:\n",
    "        submission_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nğŸ’¾ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {output_path}\")\n",
    "        \n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª\n",
    "        file_size = os.path.getsize(output_path)\n",
    "        print(f\"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes ({file_size/1024:.1f} KB)\")\n",
    "        \n",
    "        # å½¢å¼ç¢ºèª\n",
    "        check_df = pd.read_csv(output_path)\n",
    "        print(f\"âœ… æå‡ºãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼: {check_df.shape}\")\n",
    "        required_cols = ['QuestionId_Answer', 'Correct']\n",
    "        cols_present = all(col in check_df.columns for col in required_cols)\n",
    "        print(f\"   å¿…è¦åˆ—å­˜åœ¨ç¢ºèª: {cols_present}\")\n",
    "        \n",
    "        # äºˆæ¸¬å½¢å¼ãƒã‚§ãƒƒã‚¯\n",
    "        sample_predictions = check_df['Correct'].head(5).tolist()\n",
    "        print(f\"ğŸ” äºˆæ¸¬å½¢å¼ã‚µãƒ³ãƒ—ãƒ«:\")\n",
    "        for i, pred in enumerate(sample_predictions):\n",
    "            pred_parts = pred.split()\n",
    "            print(f\"   {i+1}: {pred} (è¦ç´ æ•°: {len(pred_parts)})\")\n",
    "            \n",
    "        print(f\"\\nğŸ“Š æå‡ºãƒ•ã‚¡ã‚¤ãƒ«æœ€çµ‚ç¢ºèª:\")\n",
    "        print(f\"   âœ… å½¢å¼: MAP@3ï¼ˆå„è¡Œã«3ã¤ã®äºˆæ¸¬ã‚’ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šï¼‰\")\n",
    "        print(f\"   âœ… ä»¶æ•°: {len(check_df):,}ä»¶\")\n",
    "        print(f\"   âœ… åˆ—å: {list(check_df.columns)}\")\n",
    "        \n",
    "        return submission_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None\n",
    "\n",
    "# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\n",
    "if test_predictions is not None and test_df is not None:\n",
    "    print(\"ğŸ“¤ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™...\")\n",
    "    submission_df = create_submission_file(test_df, test_predictions, \"Gemma_2b_submission.csv\")\n",
    "    \n",
    "    if submission_df is not None:\n",
    "        print(\"ğŸ‰ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†!\")\n",
    "        print(f\"ğŸ“‹ æœ€çµ‚ç¢ºèª:\")\n",
    "        print(f\"   ãƒ•ã‚¡ã‚¤ãƒ«å: Gemma_2b_submission.csv\")\n",
    "        print(f\"   ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(submission_df):,}\")\n",
    "        print(f\"   äºˆæ¸¬å½¢å¼: MAP@3 (å„è¡Œã«3ã¤ã®äºˆæ¸¬ã‚’ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Š)\")\n",
    "        print(\"\\nğŸ† Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«æå‡ºæº–å‚™å®Œäº†!\")\n",
    "        \n",
    "        # æœ€çµ‚ãƒã‚§ãƒƒã‚¯ç”¨çµ±è¨ˆ\n",
    "        print(f\"\\nğŸ“ˆ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ:\")\n",
    "        all_pred_in_submission = \" \".join(submission_df['Correct']).split()\n",
    "        unique_preds = set(all_pred_in_submission)\n",
    "        print(f\"   ä½¿ç”¨ã•ã‚ŒãŸãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ©ãƒ™ãƒ«æ•°: {len(unique_preds)}\")\n",
    "        print(f\"   ãƒ©ãƒ™ãƒ«ä¸€è¦§: {sorted(unique_preds)}\")\n",
    "    else:\n",
    "        print(\"âŒ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "else:\n",
    "    print(\"âŒ å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å‰ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c27647",
   "metadata": {},
   "source": [
    "## ğŸ¯ Summary & Performance Notes\n",
    "\n",
    "### ğŸ“Š å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ä»¥ä¸‹ã‚’å®Ÿè£…ã—ã¾ã—ãŸï¼š\n",
    "\n",
    "1. **ğŸ¤– äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿**: Kaggleã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸGemma-2-2b-itãƒ¢ãƒ‡ãƒ«\n",
    "2. **ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å‡¦ç†**: math misconceptionã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®test.csv\n",
    "3. **ğŸ”® åŠ¹ç‡çš„ãªæ¨è«–**: ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹é«˜é€Ÿäºˆæ¸¬\n",
    "4. **ğŸ“ˆ MAP@3å½¢å¼å‡ºåŠ›**: TOP-3äºˆæ¸¬ã®ç”Ÿæˆ\n",
    "5. **ğŸ“¤ æå‡ºæº–å‚™**: Kaggleå½¢å¼ã®submission.csvä½œæˆ\n",
    "\n",
    "### ğŸš€ ä½¿ç”¨æŠ€è¡“\n",
    "- **ãƒ¢ãƒ‡ãƒ«**: `google/gemma-2-2b-it` (~2.6B parameters)\n",
    "- **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: PyTorch + Transformers\n",
    "- **æ¨è«–**: äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼ˆè¿½åŠ è¨“ç·´ãªã—ï¼‰\n",
    "- **è©•ä¾¡**: MAP@3 (Mean Average Precision at 3)\n",
    "\n",
    "### âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–\n",
    "- ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªæ¨è«–\n",
    "- é©å¿œçš„ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆCPU: 4, GPU: 8ï¼‰\n",
    "- FP16ä½¿ç”¨ï¼ˆGPUç’°å¢ƒï¼‰\n",
    "- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼\n",
    "\n",
    "### ğŸ“ ä½¿ç”¨æ–¹æ³•\n",
    "1. Kaggleã§äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "2. MODEL_DATA_PATHã‚’æ­£ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã«å¤‰æ›´\n",
    "3. å…¨ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ\n",
    "4. `Gemma_2b_submission.csv`ãŒç”Ÿæˆã•ã‚Œã‚‹\n",
    "5. Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æå‡º\n",
    "\n",
    "### ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
    "- **ãƒ¡ãƒ¢ãƒªä¸è¶³**: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ä¸‹ã’ã‚‹\n",
    "- **ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã¨ãƒ‘ã‚¹ã‚’ç¢ºèª\n",
    "- **CUDA OOM**: CPUãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ\n",
    "\n",
    "**ğŸ† Good luck with your submission!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
