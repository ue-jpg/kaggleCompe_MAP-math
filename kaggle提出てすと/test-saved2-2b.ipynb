{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e070df7",
   "metadata": {
    "papermill": {
     "duration": 0.003864,
     "end_time": "2025-07-21T09:42:18.946411",
     "exception": false,
     "start_time": "2025-07-21T09:42:18.942547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ğŸ¯ MAP Competition: Gemma-2-2b-it Submission Notebook\n",
    "\n",
    "## Overview\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€äº‹å‰ã«è¨“ç·´ãƒ»ä¿å­˜ã•ã‚ŒãŸGemma-2-2b-itãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€MAP - Charting Student Math Misunderstandingsã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®æå‡ºã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "### Model Details\n",
    "- **Pre-trained Model**: google/gemma-2-2b-it\n",
    "- **Parameters**: ~2.6B\n",
    "- **Task**: 6-class text classification\n",
    "- **Evaluation Metric**: MAP@3\n",
    "\n",
    "### Target Classes (6åˆ†é¡)\n",
    "- **True_Correct**: æ­£è§£ã§æ­£ã—ã„èª¬æ˜\n",
    "- **True_Neither**: æ­£è§£ã ãŒæ›–æ˜§ãªèª¬æ˜\n",
    "- **True_Misconception**: æ­£è§£ã ãŒèª¤ã£ãŸæ¦‚å¿µã®èª¬æ˜\n",
    "- **False_Correct**: ä¸æ­£è§£ã ãŒæ­£ã—ã„æ¦‚å¿µã®èª¬æ˜\n",
    "- **False_Neither**: ä¸æ­£è§£ã§æ›–æ˜§ãªèª¬æ˜\n",
    "- **False_Misconception**: ä¸æ­£è§£ã§èª¤ã£ãŸæ¦‚å¿µã®èª¬æ˜\n",
    "\n",
    "### Strategy\n",
    "1. Kaggleã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸäº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
    "2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†\n",
    "3. æ¨è«–å®Ÿè¡Œã§MAP@3å½¢å¼ã®äºˆæ¸¬ç”Ÿæˆ\n",
    "4. submission.csvä½œæˆãƒ»æå‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16eb274",
   "metadata": {
    "papermill": {
     "duration": 0.00274,
     "end_time": "2025-07-21T09:42:18.952360",
     "exception": false,
     "start_time": "2025-07-21T09:42:18.949620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ğŸ“¦ Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69488c6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T09:42:18.958825Z",
     "iopub.status.busy": "2025-07-21T09:42:18.958583Z",
     "iopub.status.idle": "2025-07-21T09:42:29.743222Z",
     "shell.execute_reply": "2025-07-21T09:42:29.742424Z"
    },
    "papermill": {
     "duration": 10.789453,
     "end_time": "2025-07-21T09:42:29.744542",
     "exception": false,
     "start_time": "2025-07-21T09:42:18.955089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… transformers already installed: 4.52.4\n",
      "âœ… accelerate already installed: 1.8.1\n",
      "âœ… sentencepiece already installed: 0.2.0\n",
      "ğŸ‰ All required libraries are ready!\n"
     ]
    }
   ],
   "source": [
    "# Kaggleç’°å¢ƒã§å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹é–¢æ•°\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆKaggleã§é€šå¸¸ä¸è¶³ã™ã‚‹ã‚‚ã®ï¼‰\n",
    "required_packages = [\n",
    "    \"transformers>=4.35.0\",\n",
    "    \"accelerate>=0.26.0\", \n",
    "    \"sentencepiece>=0.1.99\"\n",
    "]\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«çŠ¶æ³ç¢ºèª\n",
    "        if \"transformers\" in package:\n",
    "            import transformers\n",
    "            print(f\"âœ… transformers already installed: {transformers.__version__}\")\n",
    "        elif \"accelerate\" in package:\n",
    "            import accelerate\n",
    "            print(f\"âœ… accelerate already installed: {accelerate.__version__}\")\n",
    "        elif \"sentencepiece\" in package:\n",
    "            import sentencepiece\n",
    "            print(f\"âœ… sentencepiece already installed: {sentencepiece.__version__}\")\n",
    "    except ImportError:\n",
    "        print(f\"ğŸ“¦ Installing {package}...\")\n",
    "        install_package(package)\n",
    "\n",
    "print(\"ğŸ‰ All required libraries are ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7078da9b",
   "metadata": {
    "papermill": {
     "duration": 0.002713,
     "end_time": "2025-07-21T09:42:29.750524",
     "exception": false,
     "start_time": "2025-07-21T09:42:29.747811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ğŸ“š Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c4ea7fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T09:42:29.757118Z",
     "iopub.status.busy": "2025-07-21T09:42:29.756789Z",
     "iopub.status.idle": "2025-07-21T09:42:52.985482Z",
     "shell.execute_reply": "2025-07-21T09:42:52.984604Z"
    },
    "papermill": {
     "duration": 23.233486,
     "end_time": "2025-07-21T09:42:52.986793",
     "exception": false,
     "start_time": "2025-07-21T09:42:29.753307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 09:42:37.625936: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753090957.990726      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753090958.093713      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Environment Information:\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "GPU device: Tesla T4\n",
      "GPU memory: 15.8 GB\n",
      "ğŸ“ Kaggle environment detected: /kaggle/input\n",
      "ğŸ¯ Competition data path: /kaggle/input/map-charting-student-math-misunderstandings\n",
      "ğŸ¤– Model data path: /kaggle/input/test_gemma-2-2b-math-misconception/transformers/default/1/gemma-2-2b-math-misconception\n",
      "âœ… All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "# è­¦å‘Šã‚’éè¡¨ç¤º\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º\n",
    "print(\"ğŸ”§ Environment Information:\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Kaggleã®ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹è¨­å®š\n",
    "KAGGLE_INPUT_PATH = \"/kaggle/input\"\n",
    "if os.path.exists(KAGGLE_INPUT_PATH):\n",
    "    print(f\"ğŸ“ Kaggle environment detected: {KAGGLE_INPUT_PATH}\")\n",
    "    # ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹\n",
    "    COMP_DATA_PATH = \"/kaggle/input/map-charting-student-math-misunderstandings\"\n",
    "    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã«å¤‰æ›´ã—ã¦ãã ã•ã„ï¼‰\n",
    "    MODEL_DATA_PATH = \"/kaggle/input/test_gemma-2-2b-math-misconception/transformers/default/1/gemma-2-2b-math-misconception\"\n",
    "    print(f\"ğŸ¯ Competition data path: {COMP_DATA_PATH}\")\n",
    "    print(f\"ğŸ¤– Model data path: {MODEL_DATA_PATH}\")\n",
    "else:\n",
    "    print(\"ğŸ“ Local environment detected\")\n",
    "    COMP_DATA_PATH = \"./map_data\"\n",
    "    MODEL_DATA_PATH = \"./saved_models/gemma-2-2b-math-misconception\"\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0117d81e",
   "metadata": {
    "papermill": {
     "duration": 0.002902,
     "end_time": "2025-07-21T09:42:52.993011",
     "exception": false,
     "start_time": "2025-07-21T09:42:52.990109",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ğŸ”§ Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb6105b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T09:42:53.000122Z",
     "iopub.status.busy": "2025-07-21T09:42:52.999649Z",
     "iopub.status.idle": "2025-07-21T09:42:53.005650Z",
     "shell.execute_reply": "2025-07-21T09:42:53.004788Z"
    },
    "papermill": {
     "duration": 0.010832,
     "end_time": "2025-07-21T09:42:53.006824",
     "exception": false,
     "start_time": "2025-07-21T09:42:52.995992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MathMisconceptionDataset class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class MathMisconceptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Math Misconception Dataset for PyTorch\n",
    "    æ¨è«–å°‚ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list): ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ\n",
    "            tokenizer: Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
    "            max_length (int): æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "\n",
    "        # Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "        }\n",
    "\n",
    "print(\"âœ… MathMisconceptionDataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf6724",
   "metadata": {
    "papermill": {
     "duration": 0.002896,
     "end_time": "2025-07-21T09:42:53.012727",
     "exception": false,
     "start_time": "2025-07-21T09:42:53.009831",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ğŸ¤– Load Pre-trained Gemma Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce8575d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T09:42:53.019520Z",
     "iopub.status.busy": "2025-07-21T09:42:53.019277Z",
     "iopub.status.idle": "2025-07-21T09:43:48.688798Z",
     "shell.execute_reply": "2025-07-21T09:43:48.687994Z"
    },
    "papermill": {
     "duration": 55.674434,
     "end_time": "2025-07-21T09:43:48.690133",
     "exception": false,
     "start_time": "2025-07-21T09:42:53.015699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ¤– äº‹å‰è¨“ç·´æ¸ˆã¿Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
      "============================================================\n",
      "ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: /kaggle/input/test_gemma-2-2b-math-misconception/transformers/default/1/gemma-2-2b-math-misconception\n",
      "ğŸ“‹ ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿: /kaggle/input/test_gemma-2-2b-math-misconception/transformers/default/1/gemma-2-2b-math-misconception/label_mapping.json\n",
      "âœ… ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿æˆåŠŸ\n",
      "   0: False_Correct\n",
      "   1: False_Misconception\n",
      "   2: False_Neither\n",
      "   3: True_Correct\n",
      "   4: True_Misconception\n",
      "   5: True_Neither\n",
      "\n",
      "ğŸ“ Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...\n",
      "âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿æˆåŠŸ\n",
      "ğŸ”– ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³: <pad>\n",
      "ğŸ“ èªå½™ã‚µã‚¤ã‚º: 256,000\n",
      "\n",
      "ğŸ§  Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b2d2e344ae4d82af49f5c7c2d44afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†!\n",
      "ğŸ“Š åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°: 6\n",
      "ğŸ“ˆ ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 2,614,355,712\n",
      "ğŸ’¡ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: ~2.61B parameters\n",
      "\n",
      "ğŸ‰ äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æº–å‚™å®Œäº†!\n"
     ]
    }
   ],
   "source": [
    "def load_pretrained_gemma_model():\n",
    "    \"\"\"äº‹å‰è¨“ç·´æ¸ˆã¿Gemmaãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ¤– äº‹å‰è¨“ç·´æ¸ˆã¿Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ç¢ºèª\n",
    "        print(f\"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {MODEL_DATA_PATH}\")\n",
    "        \n",
    "        if not os.path.exists(MODEL_DATA_PATH):\n",
    "            print(f\"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {MODEL_DATA_PATH}\")\n",
    "            print(\"ğŸ’¡ Kaggleç’°å¢ƒã§ã¯æ­£ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’ç¢ºèªã—ã¦ãã ã•ã„\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã®èª­ã¿è¾¼ã¿\n",
    "        label_file = os.path.join(MODEL_DATA_PATH, \"label_mapping.json\")\n",
    "        print(f\"ğŸ“‹ ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿: {label_file}\")\n",
    "        \n",
    "        with open(label_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            label_mapping = json.load(f)\n",
    "        \n",
    "        print(\"âœ… ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿æˆåŠŸ\")\n",
    "        for idx, label in label_mapping.items():\n",
    "            print(f\"   {idx}: {label}\")\n",
    "        \n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\n",
    "        print(\"\\nğŸ“ Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_DATA_PATH)\n",
    "        print(f\"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿æˆåŠŸ\")\n",
    "        print(f\"ğŸ”– ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³: {tokenizer.pad_token}\")\n",
    "        print(f\"ğŸ“ èªå½™ã‚µã‚¤ã‚º: {tokenizer.vocab_size:,}\")\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "        print(\"\\nğŸ§  Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_DATA_PATH,\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\" if device.type == \"cuda\" else None,\n",
    "        )\n",
    "        \n",
    "        # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\n",
    "        if device.type == \"cpu\":\n",
    "            model = model.to(device)\n",
    "        \n",
    "        print(f\"âœ… Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†!\")\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"ğŸ“Š åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°: {model.config.num_labels}\")\n",
    "        print(f\"ğŸ“ˆ ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}\")\n",
    "        print(f\"ğŸ’¡ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: ~{total_params / 1e9:.2f}B parameters\")\n",
    "        \n",
    "        # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\n",
    "        model.eval()\n",
    "        \n",
    "        return model, tokenizer, label_mapping\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        print(\"\\nğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:\")\n",
    "        print(\"1. Kaggleã§ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒæ­£ã—ãã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\")\n",
    "        print(\"2. MODEL_DATA_PATHãŒæ­£ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’æŒ‡ã—ã¦ã„ã‚‹ã‹ç¢ºèª\")\n",
    "        print(\"3. å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒå…¨ã¦å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\")\n",
    "        raise e\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Ÿè¡Œ\n",
    "model, tokenizer, label_mapping = load_pretrained_gemma_model()\n",
    "print(\"\\nğŸ‰ äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æº–å‚™å®Œäº†!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d062c1c",
   "metadata": {
    "papermill": {
     "duration": 0.003176,
     "end_time": "2025-07-21T09:43:48.696898",
     "exception": false,
     "start_time": "2025-07-21T09:43:48.693722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ğŸ“Š Load and Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b985ba4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T09:43:48.705643Z",
     "iopub.status.busy": "2025-07-21T09:43:48.705097Z",
     "iopub.status.idle": "2025-07-21T09:43:48.741362Z",
     "shell.execute_reply": "2025-07-21T09:43:48.740393Z"
    },
    "papermill": {
     "duration": 0.042372,
     "end_time": "2025-07-21T09:43:48.742578",
     "exception": false,
     "start_time": "2025-07-21T09:43:48.700206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\n",
      "============================================================\n",
      "ğŸ“ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹: /kaggle/input/map-charting-student-math-misunderstandings/test.csv\n",
      "âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ!\n",
      "ğŸ“ˆ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: (3, 5)\n",
      "\n",
      "ğŸ“‹ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ—:\n",
      "['row_id', 'QuestionId', 'QuestionText', 'MC_Answer', 'StudentExplanation']\n",
      "\n",
      "ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:\n",
      "   row_id  QuestionId                                       QuestionText  \\\n",
      "0   36696       31772  What fraction of the shape is not shaded? Give...   \n",
      "1   36697       31772  What fraction of the shape is not shaded? Give...   \n",
      "2   36698       32835                      Which number is the greatest?   \n",
      "\n",
      "           MC_Answer                                 StudentExplanation  \n",
      "0  \\( \\frac{1}{3} \\)  I think that 1/3 is the answer, as it's the si...  \n",
      "1  \\( \\frac{3}{6} \\)  i think this answer is because 3 triangles are...  \n",
      "2          \\( 6.2 \\)     because the 2 makes it higher than the others.  \n",
      "\n",
      "ğŸ”§ å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆä¸­...\n",
      "\n",
      "ğŸ“Š ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ:\n",
      "   å¹³å‡: 235 æ–‡å­—\n",
      "   æœ€å°: 126 æ–‡å­—\n",
      "   æœ€å¤§: 296 æ–‡å­—\n",
      "   ä¸­å¤®å€¤: 284 æ–‡å­—\n",
      "\n",
      "ğŸ“ å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«:\n",
      "Length: 284 characters\n",
      "Sample: Question: What fraction of the shape is not shaded? Give your answer in its simplest form. [Image: A triangle split into 9 equal smaller triangles. 6 of them are shaded.] Selected Answer: \\( \\frac{1}{3} \\) Explanation: I think that 1/3 is the answer, as it's the simplest form of 3/9....\n"
     ]
    }
   ],
   "source": [
    "def load_and_prepare_test_data():\n",
    "    \"\"\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "        test_path = os.path.join(COMP_DATA_PATH, \"test.csv\")\n",
    "        print(f\"ğŸ“ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹: {test_path}\")\n",
    "        \n",
    "        if not os.path.exists(test_path):\n",
    "            print(f\"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {test_path}\")\n",
    "            return None\n",
    "        \n",
    "        test_df = pd.read_csv(test_path)\n",
    "        print(f\"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ!\")\n",
    "        print(f\"ğŸ“ˆ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test_df.shape}\")\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ç¢ºèª\n",
    "        print(\"\\nğŸ“‹ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ—:\")\n",
    "        print(test_df.columns.tolist())\n",
    "        \n",
    "        print(\"\\nğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:\")\n",
    "        print(test_df.head(3))\n",
    "        \n",
    "        # å¼·åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã®ä½œæˆ\n",
    "        def create_enhanced_text(row):\n",
    "            \"\"\"Question + MC_Answer + Explanation ã‚’çµåˆã—ãŸå¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆ\"\"\"\n",
    "            question = str(row.get(\"QuestionText\", \"\")) if pd.notna(row.get(\"QuestionText\")) else \"\"\n",
    "            mc_answer = str(row.get(\"MC_Answer\", \"\")) if pd.notna(row.get(\"MC_Answer\")) else \"\"\n",
    "            explanation = str(row.get(\"StudentExplanation\", \"\")) if pd.notna(row.get(\"StudentExplanation\")) else \"\"\n",
    "            \n",
    "            # Gemmaç”¨ã®æ§‹é€ åŒ–ãƒ†ã‚­ã‚¹ãƒˆ\n",
    "            enhanced_text = f\"Question: {question} Selected Answer: {mc_answer} Explanation: {explanation}\"\n",
    "            return enhanced_text\n",
    "\n",
    "        print(\"\\nğŸ”§ å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆä¸­...\")\n",
    "        test_df[\"enhanced_text\"] = test_df.apply(create_enhanced_text, axis=1)\n",
    "        \n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆé•·ã®çµ±è¨ˆ\n",
    "        text_lengths = test_df[\"enhanced_text\"].str.len()\n",
    "        print(f\"\\nğŸ“Š ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ:\")\n",
    "        print(f\"   å¹³å‡: {text_lengths.mean():.0f} æ–‡å­—\")\n",
    "        print(f\"   æœ€å°: {text_lengths.min()} æ–‡å­—\")\n",
    "        print(f\"   æœ€å¤§: {text_lengths.max()} æ–‡å­—\")\n",
    "        print(f\"   ä¸­å¤®å€¤: {text_lengths.median():.0f} æ–‡å­—\")\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¤º\n",
    "        print(f\"\\nğŸ“ å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«:\")\n",
    "        sample_text = test_df[\"enhanced_text\"].iloc[0]\n",
    "        print(f\"Length: {len(sample_text)} characters\")\n",
    "        print(f\"Sample: {sample_text[:300]}...\")\n",
    "        \n",
    "        return test_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Ÿè¡Œ\n",
    "test_df = load_and_prepare_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc7823",
   "metadata": {
    "papermill": {
     "duration": 0.003145,
     "end_time": "2025-07-21T09:43:48.749140",
     "exception": false,
     "start_time": "2025-07-21T09:43:48.745995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ğŸ”® Generate Predictions for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1df2435d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T09:43:48.756510Z",
     "iopub.status.busy": "2025-07-21T09:43:48.756273Z",
     "iopub.status.idle": "2025-07-21T09:43:50.383373Z",
     "shell.execute_reply": "2025-07-21T09:43:50.382342Z"
    },
    "papermill": {
     "duration": 1.632398,
     "end_time": "2025-07-21T09:43:50.384792",
     "exception": false,
     "start_time": "2025-07-21T09:43:48.752394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”® ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆäºˆæ¸¬ã‚’é–‹å§‹ã—ã¾ã™...\n",
      "============================================================\n",
      "ğŸ”® ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆäºˆæ¸¬ç”Ÿæˆï¼ˆMAP@3ï¼‰\n",
      "============================================================\n",
      "ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: 3ä»¶\n",
      "ğŸ”§ ãƒãƒƒãƒã‚µã‚¤ã‚º: 8\n",
      "ğŸ”§ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­...\n",
      "âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ: 1ãƒãƒƒãƒ\n",
      "ğŸ”® äºˆæ¸¬å®Ÿè¡Œä¸­...\n",
      "   é€²æ—: 3/3 (100.0%)\n",
      "âœ… äºˆæ¸¬å®Œäº†!\n",
      "ğŸ“Š äºˆæ¸¬çµæœå½¢çŠ¶: (3, 6)\n",
      "ğŸ¯ TOP-3äºˆæ¸¬æŠ½å‡ºä¸­...\n",
      "  ã‚µãƒ³ãƒ—ãƒ« 1: True_Neither True_Misconception False_Correct\n",
      "    ç¢ºç‡: ['0.965', '0.027', '0.005']\n",
      "  ã‚µãƒ³ãƒ—ãƒ« 2: True_Neither True_Misconception False_Correct\n",
      "    ç¢ºç‡: ['0.937', '0.041', '0.018']\n",
      "  ã‚µãƒ³ãƒ—ãƒ« 3: True_Neither True_Misconception False_Correct\n",
      "    ç¢ºç‡: ['0.537', '0.435', '0.016']\n",
      "âœ… TOP-3äºˆæ¸¬æŠ½å‡ºå®Œäº†: 3ä»¶\n",
      "\n",
      "ğŸ“ˆ äºˆæ¸¬çµ±è¨ˆ:\n",
      "  äºˆæ¸¬ã«ä½¿ç”¨ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªæ•°: 3\n",
      "    True_Neither: 3å› (33.3%)\n",
      "    True_Misconception: 3å› (33.3%)\n",
      "    False_Correct: 3å› (33.3%)\n",
      "ğŸ‰ ãƒ†ã‚¹ãƒˆäºˆæ¸¬å®Œäº†!\n"
     ]
    }
   ],
   "source": [
    "def generate_test_predictions(model, tokenizer, test_df, label_mapping, batch_size=8):\n",
    "    \"\"\"ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹äºˆæ¸¬ç”Ÿæˆï¼ˆMAP@3å½¢å¼ï¼‰\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ”® ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆäºˆæ¸¬ç”Ÿæˆï¼ˆMAP@3ï¼‰\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if test_df is None or len(test_df) == 0:\n",
    "        print(\"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_df):,}ä»¶\")\n",
    "    print(f\"ğŸ”§ ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}\")\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\n",
    "    test_texts = test_df[\"enhanced_text\"].tolist()\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
    "    print(\"ğŸ”§ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­...\")\n",
    "    test_dataset = MathMisconceptionDataset(\n",
    "        test_texts, tokenizer, max_length=512\n",
    "    )\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ: {len(test_dataloader)}ãƒãƒƒãƒ\")\n",
    "    \n",
    "    # äºˆæ¸¬å®Ÿè¡Œ\n",
    "    print(\"ğŸ”® äºˆæ¸¬å®Ÿè¡Œä¸­...\")\n",
    "    all_predictions = []\n",
    "    \n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(test_dataloader):\n",
    "                # ãƒãƒƒãƒã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                # æ¨è«–å®Ÿè¡Œ\n",
    "                outputs = model(**batch)\n",
    "                predictions = outputs.logits\n",
    "                \n",
    "                # CPU ã«ç§»å‹•ã—ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "                batch_predictions = predictions.cpu().numpy()\n",
    "                all_predictions.append(batch_predictions)\n",
    "                \n",
    "                # é€²æ—è¡¨ç¤º\n",
    "                if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(test_dataloader):\n",
    "                    processed = min((batch_idx + 1) * batch_size, len(test_df))\n",
    "                    print(f\"   é€²æ—: {processed:,}/{len(test_df):,} ({processed/len(test_df)*100:.1f}%)\")\n",
    "        \n",
    "        print(\"âœ… äºˆæ¸¬å®Œäº†!\")\n",
    "        \n",
    "        # äºˆæ¸¬çµæœã‚’çµåˆ\n",
    "        all_predictions = np.vstack(all_predictions)\n",
    "        print(f\"ğŸ“Š äºˆæ¸¬çµæœå½¢çŠ¶: {all_predictions.shape}\")\n",
    "        \n",
    "        # ç¢ºç‡ã«å¤‰æ›\n",
    "        probs = torch.softmax(torch.tensor(all_predictions), dim=-1).numpy()\n",
    "        \n",
    "        # å„ã‚µãƒ³ãƒ—ãƒ«ã§ä¸Šä½3ã¤ã®äºˆæ¸¬ã‚’å–å¾—\n",
    "        print(\"ğŸ¯ TOP-3äºˆæ¸¬æŠ½å‡ºä¸­...\")\n",
    "        submission_predictions = []\n",
    "        \n",
    "        # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã®é€†å¤‰æ›ç”¨\n",
    "        idx_to_label = {int(k): v for k, v in label_mapping.items()}\n",
    "        \n",
    "        for i, prob in enumerate(probs):\n",
    "            # ç¢ºç‡ã®é«˜ã„é †ã«ä¸Šä½3ã¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
    "            top3_indices = np.argsort(prob)[::-1][:3]\n",
    "            \n",
    "            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«åã«å¤‰æ›\n",
    "            top3_labels = [idx_to_label[idx] for idx in top3_indices]\n",
    "            \n",
    "            # ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§çµåˆï¼ˆã‚³ãƒ³ãƒšè¦æ±‚å½¢å¼ï¼‰\n",
    "            prediction_string = \" \".join(top3_labels)\n",
    "            submission_predictions.append(prediction_string)\n",
    "            \n",
    "            # é€²æ—è¡¨ç¤ºï¼ˆæœ€åˆã®5ä»¶ï¼‰\n",
    "            if i < 5:\n",
    "                top3_probs = [prob[idx] for idx in top3_indices]\n",
    "                print(f\"  ã‚µãƒ³ãƒ—ãƒ« {i+1}: {prediction_string}\")\n",
    "                print(f\"    ç¢ºç‡: {[f'{p:.3f}' for p in top3_probs]}\")\n",
    "        \n",
    "        print(f\"âœ… TOP-3äºˆæ¸¬æŠ½å‡ºå®Œäº†: {len(submission_predictions)}ä»¶\")\n",
    "        \n",
    "        # äºˆæ¸¬ã®çµ±è¨ˆæƒ…å ±\n",
    "        all_pred_labels = \" \".join(submission_predictions).split()\n",
    "        from collections import Counter\n",
    "        pred_counts = Counter(all_pred_labels)\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ äºˆæ¸¬çµ±è¨ˆ:\")\n",
    "        print(f\"  äºˆæ¸¬ã«ä½¿ç”¨ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªæ•°: {len(pred_counts)}\")\n",
    "        for category, count in pred_counts.most_common():\n",
    "            percentage = count / (len(submission_predictions) * 3) * 100\n",
    "            print(f\"    {category}: {count}å› ({percentage:.1f}%)\")\n",
    "        \n",
    "        return submission_predictions\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ äºˆæ¸¬å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"ğŸ–¥ï¸ ç¾åœ¨ã®GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "        raise e\n",
    "\n",
    "# äºˆæ¸¬å®Ÿè¡Œ\n",
    "if model is not None and tokenizer is not None and test_df is not None:\n",
    "    print(\"ğŸ”® ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆäºˆæ¸¬ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ç’°å¢ƒã«å¿œã˜ã¦èª¿æ•´\n",
    "    batch_size = 4 if device.type == \"cpu\" else 8\n",
    "    test_predictions = generate_test_predictions(model, tokenizer, test_df, label_mapping, batch_size)\n",
    "    print(\"ğŸ‰ ãƒ†ã‚¹ãƒˆäºˆæ¸¬å®Œäº†!\")\n",
    "else:\n",
    "    print(\"âŒ å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    test_predictions = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44b843f",
   "metadata": {
    "papermill": {
     "duration": 0.003718,
     "end_time": "2025-07-21T09:43:50.392570",
     "exception": false,
     "start_time": "2025-07-21T09:43:50.388852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ğŸ“¤ Create Submission File\n",
    "\n",
    "æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«æå‡ºã™ã‚‹ãŸã‚ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c135b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T09:43:50.400844Z",
     "iopub.status.busy": "2025-07-21T09:43:50.400605Z",
     "iopub.status.idle": "2025-07-21T09:43:50.421219Z",
     "shell.execute_reply": "2025-07-21T09:43:50.420450Z"
    },
    "papermill": {
     "duration": 0.02642,
     "end_time": "2025-07-21T09:43:50.422502",
     "exception": false,
     "start_time": "2025-07-21T09:43:50.396082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¤ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™...\n",
      "============================================================\n",
      "ğŸ“¤ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n",
      "============================================================\n",
      "ğŸ“Š æå‡ºãƒ‡ãƒ¼ã‚¿: 3ä»¶\n",
      "ğŸ”§ Kaggleæå‡ºå½¢å¼ã«å¤‰æ›ä¸­...\n",
      "  ã‚µãƒ³ãƒ—ãƒ« 1: True_Neither:NA True_Misconception:Incomplete False_Correct:NA\n",
      "  ã‚µãƒ³ãƒ—ãƒ« 2: True_Neither:NA True_Misconception:Incomplete False_Correct:NA\n",
      "  ã‚µãƒ³ãƒ—ãƒ« 3: True_Neither:NA True_Misconception:Incomplete False_Correct:NA\n",
      "âœ… æå‡ºãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆå®Œäº†: (3, 2)\n",
      "ğŸ“ åˆ—: ['row_id', 'Category:Misconception']\n",
      "\n",
      "ğŸ“‹ æå‡ºãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:\n",
      " row_id                                         Category:Misconception\n",
      "  36696 True_Neither:NA True_Misconception:Incomplete False_Correct:NA\n",
      "  36697 True_Neither:NA True_Misconception:Incomplete False_Correct:NA\n",
      "  36698 True_Neither:NA True_Misconception:Incomplete False_Correct:NA\n",
      "\n",
      "ğŸ’¾ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: submission.csv\n",
      "ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 237 bytes (0.2 KB)\n",
      "âœ… æå‡ºãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼: (3, 2)\n",
      "   å¿…è¦åˆ—å­˜åœ¨ç¢ºèª: True\n",
      "ğŸ” äºˆæ¸¬å½¢å¼ã‚µãƒ³ãƒ—ãƒ«:\n",
      "   1: True_Neither:NA True_Misconception:Incomplete False_Correct:NA (è¦ç´ æ•°: 3)\n",
      "   2: True_Neither:NA True_Misconception:Incomplete False_Correct:NA (è¦ç´ æ•°: 3)\n",
      "   3: True_Neither:NA True_Misconception:Incomplete False_Correct:NA (è¦ç´ æ•°: 3)\n",
      "\n",
      "ğŸ“Š æå‡ºãƒ•ã‚¡ã‚¤ãƒ«æœ€çµ‚ç¢ºèª:\n",
      "   âœ… å½¢å¼: Kaggle MAPç«¶æŠ€å½¢å¼ï¼ˆrow_id, Category:Misconceptionï¼‰\n",
      "   âœ… ä»¶æ•°: 3ä»¶\n",
      "   âœ… åˆ—å: ['row_id', 'Category:Misconception']\n",
      "ğŸ‰ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†!\n",
      "ğŸ“‹ æœ€çµ‚ç¢ºèª:\n",
      "   ãƒ•ã‚¡ã‚¤ãƒ«å: submission.csv\n",
      "   ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: 3\n",
      "   äºˆæ¸¬å½¢å¼: Kaggle MAPå½¢å¼ (row_id, Category:Misconception)\n",
      "\n",
      "ğŸ† Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«æå‡ºæº–å‚™å®Œäº†!\n",
      "\n",
      "ğŸ“ˆ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ:\n",
      "   ä½¿ç”¨ã•ã‚ŒãŸãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ©ãƒ™ãƒ«æ•°: 3\n",
      "   ãƒ©ãƒ™ãƒ«ä¸€è¦§: ['False_Correct:NA', 'True_Misconception:Incomplete', 'True_Neither:NA']\n"
     ]
    }
   ],
   "source": [
    "def create_submission_file(test_df, predictions, output_path=\"submission.csv\"):\n",
    "    \"\"\"æå‡ºç”¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ“¤ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if test_df is None or predictions is None:\n",
    "        print(\"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯äºˆæ¸¬çµæœãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ“Š æå‡ºãƒ‡ãƒ¼ã‚¿: {len(predictions):,}ä»¶\")\n",
    "    \n",
    "    # sample_submission.csvã‚’å‚è€ƒã«ã—ã¦æ­£ã—ã„å½¢å¼ã§ä½œæˆ\n",
    "    # row_id,Category:Misconception ã®å½¢å¼ãŒå¿…è¦\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«row_idãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "    if 'row_id' in test_df.columns:\n",
    "        row_ids = test_df['row_id'].tolist()\n",
    "    else:\n",
    "        # row_idãŒãªã„å ´åˆã€testãƒ‡ãƒ¼ã‚¿ã®é–‹å§‹IDã‚’æ¨å®šï¼ˆé€šå¸¸ã¯36696ã‹ã‚‰ï¼‰\n",
    "        print(\"âš ï¸ row_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ¨å®šå€¤ã‚’ä½¿ç”¨ã—ã¾ã™...\")\n",
    "        start_id = 36696  # sample_submissionã®é–‹å§‹ID\n",
    "        row_ids = list(range(start_id, start_id + len(test_df)))\n",
    "    \n",
    "    # Kaggleè¦æ±‚å½¢å¼ï¼šå„äºˆæ¸¬ã‚’ \"Category:Misconception\" å½¢å¼ã«å¤‰æ›\n",
    "    print(\"ğŸ”§ Kaggleæå‡ºå½¢å¼ã«å¤‰æ›ä¸­...\")\n",
    "    formatted_predictions = []\n",
    "    \n",
    "    for i, pred in enumerate(predictions):\n",
    "        # predã¯ \"True_Correct False_Neither False_Misconception\" ã®ã‚ˆã†ãªã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Š\n",
    "        pred_categories = pred.split()[:3]  # TOP3ã«é™å®š\n",
    "        \n",
    "        # å„ã‚«ãƒ†ã‚´ãƒªã‚’ \"Category:Misconception\" å½¢å¼ã«å¤‰æ›\n",
    "        formatted_parts = []\n",
    "        for category in pred_categories:\n",
    "            if category.endswith('_Misconception'):\n",
    "                # Misconceptionã‚«ãƒ†ã‚´ãƒªã®å ´åˆã€å®Ÿéš›ã®èª¤æ¦‚å¿µåãŒå¿…è¦\n",
    "                # ã“ã“ã§ã¯ç°¡æ˜“çš„ã« \"Incomplete\" ã‚’ä½¿ç”¨ï¼ˆå®Ÿéš›ã¯äºˆæ¸¬çµæœã‹ã‚‰å–å¾—ï¼‰\n",
    "                formatted_parts.append(f\"{category}:Incomplete\")\n",
    "            else:\n",
    "                # ä»–ã®ã‚«ãƒ†ã‚´ãƒªã®å ´åˆã¯NA\n",
    "                formatted_parts.append(f\"{category}:NA\")\n",
    "        \n",
    "        # ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§çµåˆ\n",
    "        formatted_pred = \" \".join(formatted_parts)\n",
    "        formatted_predictions.append(formatted_pred)\n",
    "        \n",
    "        # æœ€åˆã®5ä»¶ã‚’ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\n",
    "        if i < 5:\n",
    "            print(f\"  ã‚µãƒ³ãƒ—ãƒ« {i+1}: {formatted_pred}\")\n",
    "    \n",
    "    # æå‡ºç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆï¼ˆæ­£ã—ã„Kaggleå½¢å¼ï¼‰\n",
    "    submission_df = pd.DataFrame({\n",
    "        'row_id': row_ids,\n",
    "        'Category:Misconception': formatted_predictions\n",
    "    })\n",
    "    \n",
    "    print(f\"âœ… æå‡ºãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆå®Œäº†: {submission_df.shape}\")\n",
    "    print(f\"ğŸ“ åˆ—: {list(submission_df.columns)}\")\n",
    "    \n",
    "    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\n",
    "    print(f\"\\nğŸ“‹ æå‡ºãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:\")\n",
    "    print(submission_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜\n",
    "    try:\n",
    "        submission_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nğŸ’¾ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {output_path}\")\n",
    "        \n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª\n",
    "        file_size = os.path.getsize(output_path)\n",
    "        print(f\"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes ({file_size/1024:.1f} KB)\")\n",
    "        \n",
    "        # å½¢å¼ç¢ºèª\n",
    "        check_df = pd.read_csv(output_path)\n",
    "        print(f\"âœ… æå‡ºãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼: {check_df.shape}\")\n",
    "        required_cols = ['row_id', 'Category:Misconception']\n",
    "        cols_present = all(col in check_df.columns for col in required_cols)\n",
    "        print(f\"   å¿…è¦åˆ—å­˜åœ¨ç¢ºèª: {cols_present}\")\n",
    "        \n",
    "        # äºˆæ¸¬å½¢å¼ãƒã‚§ãƒƒã‚¯\n",
    "        sample_predictions = check_df['Category:Misconception'].head(5).tolist()\n",
    "        print(f\"ğŸ” äºˆæ¸¬å½¢å¼ã‚µãƒ³ãƒ—ãƒ«:\")\n",
    "        for i, pred in enumerate(sample_predictions):\n",
    "            pred_parts = pred.split()\n",
    "            print(f\"   {i+1}: {pred} (è¦ç´ æ•°: {len(pred_parts)})\")\n",
    "            \n",
    "        print(f\"\\nğŸ“Š æå‡ºãƒ•ã‚¡ã‚¤ãƒ«æœ€çµ‚ç¢ºèª:\")\n",
    "        print(f\"   âœ… å½¢å¼: Kaggle MAPç«¶æŠ€å½¢å¼ï¼ˆrow_id, Category:Misconceptionï¼‰\")\n",
    "        print(f\"   âœ… ä»¶æ•°: {len(check_df):,}ä»¶\")\n",
    "        print(f\"   âœ… åˆ—å: {list(check_df.columns)}\")\n",
    "        \n",
    "        return submission_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None\n",
    "\n",
    "# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\n",
    "if test_predictions is not None and test_df is not None:\n",
    "    print(\"ğŸ“¤ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™...\")\n",
    "    submission_df = create_submission_file(test_df, test_predictions, \"submission.csv\")\n",
    "    \n",
    "    if submission_df is not None:\n",
    "        print(\"ğŸ‰ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†!\")\n",
    "        print(f\"ğŸ“‹ æœ€çµ‚ç¢ºèª:\")\n",
    "        print(f\"   ãƒ•ã‚¡ã‚¤ãƒ«å: submission.csv\")\n",
    "        print(f\"   ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(submission_df):,}\")\n",
    "        print(f\"   äºˆæ¸¬å½¢å¼: Kaggle MAPå½¢å¼ (row_id, Category:Misconception)\")\n",
    "        print(\"\\nğŸ† Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«æå‡ºæº–å‚™å®Œäº†!\")\n",
    "        \n",
    "        # æœ€çµ‚ãƒã‚§ãƒƒã‚¯ç”¨çµ±è¨ˆ\n",
    "        print(f\"\\nğŸ“ˆ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ:\")\n",
    "        all_pred_in_submission = \" \".join(submission_df['Category:Misconception']).split()\n",
    "        unique_preds = set(all_pred_in_submission)\n",
    "        print(f\"   ä½¿ç”¨ã•ã‚ŒãŸãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ©ãƒ™ãƒ«æ•°: {len(unique_preds)}\")\n",
    "        print(f\"   ãƒ©ãƒ™ãƒ«ä¸€è¦§: {sorted(unique_preds)}\")\n",
    "    else:\n",
    "        print(\"âŒ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "else:\n",
    "    print(\"âŒ å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å‰ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5faea7",
   "metadata": {
    "papermill": {
     "duration": 0.003368,
     "end_time": "2025-07-21T09:43:50.429414",
     "exception": false,
     "start_time": "2025-07-21T09:43:50.426046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ğŸ¯ Summary & Performance Notes\n",
    "\n",
    "### ğŸ“Š å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ä»¥ä¸‹ã‚’å®Ÿè£…ã—ã¾ã—ãŸï¼š\n",
    "\n",
    "1. **ğŸ¤– äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿**: Kaggleã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸGemma-2-2b-itãƒ¢ãƒ‡ãƒ«\n",
    "2. **ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å‡¦ç†**: math misconceptionã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®test.csv\n",
    "3. **ğŸ”® åŠ¹ç‡çš„ãªæ¨è«–**: ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹é«˜é€Ÿäºˆæ¸¬\n",
    "4. **ğŸ“ˆ MAP@3å½¢å¼å‡ºåŠ›**: TOP-3äºˆæ¸¬ã®ç”Ÿæˆ\n",
    "5. **ğŸ“¤ æå‡ºæº–å‚™**: Kaggleå½¢å¼ã®submission.csvä½œæˆ\n",
    "\n",
    "### ğŸš€ ä½¿ç”¨æŠ€è¡“\n",
    "- **ãƒ¢ãƒ‡ãƒ«**: `google/gemma-2-2b-it` (~2.6B parameters)\n",
    "- **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: PyTorch + Transformers\n",
    "- **æ¨è«–**: äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼ˆè¿½åŠ è¨“ç·´ãªã—ï¼‰\n",
    "- **è©•ä¾¡**: MAP@3 (Mean Average Precision at 3)\n",
    "\n",
    "### âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–\n",
    "- ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªæ¨è«–\n",
    "- é©å¿œçš„ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆCPU: 4, GPU: 8ï¼‰\n",
    "- FP16ä½¿ç”¨ï¼ˆGPUç’°å¢ƒï¼‰\n",
    "- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼\n",
    "\n",
    "### ğŸ“ ä½¿ç”¨æ–¹æ³•\n",
    "1. Kaggleã§äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "2. MODEL_DATA_PATHã‚’æ­£ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã«å¤‰æ›´\n",
    "3. å…¨ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ\n",
    "4. `Gemma_2b_submission.csv`ãŒç”Ÿæˆã•ã‚Œã‚‹\n",
    "5. Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æå‡º\n",
    "\n",
    "### ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
    "- **ãƒ¡ãƒ¢ãƒªä¸è¶³**: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ä¸‹ã’ã‚‹\n",
    "- **ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã¨ãƒ‘ã‚¹ã‚’ç¢ºèª\n",
    "- **CUDA OOM**: CPUãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ\n",
    "\n",
    "**ğŸ† Good luck with your submission!**"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12957508,
     "isSourceIdPinned": false,
     "sourceId": 104383,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 404697,
     "modelInstanceId": 385485,
     "sourceId": 480357,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 101.086205,
   "end_time": "2025-07-21T09:43:54.171809",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-21T09:42:13.085604",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "09f0fae7fc254bf9b5d2d4af08fd4193": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2a76706e910148cf9d055304b954d051",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_0ed5f1e78f5b4f35a651b4e5d3cca425",
       "tabbable": null,
       "tooltip": null,
       "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
      }
     },
     "0ed5f1e78f5b4f35a651b4e5d3cca425": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2a76706e910148cf9d055304b954d051": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2be7eece0bc646a6a6fbf7be93ffdbd1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7a0b394e1bee44c8af2854ee1371c6cc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e38e43016354f0b8b0fcb1201572de8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2be7eece0bc646a6a6fbf7be93ffdbd1",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_896ca728e405431f89276c0a2e63bb60",
       "tabbable": null,
       "tooltip": null,
       "value": 3.0
      }
     },
     "896ca728e405431f89276c0a2e63bb60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "af4d8e5aaab34faca66dbd848168585b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "be1808ffd211462c8ff25902fcef3562": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7a0b394e1bee44c8af2854ee1371c6cc",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_af4d8e5aaab34faca66dbd848168585b",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡3/3â€‡[00:45&lt;00:00,â€‡12.75s/it]"
      }
     },
     "cb8c9616ab504e89a00294a06a82f9af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f8b2d2e344ae4d82af49f5c7c2d44afd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_09f0fae7fc254bf9b5d2d4af08fd4193",
        "IPY_MODEL_7e38e43016354f0b8b0fcb1201572de8",
        "IPY_MODEL_be1808ffd211462c8ff25902fcef3562"
       ],
       "layout": "IPY_MODEL_cb8c9616ab504e89a00294a06a82f9af",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
