#!/usr/bin/env python3
"""
VertexAIç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

MAPç«¶æŠ€ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ã€VertexAIã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«é©ã—ãŸå½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚
- ãƒ†ã‚­ã‚¹ãƒˆ: Question + Selected Answer + Student Explanation
- ãƒ©ãƒ™ãƒ«: 6ã¤ã®ã‚«ãƒ†ã‚´ãƒª (True/False_Correct/Neither/Misconception)
"""

import pandas as pd
import numpy as np
import os
from pathlib import Path


def create_enhanced_text(row):
    """è³ªå•ã€é¸æŠç­”ãˆã€å­¦ç”Ÿèª¬æ˜ã‚’çµ±åˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ"""
    question = str(row["QuestionText"]) if pd.notna(row["QuestionText"]) else ""
    mc_answer = str(row["MC_Answer"]) if pd.notna(row["MC_Answer"]) else ""
    explanation = (
        str(row["StudentExplanation"]) if pd.notna(row["StudentExplanation"]) else ""
    )

    # çµ±åˆãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
    enhanced_text = f"Question: {question} Selected Answer: {mc_answer} Student Explanation: {explanation}"
    return enhanced_text.strip()


def create_category_label(row):
    """Categoryã‚«ãƒ©ãƒ ã‹ã‚‰6ã¤ã®åŸºæœ¬ã‚«ãƒ†ã‚´ãƒªã‚’ä½œæˆ"""
    category = str(row["Category"])

    # 6ã¤ã®åŸºæœ¬ã‚«ãƒ†ã‚´ãƒªã«ãƒãƒƒãƒ”ãƒ³ã‚°
    if category == "True_Correct":
        return "True_Correct"
    elif category == "True_Neither":
        return "True_Neither"
    elif category == "True_Misconception":
        return "True_Misconception"
    elif category == "False_Correct":
        return "False_Correct"
    elif category == "False_Neither":
        return "False_Neither"
    elif category == "False_Misconception":
        return "False_Misconception"
    else:
        return "Unknown"


def prepare_vertex_dataset():
    """VertexAIç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™"""

    print("ğŸš€ VertexAIç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™é–‹å§‹")

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train_path = "../map_data/train.csv"
    if not os.path.exists(train_path):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {train_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False

    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {train_path}")
    df = pd.read_csv(train_path)
    print(f"   ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")

    # ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±è¡¨ç¤º
    print("\nğŸ“‹ ã‚«ãƒ©ãƒ æƒ…å ±:")
    for col in df.columns:
        print(f"   {col}: {df[col].dtype}, æ¬ æå€¤: {df[col].isnull().sum()}")

    # å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
    print("\nğŸ”§ å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆä½œæˆä¸­...")
    df["enhanced_text"] = df.apply(create_enhanced_text, axis=1)

    # ã‚«ãƒ†ã‚´ãƒªãƒ©ãƒ™ãƒ«ä½œæˆ
    print("ğŸ·ï¸ ã‚«ãƒ†ã‚´ãƒªãƒ©ãƒ™ãƒ«ä½œæˆä¸­...")
    df["category_label"] = df.apply(create_category_label, axis=1)

    # Unknown ã‚«ãƒ†ã‚´ãƒªã®ç¢ºèª
    unknown_count = (df["category_label"] == "Unknown").sum()
    if unknown_count > 0:
        print(f"âš ï¸ è­¦å‘Š: Unknown ã‚«ãƒ†ã‚´ãƒªãŒ {unknown_count} ä»¶ã‚ã‚Šã¾ã™")
        print("Unknown ã‚«ãƒ†ã‚´ãƒªã®å…ƒãƒ‡ãƒ¼ã‚¿:")
        print(df[df["category_label"] == "Unknown"]["Category"].value_counts())

    # Unknownã‚’é™¤å¤–
    df_clean = df[df["category_label"] != "Unknown"].copy()
    print(f"ğŸ“Š ã‚¯ãƒªãƒ¼ãƒ³å¾Œã®ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df_clean.shape}")

    # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒç¢ºèª
    print("\nğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ:")
    category_counts = df_clean["category_label"].value_counts()
    for category, count in category_counts.items():
        percentage = (count / len(df_clean)) * 100
        print(f"   {category}: {count:,} ({percentage:.1f}%)")

    # VertexAIç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    vertex_dataset = df_clean[["enhanced_text", "category_label"]].copy()
    vertex_dataset.columns = ["text", "label"]

    # ãƒ†ã‚­ã‚¹ãƒˆé•·ã®çµ±è¨ˆ
    text_lengths = vertex_dataset["text"].str.len()
    print(f"\nğŸ“ ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ:")
    print(f"   å¹³å‡: {text_lengths.mean():.1f} æ–‡å­—")
    print(f"   ä¸­å¤®å€¤: {text_lengths.median():.1f} æ–‡å­—")
    print(f"   æœ€å¤§: {text_lengths.max()} æ–‡å­—")
    print(f"   æœ€å°: {text_lengths.min()} æ–‡å­—")
    print(
        f"   512æ–‡å­—ä»¥ä¸‹: {(text_lengths <= 512).sum()} ({(text_lengths <= 512).mean()*100:.1f}%)"
    )

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜
    output_path = "vertex_training_dataset.csv"
    vertex_dataset.to_csv(output_path, index=False, encoding="utf-8")
    print(f"\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜å®Œäº†: {output_path}")
    print(f"   ä¿å­˜ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {vertex_dataset.shape}")

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
    print("\nğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:")
    for i, (_, row) in enumerate(vertex_dataset.head(3).iterrows()):
        print(f"\n--- ã‚µãƒ³ãƒ—ãƒ« {i+1} ---")
        print(f"ãƒ©ãƒ™ãƒ«: {row['label']}")
        print(f"ãƒ†ã‚­ã‚¹ãƒˆ: {row['text'][:200]}...")

    # è¨“ç·´/æ¤œè¨¼åˆ†å‰²ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚ä½œæˆ
    from sklearn.model_selection import train_test_split

    print("\nğŸ”„ è¨“ç·´/æ¤œè¨¼åˆ†å‰²ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­...")
    train_data, val_data = train_test_split(
        vertex_dataset, test_size=0.2, random_state=42, stratify=vertex_dataset["label"]
    )

    # åˆ†å‰²ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    train_data.to_csv("vertex_train_dataset.csv", index=False, encoding="utf-8")
    val_data.to_csv("vertex_val_dataset.csv", index=False, encoding="utf-8")

    print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train_data.shape} â†’ vertex_train_dataset.csv")
    print(f"   æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {val_data.shape} â†’ vertex_val_dataset.csv")

    print("\nâœ… VertexAIç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†ï¼")

    return True


def show_dataset_info():
    """ä½œæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æƒ…å ±ã‚’è¡¨ç¤º"""
    files = [
        "vertex_training_dataset.csv",
        "vertex_train_dataset.csv",
        "vertex_val_dataset.csv",
    ]

    print("\nğŸ“‚ ä½œæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
    for file in files:
        if os.path.exists(file):
            df = pd.read_csv(file)
            print(f"   {file}: {df.shape}")
            print(f"      ãƒ©ãƒ™ãƒ«æ•°: {df['label'].nunique()}")
            print(f"      ã‚µã‚¤ã‚º: {os.path.getsize(file) / 1024 / 1024:.2f} MB")


if __name__ == "__main__":
    # ãƒ•ã‚©ãƒ«ãƒ€ç¢ºèªãƒ»ä½œæˆ
    vertex_folder = Path(
        "C:/Users/mouse/Desktop/P/learnMachineLearning/kaggleã¯ã˜ã‚ã¦ã®ã‚³ãƒ³ãƒš/Vertex_AI_å®Ÿé¨“"
    )
    if not vertex_folder.exists():
        vertex_folder.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ: {vertex_folder}")

    # ãƒ•ã‚©ãƒ«ãƒ€ã«ç§»å‹•
    os.chdir(vertex_folder)
    print(f"ğŸ“ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Ÿè¡Œ
    success = prepare_vertex_dataset()

    if success:
        show_dataset_info()
        print("\nğŸ‰ ã™ã¹ã¦å®Œäº†ï¼VertexAIã§ã®å®Ÿé¨“æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚")
    else:
        print("\nâŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
