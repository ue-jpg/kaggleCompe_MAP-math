"""
MAP - Gemmaãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸ6åˆ†é¡NLPãƒ¢ãƒ‡ãƒ«ï¼ˆã‚³ãƒ³ãƒšå½¢å¼æº–æ‹ ï¼‰

ğŸ¯ Gemma 2Bå°‚ç”¨å®Ÿè£…
æ­£ã—ã„ã‚³ãƒ³ãƒšå‡ºåŠ›å½¢å¼:
- True_Correct, True_Neither, True_Misconception
- False_Correct, False_Neither, False_Misconception

MC_Answer ã®æ­£èª¤ Ã— StudentExplanation ã®åˆ†é¡ = 6ã‚¯ãƒ©ã‚¹åˆ†é¡

âš ï¸ æ³¨æ„: Gemmaãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™
"""

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
import warnings

warnings.filterwarnings("ignore")


class MathMisconceptionDataset(Dataset):
    """Math Misconception Dataset for PyTorch"""

    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt",
        )

        return {
            "input_ids": encoding["input_ids"].flatten(),
            "attention_mask": encoding["attention_mask"].flatten(),
            "labels": torch.tensor(label, dtype=torch.long),
        }


def load_and_prepare_data():
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ï¼ˆ6åˆ†é¡å½¢å¼ï¼‰"""
    print("=" * 60)
    print("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ï¼ˆ6åˆ†é¡å½¢å¼ï¼‰")
    print("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train_df = pd.read_csv("map_data/train.csv")
    test_df = pd.read_csv("map_data/test.csv")

    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train_df.shape}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test_df.shape}")

    # ã‚³ãƒ³ãƒšå½¢å¼ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆï¼ˆ6åˆ†é¡ï¼‰
    # Categoryåˆ—ãŒæ—¢ã«æ­£ã—ã„å½¢å¼ã«ãªã£ã¦ã„ã‚‹
    print("Categoryåˆ†å¸ƒ:")
    print(train_df["Category"].value_counts())

    # NaNå€¤é™¤å»
    before_len = len(train_df)
    train_df = train_df.dropna(subset=["Category", "StudentExplanation"])
    after_len = len(train_df)
    print(f"NaNé™¤å»: {before_len} -> {after_len} ({before_len - after_len}è¡Œå‰Šé™¤)")

    # ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆ
    def create_enhanced_text(row):
        """å¼·åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã‚’ä½œæˆ"""
        question = str(row["QuestionText"]) if pd.notna(row["QuestionText"]) else ""
        mc_answer = str(row["MC_Answer"]) if pd.notna(row["MC_Answer"]) else ""
        explanation = (
            str(row["StudentExplanation"])
            if pd.notna(row["StudentExplanation"])
            else ""
        )

        # è³ªå•ã€é¸æŠã•ã‚ŒãŸç­”ãˆã€èª¬æ˜ã‚’çµåˆ
        enhanced_text = f"Question: {question} Selected Answer: {mc_answer} Explanation: {explanation}"
        return enhanced_text

    train_df["enhanced_text"] = train_df.apply(create_enhanced_text, axis=1)
    test_df["enhanced_text"] = test_df.apply(create_enhanced_text, axis=1)

    # 6ã¤ã®ã‚«ãƒ†ã‚´ãƒªã®ç¢ºèª
    unique_categories = sorted(train_df["Category"].unique())
    print(f"\nãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚«ãƒ†ã‚´ãƒªæ•°: {len(unique_categories)}")
    print("ã‚«ãƒ†ã‚´ãƒªä¸€è¦§:")
    for i, cat in enumerate(unique_categories):
        count = (train_df["Category"] == cat).sum()
        print(f"  {i}: {cat} ({count}ä»¶)")

    return train_df, test_df


def prepare_model(num_labels=6, model_name="google/gemma-2b"):
    """ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ï¼ˆGemma 2Bä½¿ç”¨ï¼‰"""
    print(f"\n" + "=" * 60)
    print(f"Gemmaãƒ¢ãƒ‡ãƒ«æº–å‚™: {model_name}")
    print("=" * 60)

    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

    try:
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        print("Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)

        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆ6åˆ†é¡ç”¨ï¼‰
        print("Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=num_labels,
            torch_dtype=torch.float16 if device.type == "cuda" else torch.float32,
            device_map="auto" if device.type == "cuda" else None,
            low_cpu_mem_usage=True,
        )

        print(f"Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {num_labels}ã‚¯ãƒ©ã‚¹åˆ†é¡")
        print(f"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model.num_parameters():,}")

        return model, tokenizer, device

    except Exception as e:
        print(f"âŒ Gemmaãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        print("\nè€ƒãˆã‚‰ã‚Œã‚‹åŸå› :")
        print("1. Hugging Face Hub ã¸ã®èªè¨¼ãŒå¿…è¦")
        print("2. ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã®å•é¡Œ")
        print("3. ãƒ¡ãƒ¢ãƒªä¸è¶³")
        print("4. transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å•é¡Œ")
        print("\nè§£æ±ºæ–¹æ³•:")
        print("- huggingface-cli login ã§ãƒ­ã‚°ã‚¤ãƒ³")
        print("- ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šç¢ºèª")
        print("- ãƒ¡ãƒ¢ãƒªç©ºãå®¹é‡ç¢ºèª")
        print("\nãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        exit(1)


def compute_map3_metrics(eval_pred):
    """MAP@3ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
    predictions, labels = eval_pred
    predictions = torch.softmax(torch.tensor(predictions), dim=-1).numpy()

    map_scores = []
    for i, true_label in enumerate(labels):
        # ä¸Šä½3ã¤ã®äºˆæ¸¬ã‚’å–å¾—
        top3_indices = np.argsort(predictions[i])[::-1][:3]

        # MAPè¨ˆç®—
        score = 0.0
        for j, pred_idx in enumerate(top3_indices):
            if pred_idx == true_label:
                score = 1.0 / (j + 1)
                break
        map_scores.append(score)

    map3_score = np.mean(map_scores)
    accuracy = accuracy_score(labels, np.argmax(predictions, axis=1))

    return {"map3": map3_score, "accuracy": accuracy}


def fine_tune_model(train_df, model, tokenizer, device):
    """ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
    print(f"\n" + "=" * 60)
    print("6åˆ†é¡ãƒ¢ãƒ‡ãƒ« ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")
    print("=" * 60)

    # ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    label_encoder = LabelEncoder()
    train_df["encoded_labels"] = label_encoder.fit_transform(train_df["Category"])

    print(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«:")
    for i, label in enumerate(label_encoder.classes_):
        print(f"  {i}: {label}")

    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    train_texts = train_df["enhanced_text"].tolist()
    train_labels = train_df["encoded_labels"].tolist()

    # å°‘æ•°ã‚¯ãƒ©ã‚¹å¯¾ç­–ï¼šstratifyã‚’æ…é‡ã«é©ç”¨
    try:
        X_train, X_val, y_train, y_val = train_test_split(
            train_texts,
            train_labels,
            test_size=0.2,
            random_state=42,
            stratify=train_labels,
        )
        print("Stratified splité©ç”¨")
    except ValueError:
        X_train, X_val, y_train, y_val = train_test_split(
            train_texts, train_labels, test_size=0.2, random_state=42
        )
        print("Regular splité©ç”¨ï¼ˆå°‘æ•°ã‚¯ãƒ©ã‚¹ã®ãŸã‚ï¼‰")

    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}")
    print(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val)}")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆGemmaç”¨ï¼‰
    train_dataset = MathMisconceptionDataset(
        X_train, y_train, tokenizer, max_length=512
    )  # Gemmaã¯é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ´»ç”¨
    val_dataset = MathMisconceptionDataset(X_val, y_val, tokenizer, max_length=512)

    # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # è¨“ç·´è¨­å®šï¼ˆGemmaæœ€é©åŒ–ï¼‰
    training_args = TrainingArguments(
        output_dir="./gemma_6class_model",
        num_train_epochs=3,  # Gemmaã¯å°‘ãªã„ã‚¨ãƒãƒƒã‚¯ã§ã‚‚åŠ¹æœçš„
        per_device_train_batch_size=2,  # Gemmaã¯å¤§ããªãƒ¢ãƒ‡ãƒ«ãªã®ã§å°ãƒãƒƒãƒ
        per_device_eval_batch_size=2,
        warmup_steps=100,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=50,
        evaluation_strategy="steps",
        eval_steps=200,
        save_strategy="steps",
        save_steps=400,
        load_best_model_at_end=True,
        metric_for_best_model="map3",
        greater_is_better=True,
        report_to=None,
        dataloader_pin_memory=False,
        gradient_accumulation_steps=8,  # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º=16
        fp16=device.type == "cuda",  # GPUä½¿ç”¨æ™‚ã¯åŠç²¾åº¦
        optim="adamw_torch",
        learning_rate=2e-5,  # Gemmaç”¨ã®å­¦ç¿’ç‡
        save_total_limit=2,  # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç¯€ç´„
    )

    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®š
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_map3_metrics,
    )

    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
    print("ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...")
    trainer.train()

    # æœ€çµ‚è©•ä¾¡
    print("\næœ€çµ‚è©•ä¾¡:")
    eval_results = trainer.evaluate()
    for key, value in eval_results.items():
        print(f"{key}: {value:.4f}")

    return trainer, label_encoder


def generate_submission(trainer, tokenizer, test_df, label_encoder):
    """ã‚³ãƒ³ãƒšå½¢å¼ã®æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ"""
    print(f"\n" + "=" * 60)
    print("ã‚³ãƒ³ãƒšå½¢å¼æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ")
    print("=" * 60)

    test_texts = test_df["enhanced_text"].tolist()
    test_dataset = MathMisconceptionDataset(
        test_texts, [0] * len(test_texts), tokenizer, max_length=512  # Gemmaç”¨
    )

    # äºˆæ¸¬å®Ÿè¡Œ
    predictions = trainer.predict(test_dataset)

    # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é©ç”¨
    probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1).numpy()

    # å„ã‚µãƒ³ãƒ—ãƒ«ã§ä¸Šä½3ã¤ã®äºˆæ¸¬ã‚’å–å¾—
    submission_predictions = []
    for prob in probs:
        top3_indices = np.argsort(prob)[::-1][:3]
        top3_labels = [label_encoder.classes_[idx] for idx in top3_indices]
        submission_predictions.append(" ".join(top3_labels))

    # æå‡ºå½¢å¼ã®DataFrameä½œæˆ
    submission_df = pd.DataFrame(
        {
            "row_id": range(len(test_df)),
            "Category:Misconception": submission_predictions,
        }
    )

    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    print("æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚µãƒ³ãƒ—ãƒ«:")
    print(submission_df.head(10))

    print(f"äºˆæ¸¬å®Œäº†: {len(submission_df)}è¡Œ")
    return submission_df


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ¯ MAPç«¶æŠ€ - Gemma 6åˆ†é¡NLPãƒ¢ãƒ‡ãƒ«ï¼ˆã‚³ãƒ³ãƒšå½¢å¼æº–æ‹ ï¼‰")
    print("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    train_df, test_df = load_and_prepare_data()

    # ãƒ¢ãƒ‡ãƒ«æº–å‚™ï¼ˆ6åˆ†é¡ï¼‰
    model, tokenizer, device = prepare_model(num_labels=6)

    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    trainer, label_encoder = fine_tune_model(train_df, model, tokenizer, device)

    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
    submission_df = generate_submission(trainer, tokenizer, test_df, label_encoder)

    # çµæœä¿å­˜
    submission_df.to_csv("gemma_6class_submission.csv", index=False)
    print(f"\næå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: gemma_6class_submission.csv")

    print("\n" + "=" * 60)
    print("6åˆ†é¡ãƒ¢ãƒ‡ãƒ«é–‹ç™ºå®Œäº†ï¼")
    print("=" * 60)

    return trainer, label_encoder, submission_df


if __name__ == "__main__":
    print("ğŸš€ MAPç«¶æŠ€ - Gemma 6åˆ†é¡ãƒ¢ãƒ‡ãƒ«é–‹å§‹")
    print("=" * 60)

    # ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ãƒã‚§ãƒƒã‚¯
    print("ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ãƒã‚§ãƒƒã‚¯:")
    print(f"PyTorch: {torch.__version__}")
    print(f"CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(
            f"GPU ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB"
        )

    try:
        trainer, label_encoder, submission_df = main()
        print("\nğŸ‰ Gemmaãƒ¢ãƒ‡ãƒ«é–‹ç™ºå®Œäº†ï¼")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        exit(0)

    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("\nGemmaãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œã«ã¯ä»¥ä¸‹ãŒå¿…è¦ã§ã™:")
        print("1. ååˆ†ãªGPUãƒ¡ãƒ¢ãƒª (æ¨å¥¨: 8GBä»¥ä¸Š)")
        print("2. Hugging Face Hubèªè¨¼ (huggingface-cli login)")
        print("3. å®‰å®šã—ãŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶š")
        print("\nãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        exit(1)
