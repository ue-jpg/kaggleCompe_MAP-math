"""
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·åˆ†æ
"""

import pandas as pd


def analyze_prompt_length():
    # ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ©ãƒ™ãƒ«å–å¾—
    df = pd.read_csv("../map_data/train.csv")
    labels = (
        df["Category"].astype(str) + ":" + df["Misconception"].fillna("NA").astype(str)
    )
    all_labels = sorted(labels.unique())
    labels_text = "\n".join([f"- {label}" for label in all_labels])

    # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ–‡å­—æ•°
    sections = {
        "å½¹å‰²è¨­å®š": "You are an expert math educator analyzing student responses for mathematical misconceptions.",
        "å•é¡Œä¾‹": "Question: What is 1/2 + 1/3?\nCorrect Answer: 5/6\nStudent's Explanation: I added 1+1=2 and 2+3=5, so the answer is 2/5",
        "åˆ†é¡ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³": """CLASSIFICATION GUIDELINES:
â€¢ True_Correct:NA = Student demonstrates correct understanding
â€¢ False_Correct:NA = Student gives correct answer but for wrong reasons  
â€¢ True_Neither:NA = Correct answer but unclear/incomplete reasoning
â€¢ False_Neither:NA = Incorrect answer but no specific misconception identified
â€¢ True_Misconception:[Type] = Correct answer but demonstrates specific misconception
â€¢ False_Misconception:[Type] = Incorrect answer with identifiable misconception""",
        "ã‚¿ã‚¹ã‚¯èª¬æ˜": f"TASK: Classify this student's response using EXACTLY ONE of these {len(all_labels)} labels:",
        "ãƒ©ãƒ™ãƒ«ãƒªã‚¹ãƒˆ": labels_text,
        "å›ç­”ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ": "Classification:",
    }

    print("=== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥æ–‡å­—æ•°åˆ†æ ===")
    total_chars = 0
    for name, content in sections.items():
        chars = len(content)
        total_chars += chars
        print(f"{name}: {chars:,} æ–‡å­—")

    print(f"\nåˆè¨ˆæ–‡å­—æ•°: {total_chars:,} æ–‡å­—")
    print(f"æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°: {total_chars // 4:,} ãƒˆãƒ¼ã‚¯ãƒ³")
    print(f"ãƒ©ãƒ™ãƒ«ãƒªã‚¹ãƒˆã®å‰²åˆ: {len(labels_text)/total_chars*100:.1f}%")

    print("\n=== LLMãƒ¢ãƒ‡ãƒ«åˆ¥æ¨å¥¨é•· ===")
    print("â€¢ GPT-3.5/4: 1,000-2,000ãƒˆãƒ¼ã‚¯ãƒ³ãŒæœ€é©")
    print("â€¢ Claude: 1,500-3,000ãƒˆãƒ¼ã‚¯ãƒ³ãŒæœ€é©")
    print("â€¢ Gemma-2B: 500-1,000ãƒˆãƒ¼ã‚¯ãƒ³ãŒæ¨å¥¨")
    print("â€¢ ç¾åœ¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ç´„750ãƒˆãƒ¼ã‚¯ãƒ³ âœ…")

    print(f"\n=== ç¾åœ¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è©•ä¾¡ ===")
    token_count = total_chars // 4
    if token_count < 500:
        evaluation = "çŸ­ã„ - åŠ¹ç‡çš„ã ãŒæƒ…å ±ä¸è¶³ã®å¯èƒ½æ€§"
    elif token_count < 1000:
        evaluation = "é©åˆ‡ - ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ âœ…"
    elif token_count < 2000:
        evaluation = "ã‚„ã‚„é•·ã„ - è©³ç´°ã ãŒæ³¨æ„æ•£æ¼«ã®ãƒªã‚¹ã‚¯"
    else:
        evaluation = "é•·ã™ãã‚‹ - çŸ­ç¸®ã‚’æ¨å¥¨"

    print(f"è©•ä¾¡: {evaluation}")

    # æ”¹å–„ææ¡ˆ
    print(f"\n=== æ”¹å–„ææ¡ˆ ===")
    if len(labels_text) / total_chars > 0.7:
        print("âš ï¸  ãƒ©ãƒ™ãƒ«ãƒªã‚¹ãƒˆãŒå…¨ä½“ã®70%ä»¥ä¸Šã‚’å ã‚ã¦ã„ã¾ã™")
        print("ğŸ’¡ è§£æ±ºç­–:")
        print("   1. ãƒ©ãƒ™ãƒ«ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–")
        print("   2. ä¸»è¦ãƒ©ãƒ™ãƒ«ã®ã¿è¡¨ç¤ºã—ã€è©³ç´°ã¯åˆ¥é€”å‚ç…§")
        print("   3. éšå±¤çš„åˆ†é¡ï¼ˆã¾ãšå¤§åˆ†é¡ã€æ¬¡ã«è©³ç´°åˆ†é¡ï¼‰")
    else:
        print("âœ… ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹æˆã§ã™")


if __name__ == "__main__":
    analyze_prompt_length()
