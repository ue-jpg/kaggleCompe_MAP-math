#!/usr/bin/env pytho# è¨­å®š
MODEL_PATH = "./colab/colabã§è¨“ç·´ã—ã¦ä¿å­˜/kaggle-ready-model"  # çµ±åˆãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
DATA_PATH = "./map_data/train.csv"  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹
# -*- coding: utf-8 -*-
"""
Kaggle Ready Model Performance Evaluation
çµ±åˆãƒ¢ãƒ‡ãƒ«ï¼ˆkaggle-ready-modelï¼‰ã®æ€§èƒ½è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

train.csvã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦äºˆæ¸¬ã‚’è¡Œã„ã€å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«ã¨æ¯”è¼ƒã—ã¾ã™ã€‚
"""

import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import json
from pathlib import Path
import warnings

warnings.filterwarnings("ignore")

# è¨­å®š
MODEL_PATH = r"c:\Users\mouse\Desktop\NotDelete\GitHub\kaggleCompe_MAP-math\colab\colabã§è¨“ç·´ã—ã¦ä¿å­˜\kaggle-ready-model"  # çµ±åˆãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
DATA_PATH = r"c:\Users\mouse\Desktop\NotDelete\GitHub\kaggleCompe_MAP-math\map_data\train.csv"  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹
MAX_LENGTH = 512
BATCH_SIZE = 2  # ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ¡ãƒ¢ãƒªä¸è¶³å¯¾ç­–ã§å°ã•ãï¼‰

# ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆ6åˆ†é¡ï¼‰
CATEGORY_MAPPING = {
    0: "True",
    1: "False",
    2: "Correct",
    3: "Neither",
    4: "Misconception",
    5: "Unknown",  # äºˆå‚™ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã«ä¾å­˜ï¼‰
}


def create_enhanced_text(row):
    """Question + MC_Answer + Student Explanation ã‚’çµ±åˆ"""
    question = str(row["QuestionText"]) if pd.notna(row["QuestionText"]) else ""
    mc_answer = str(row["MC_Answer"]) if pd.notna(row["MC_Answer"]) else ""
    explanation = (
        str(row["StudentExplanation"]) if pd.notna(row["StudentExplanation"]) else ""
    )

    enhanced_text = (
        f"Question: {question} Selected Answer: {mc_answer} Explanation: {explanation}"
    )
    return enhanced_text


def load_model_and_tokenizer(model_path):
    """çµ±åˆãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿"""
    print("=" * 60)
    print("ğŸ¤– çµ±åˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿")
    print("=" * 60)

    try:
        # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®ç¢ºèª
        model_path = Path(model_path)
        if not model_path.exists():
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")

        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {model_path}")

        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
        print("ğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)

        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        print(f"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")
        print(f"ğŸ”– ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³: {tokenizer.pad_token}")

        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        print("ğŸ§  ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­ï¼ˆCPUå°‚ç”¨ã€ä½ãƒ¡ãƒ¢ãƒªãƒ¢ãƒ¼ãƒ‰ï¼‰...")
        model = AutoModelForSequenceClassification.from_pretrained(
            model_path,
            local_files_only=True,
            torch_dtype=torch.float32,  # CPUç”¨
            low_cpu_mem_usage=True,  # ä½ãƒ¡ãƒ¢ãƒªä½¿ç”¨ãƒ¢ãƒ¼ãƒ‰
        )

        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®šï¼ˆCPUã®ã¿ï¼‰
        device = torch.device("cpu")
        model.to(device)
        model.eval()

        print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
        print(f"ğŸ–¥ï¸ ãƒ‡ãƒã‚¤ã‚¹: {device}")
        print(f"ğŸ“Š åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°: {model.config.num_labels}")

        return model, tokenizer, device

    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return None, None, None


def load_and_prepare_data(data_path):
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†")
    print("=" * 60)

    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = pd.read_csv(data_path)
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {df.shape}")

        # NaNå€¤ã®é™¤å»
        before_len = len(df)
        df = df.dropna(subset=["Category", "StudentExplanation"])
        after_len = len(df)
        print(
            f"ğŸ§¹ NaNé™¤å»: {before_len} -> {after_len} ({before_len - after_len}è¡Œå‰Šé™¤)"
        )

        # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã®ç¢ºèª
        print("\nğŸ“‹ Categoryåˆ†å¸ƒ:")
        category_counts = df["Category"].value_counts()
        print(category_counts)

        # ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        unique_categories = sorted(df["Category"].unique())
        category_to_id = {cat: i for i, cat in enumerate(unique_categories)}
        id_to_category = {i: cat for cat, i in category_to_id.items()}

        df["label_id"] = df["Category"].map(category_to_id)

        print(f"\nğŸ“Š ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°:")
        for label_id, category in id_to_category.items():
            count = (df["label_id"] == label_id).sum()
            print(f"  {label_id}: {category} ({count:,}ä»¶)")

        # å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆ
        print("\nğŸ”§ å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆä¸­...")
        df["enhanced_text"] = df.apply(create_enhanced_text, axis=1)

        # ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ
        text_lengths = df["enhanced_text"].str.len()
        print(f"\nğŸ“ ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ:")
        print(f"  å¹³å‡: {text_lengths.mean():.1f} æ–‡å­—")
        print(f"  ä¸­å¤®å€¤: {text_lengths.median():.1f} æ–‡å­—")
        print(f"  æœ€å¤§: {text_lengths.max()} æ–‡å­—")
        print(
            f"  512æ–‡å­—ä»¥ä¸‹: {(text_lengths <= 512).sum()} ({(text_lengths <= 512).mean()*100:.1f}%)"
        )

        return df, id_to_category

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return None, None


def predict_batch(model, tokenizer, texts, device, max_length=512):
    """ãƒãƒƒãƒäºˆæ¸¬"""
    predictions = []
    probabilities = []

    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    encoded = tokenizer(
        texts, truncation=True, padding=True, max_length=max_length, return_tensors="pt"
    )

    # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
    input_ids = encoded["input_ids"].to(device)
    attention_mask = encoded["attention_mask"].to(device)

    # äºˆæ¸¬
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=-1)

        predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())
        probabilities.extend(probs.cpu().numpy())

    return predictions, probabilities


def evaluate_model(model, tokenizer, df, id_to_category, device):
    """ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡"""
    print("\n" + "=" * 60)
    print("ğŸ¯ ãƒ¢ãƒ‡ãƒ«è©•ä¾¡å®Ÿè¡Œ")
    print("=" * 60)

    texts = df["enhanced_text"].tolist()
    true_labels = df["label_id"].tolist()

    all_predictions = []
    all_probabilities = []

    # ãƒãƒƒãƒã”ã¨ã«äºˆæ¸¬
    print(f"ğŸ“Š ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(texts)}")
    print(f"ğŸ”„ ãƒãƒƒãƒã‚µã‚¤ã‚º: {BATCH_SIZE}")

    for i in range(0, len(texts), BATCH_SIZE):
        batch_texts = texts[i : i + BATCH_SIZE]
        batch_predictions, batch_probabilities = predict_batch(
            model, tokenizer, batch_texts, device, MAX_LENGTH
        )

        all_predictions.extend(batch_predictions)
        all_probabilities.extend(batch_probabilities)

        if (i // BATCH_SIZE + 1) % 10 == 0:
            print(f"  å‡¦ç†æ¸ˆã¿: {min(i + BATCH_SIZE, len(texts)):,} / {len(texts):,}")

    print("âœ… äºˆæ¸¬å®Œäº†")

    # ç²¾åº¦è¨ˆç®—
    accuracy = accuracy_score(true_labels, all_predictions)
    print(f"\nğŸ“ˆ å…¨ä½“ç²¾åº¦: {accuracy:.4f} ({accuracy*100:.2f}%)")

    # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
    print("\nğŸ“Š åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
    target_names = [id_to_category[i] for i in sorted(id_to_category.keys())]

    # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ãƒ©ãƒ™ãƒ«ã®ã¿ã‚’å–å¾—
    unique_labels = sorted(list(set(true_labels)))
    labels_present = [i for i in sorted(id_to_category.keys()) if i in unique_labels]
    target_names_present = [id_to_category[i] for i in labels_present]

    print(
        classification_report(
            true_labels,
            all_predictions,
            labels=labels_present,
            target_names=target_names_present,
            digits=4,
        )
    )

    # æ··åŒè¡Œåˆ—
    print("\nğŸ“‹ æ··åŒè¡Œåˆ—:")
    cm = confusion_matrix(true_labels, all_predictions, labels=labels_present)
    cm_df = pd.DataFrame(cm, index=target_names_present, columns=target_names_present)
    print(cm_df)

    return all_predictions, all_probabilities


def create_detailed_results(df, predictions, probabilities, id_to_category):
    """è©³ç´°ãªçµæœã®ä½œæˆ"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ è©³ç´°çµæœä½œæˆ")
    print("=" * 60)

    # çµæœãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    results_df = df.copy()
    results_df["predicted_label_id"] = predictions
    results_df["predicted_category"] = [id_to_category[pred] for pred in predictions]
    results_df["is_correct"] = (
        results_df["label_id"] == results_df["predicted_label_id"]
    )

    # äºˆæ¸¬ç¢ºç‡ã®è¿½åŠ 
    prob_array = np.array(probabilities)
    for i, category in id_to_category.items():
        results_df[f"prob_{category}"] = prob_array[:, i]

    # æœ€å¤§ç¢ºç‡ã®è¿½åŠ 
    results_df["max_probability"] = prob_array.max(axis=1)

    # çµæœçµ±è¨ˆ
    correct_count = results_df["is_correct"].sum()
    total_count = len(results_df)
    accuracy = correct_count / total_count

    print(f"âœ… è©³ç´°çµæœä½œæˆå®Œäº†")
    print(f"ğŸ“Š æ­£è§£æ•°: {correct_count:,} / {total_count:,} ({accuracy*100:.2f}%)")

    return results_df


def save_results(results_df, output_path="model_evaluation_results.csv"):
    """çµæœã®ä¿å­˜"""
    print(f"\nğŸ’¾ çµæœä¿å­˜: {output_path}")

    try:
        # ä¸»è¦ãªåˆ—ã‚’é¸æŠã—ã¦ä¿å­˜
        columns_to_save = [
            "QuestionId",
            "QuestionText",
            "MC_Answer",
            "StudentExplanation",
            "Category",
            "predicted_category",
            "is_correct",
            "max_probability",
        ]

        # ç¢ºç‡åˆ—ã‚‚è¿½åŠ 
        prob_columns = [col for col in results_df.columns if col.startswith("prob_")]
        columns_to_save.extend(prob_columns)

        # å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã‚’é¸æŠ
        available_columns = [
            col for col in columns_to_save if col in results_df.columns
        ]

        save_df = results_df[available_columns]
        save_df.to_csv(output_path, index=False, encoding="utf-8")

        print(f"âœ… çµæœä¿å­˜å®Œäº†: {len(save_df)} è¡Œ, {len(available_columns)} åˆ—")
        print(f"ğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")

        return True

    except Exception as e:
        print(f"âŒ çµæœä¿å­˜å¤±æ•—: {e}")
        return False


def show_sample_predictions(results_df, num_samples=10):
    """ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬çµæœã®è¡¨ç¤º"""
    print("\n" + "=" * 60)
    print("ğŸ” ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬çµæœ")
    print("=" * 60)

    # æ­£è§£ã¨ä¸æ­£è§£ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º
    correct_samples = results_df[results_df["is_correct"] == True].head(
        num_samples // 2
    )
    incorrect_samples = results_df[results_df["is_correct"] == False].head(
        num_samples // 2
    )

    print("âœ… æ­£è§£ã‚µãƒ³ãƒ—ãƒ«:")
    for idx, (_, row) in enumerate(correct_samples.iterrows()):
        print(f"\n[{idx+1}] QuestionId: {row.get('QuestionId', 'N/A')}")
        print(f"  å®Ÿéš›: {row['Category']} â†’ äºˆæ¸¬: {row['predicted_category']}")
        print(f"  ç¢ºç‡: {row['max_probability']:.3f}")
        print(f"  å­¦ç”Ÿèª¬æ˜: {row['StudentExplanation'][:100]}...")

    print("\nâŒ ä¸æ­£è§£ã‚µãƒ³ãƒ—ãƒ«:")
    for idx, (_, row) in enumerate(incorrect_samples.iterrows()):
        print(f"\n[{idx+1}] QuestionId: {row.get('QuestionId', 'N/A')}")
        print(f"  å®Ÿéš›: {row['Category']} â†’ äºˆæ¸¬: {row['predicted_category']}")
        print(f"  ç¢ºç‡: {row['max_probability']:.3f}")
        print(f"  å­¦ç”Ÿèª¬æ˜: {row['StudentExplanation'][:100]}...")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸš€ Kaggle Ready Model æ€§èƒ½è©•ä¾¡é–‹å§‹")
    print("=" * 80)

    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model, tokenizer, device = load_model_and_tokenizer(MODEL_PATH)
    if model is None:
        print("âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df, id_to_category = load_and_prepare_data(DATA_PATH)
    if df is None:
        print("âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return

    # ã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚ï¼‰
    sample_size = min(100, len(df))  # æœ€å¤§100ã‚µãƒ³ãƒ—ãƒ«ï¼ˆãƒ¡ãƒ¢ãƒªä¸è¶³å¯¾ç­–ï¼‰
    if len(df) > sample_size:
        print(f"\nâš ï¸ ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’åˆ¶é™: {len(df)} â†’ {sample_size}")
        df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)

    # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
    predictions, probabilities = evaluate_model(
        model, tokenizer, df, id_to_category, device
    )

    # è©³ç´°çµæœä½œæˆ
    results_df = create_detailed_results(df, predictions, probabilities, id_to_category)

    # çµæœä¿å­˜
    save_results(results_df)

    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    show_sample_predictions(results_df)

    print("\nğŸ‰ è©•ä¾¡å®Œäº†!")
    print("ğŸ“ è©³ç´°çµæœã¯ 'model_evaluation_results.csv' ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")


if __name__ == "__main__":
    main()
