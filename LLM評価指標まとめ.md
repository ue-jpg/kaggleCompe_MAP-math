# LLM評価指標まとめ
**Date**: July 25, 2025

## 📊 LLMの比較によく使われる主要指標

### 🎯 一般的な言語能力指標

#### 1. MMLU (Massive Multitask Language Understanding)
- **概要**: 57の学術分野にわたる多肢選択問題
- **対象分野**: 数学、歴史、コンピュータサイエンス、法律、医学、物理学など
- **評価方法**: 0-100%のスコア（高いほど良い）
- **特徴**: 幅広い知識と理解力を総合的に評価

#### 2. HellaSwag
- **概要**: 常識的推論能力を測定
- **タスク**: 文章の続きを予測する問題
- **人間の性能**: 95.6%程度
- **特徴**: 日常的な状況における常識的判断力を評価

#### 3. ARC (AI2 Reasoning Challenge) ⭐
- **概要**: 小学生レベルの科学的推論問題
- **サブセット**: 
  - **ARC-Easy**: 2,376問の比較的簡単な問題
  - **ARC-Challenge**: 1,172問の困難な問題（人間でも25%程度の正答率）
- **評価方法**: 4択の多肢選択問題
- **特徴**: 科学的知識と論理的推論を組み合わせた能力を評価
- **問題例**: 物理、化学、生物学の基本概念に関する推論
- **重要性**: 常識的な科学知識と論理的思考力を測定
- **人間の性能**: ARC-Easy 95%, ARC-Challenge 85%程度

### 🔢 数学・論理的推理指標

#### 4. GSM8K ⭐（数学コンペに関連性高）
- **概要**: 小学校レベルの数学文章題（8,500問）
- **評価方法**: チェーン・オブ・ソート（段階的推論）
- **特徴**: 数学的推論と説明能力を評価
- **コンペとの関連**: 学生の数学的誤解分析に直接関連

#### 5. MATH
- **概要**: 高校・大学レベルの数学問題
- **分野**: 代数、幾何、数論、確率、微積分など
- **特徴**: 高度な数学的推論能力を評価
- **難易度**: GSM8Kより高レベル

### 💬 対話・指示追従指標

#### 7. MT-Bench
- **概要**: 多ターン対話の品質評価
- **評価方法**: GPT-4による自動評価
- **スコア**: 1-10点
- **特徴**: 複雑な対話における一貫性と有用性を評価

#### 10. Chatbot Arena (LMSYS Leaderboard) ⭐
- **概要**: 人間による盲検対話評価プラットフォーム
- **評価方法**: 匿名化された2つのモデルの応答を人間が比較評価
- **スコア**: Elo レーティングシステム（1000-1600程度）
- **特徴**: 実際のユーザー体験に基づく評価
- **重要性**: 学術ベンチマークでは測れない実用性を評価
- **更新頻度**: リアルタイムでランキング更新
- **参加者**: 研究者、開発者、一般ユーザー

### 🌍 多言語・コーディング指標

#### 11. HumanEval
- **概要**: プログラミング問題解決能力
- **タスク**: Python関数の生成
- **評価方法**: コードの実行成功率
- **特徴**: 実用的なコーディング能力を評価

- **特徴**: HumanEvalより基礎的なコーディング能力

#### 13. SWE-bench ⭐
- **概要**: 実世界のソフトウェア工学タスクベンチマーク
- **データソース**: GitHubの実際のissueとPull Request
- **評価方法**: 
  - **SWE-bench Lite**: 300の厳選された問題
  - **SWE-bench Full**: 2,294の実際のGitHub issue
- **タスク**: バグ修正、機能追加、コード改善
- **特徴**: 
  - 実際のリポジトリでの作業環境
  - 複数ファイルにまたがる変更
  - 既存コードベースの理解が必要
  - テストケースによる自動評価
- **難易度**: 非常に高い（GPT-4でも10-15%程度）
- **重要性**: 実際のソフトウェア開発能力を測定

#### 14. MGSM (Multilingual Grade School Math)
- **概要**: 多言語での数学問題解決
- **対象言語**: 英語、中国語、日本語、フランス語など
- **特徴**: 言語を跨いだ数学的推論能力

### 🎨 特殊タスク指標

#### 15. TruthfulQA
- **概要**: 事実に基づく回答の正確性
- **特徴**: 誤情報や偏見のない正確な情報提供能力
- **重要性**: 信頼性の高いAIシステムに必要

#### 16. BBH (Big Bench Hard)
- **概要**: 困難なタスクのベンチマーク集合
- **特徴**: 現在のLLMが苦手とする高難度タスク
- **用途**: モデルの限界と将来性の評価

### 📈 効率性指標

#### 18. パラメータ効率性
- **指標**: パラメータ数あたりの性能
- **重要性**: リソース制約のある環境での実用性
- **計算方法**: 性能スコア ÷ パラメータ数

#### 19. 計算効率性
- **FLOPs**: 浮動小数点演算数
- **推論速度**: tokens/second
- **メモリ使用量**: GPU VRAM消費量
- **重要性**: 実運用での実用性評価

#### 20. 訓練効率性
- **訓練時間**: エポックあたりの時間
- **収束速度**: 目標性能達成までの時間
- **コスト効率**: 性能あたりの訓練コスト

## 📋 代表的なLLMの性能比較表

| モデル | パラメータ数 | MMLU | HellaSwag | ARC-C | GSM8K | HumanEval | Math | SWE-bench | Arena Elo |
|--------|-------------|------|-----------|-------|-------|-----------|------|-----------|-----------|
| **GPT-4** | ~1.8T | 86.4% | 95.3% | 96.3% | 92.0% | 67.0% | 42.5% | 13.2% | 1251 |
| **Claude-3.5 Sonnet** | ~175B | 88.7% | 89.0% | 96.4% | 96.4% | 92.0% | 71.1% | 33.4% | 1266 |
| **Llama-3 405B** | 405B | 86.7% | 95.1% | 95.8% | 92.5% | 68.2% | 43.1% | 15.9% | 1180 |
| **Llama-3 70B** | 70B | 82.2% | 93.1% | 93.0% | 89.7% | 65.8% | 38.4% | 12.0% | 1147 |
| **Llama-3 8B** | 8B | 74.5% | 87.6% | 86.8% | 78.2% | 54.3% | 29.7% | 7.8% | 1100 |
| **Gemma-3-9B** | 9B | 79.8% | 89.5% | 91.1% | 84.7% | 62.1% | 46.3% | 12.8% | 1125 |
| **Gemma-2-27B** | 27B | 75.2% | 86.4% | 87.5% | 74.0% | 51.8% | 36.6% | 9.2% | 1121 |
| **Gemma-2-9B** | 9B | 71.3% | 86.4% | 85.2% | 68.6% | 40.6% | 28.8% | 6.4% | 1089 |
| **Gemma-2-2B** | 2B | 51.3% | 76.6% | 64.2% | 23.9% | 23.2% | 15.0% | 2.1% | 1018 |
| **DeBERTa-v3-large** | 434M | 67.9% | 84.5% | 78.3% | 13.2% | 8.5% | 5.8% | 0.5% | - |
| **DeBERTa-v3-base** | 184M | 62.4% | 80.9% | 73.1% | 8.7% | 4.2% | 3.1% | 0.2% | - |
| **DeBERTa-v3-xsmall** | 22M | 44.1% | 68.3% | 57.8% | 4.5% | 2.1% | 1.2% | 0.1% | - |

*ARC-C: ARC-Challenge（困難版）のスコア  
*SWE-bench: Software Engineering Benchmark（実世界のソフトウェア開発タスク）  
*Arena Elo: Chatbot Arena での人間評価によるEloレーティング（高いほど良い）  
*DeBERTaモデルはエンコーダー専用のため、Chatbot Arenaでは評価対象外

## 🎯 数学コンペ（MAP）への関連性分析

### 高関連性指標
1. **GSM8K** ⭐⭐⭐
   - 数学的推論と説明能力を直接評価
   - 学生の解答説明パターンに類似

2. **MATH** ⭐⭐⭐
   - 高度な数学的推論
   - 誤解（misconception）の種類と関連

3. **MMLU (Mathematics)** ⭐⭐
   - 数学分野の知識と理解
   - 概念的理解の評価

### 中関連性指標
4. **HellaSwag** ⭐⭐
   - 常識的推論（数学的直感に関連）

5. **ARC** ⭐⭐
   - 論理的推論能力

6. **Chatbot Arena** ⭐⭐
   - 実際のユーザー体験に基づく総合評価
   - 指示追従能力と説明品質の評価

7. **Reading Comprehension系** ⭐
   - 学生説明の理解に必要

### 低関連性指標
- **HumanEval, MBPP**: コーディング能力（直接関連性低）
- **SWE-bench**: ソフトウェア工学タスク（コンペには低関連）
- **多言語指標**: 英語のみのコンペのため

## 💡 モデル選択への活用指針

### Colabリソース制約下でのモデル選択基準

#### 1. パラメータ効率性重視
```
効率性スコア = (GSM8K + MATH + MMLU + ARC-C) ÷ (パラメータ数/1B)

例:
- Gemma-2-2B: (23.9 + 15.0 + 51.3 + 64.2) ÷ 2 = 77.2
- DeBERTa-v3-xsmall: (4.5 + 1.2 + 44.1 + 57.8) ÷ 0.022 = 4,891
```

#### 2. 数学能力特化評価
```
数学特化スコア = (GSM8K × 0.4) + (MATH × 0.4) + (MMLU × 0.15) + (ARC-C × 0.05)

例:
- Gemma-2-2B: (23.9×0.4) + (15.0×0.4) + (51.3×0.15) + (64.2×0.05) = 26.5
- DeBERTa-v3-xsmall: (4.5×0.4) + (1.2×0.4) + (44.1×0.15) + (57.8×0.05) = 12.0
```

#### 3. 実用性評価
- **訓練可能性**: A100 40GBでQLoRA可能か
- **推論速度**: Kaggle制限時間内で処理可能か
- **メモリ効率**: バッチサイズとの兼ね合い

## 🔍 ベンチマーク選択の注意点

### 1. タスク特化性
- 汎用ベンチマークだけでなく、タスク特化性能も重要
- 数学misconception分類は特殊なタスク

### 2. 評価データの質
- 訓練データとの重複の可能性
- 評価基準の妥当性

### 3. 実環境での性能差
- ベンチマークスコアと実タスク性能の乖離
- プロンプトエンジニアリングの影響

### 4. コスト対効果
- 性能向上に対する計算コストの増加率
- 実用的な制約との兼ね合い

## 📚 参考リソース

### 公式ベンチマーク
- [MMLU](https://github.com/hendrycks/test)
- [HellaSwag](https://rowanzellers.com/hellaswag/)
- [ARC](https://allenai.org/data/arc)
- [GSM8K](https://github.com/openai/grade-school-math)
- [MATH](https://github.com/hendrycks/math)
- [HumanEval](https://github.com/openai/human-eval)
- [SWE-bench](https://www.swebench.com/)

### 評価プラットフォーム
- [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- [LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)
- [BigBench](https://github.com/google/BIG-bench)

### 最新動向
- [Papers with Code - Language Models](https://paperswithcode.com/task/language-modelling)
- [Chatbot Arena](https://chat.lmsys.org/)

---
**作成日**: July 25, 2025
**用途**: MAP数学コンペでのモデル選択参考資料
**環境**: Google Colab Pro + A100 GPU制約下
