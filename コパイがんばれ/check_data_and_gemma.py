"""
MAPç«¶æŠ€ - ãƒ‡ãƒ¼ã‚¿ç¢ºèªã¨Gemmaãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ç›®çš„:
1. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®åˆ†å¸ƒç¢ºèª
2. å®Ÿéš›ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆä¾‹ã®è¡¨ç¤º
3. Gemmaãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
4. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç¢ºèª

å®Ÿéš›ã®è¨“ç·´å‰ã®äº‹å‰ç¢ºèªç”¨
"""

import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import warnings

warnings.filterwarnings("ignore")


def check_data_overview():
    """ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®ç¢ºèª"""
    print("ğŸ” ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ç¢ºèª")
    print("=" * 60)

    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        train_df = pd.read_csv("map_data/train.csv")
        test_df = pd.read_csv("map_data/test.csv")
        sample_submission = pd.read_csv("map_data/sample_submission.csv")

        print(f"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train_df.shape}")
        print(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test_df.shape}")
        print(f"âœ… ã‚µãƒ³ãƒ—ãƒ«æå‡º: {sample_submission.shape}")

        # åŸºæœ¬çµ±è¨ˆ
        print(f"\nğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
        print(f"   ç·å­¦ç”Ÿå›ç­”æ•°: {len(train_df):,}")
        print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯è³ªå•æ•°: {train_df['QuestionId'].nunique()}")

        # ã‚«ãƒ©ãƒ æƒ…å ±
        print(f"\nğŸ“‹ ã‚«ãƒ©ãƒ æƒ…å ±:")
        for col in train_df.columns:
            non_null = train_df[col].notna().sum()
            print(
                f"   {col}: {non_null:,}/{len(train_df):,} ({non_null/len(train_df)*100:.1f}%)"
            )

        return train_df, test_df, sample_submission

    except FileNotFoundError as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
        print("map_data/ãƒ•ã‚©ãƒ«ãƒ€ã«ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
        print("- train.csv")
        print("- test.csv")
        print("- sample_submission.csv")
        return None, None, None
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None, None, None


def analyze_target_variables(train_df):
    """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®è©³ç´°åˆ†æ"""
    if train_df is None:
        return

    print("\nğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°åˆ†æ")
    print("=" * 60)

    # Categoryåˆ†æ
    print("ğŸ“Š Categoryåˆ†å¸ƒ:")
    category_dist = train_df["Category"].value_counts()
    for cat, count in category_dist.items():
        percentage = count / len(train_df) * 100
        print(f"   {cat}: {count:,}ä»¶ ({percentage:.1f}%)")

    # Misconceptionåˆ†æ
    print(f"\nğŸ“Š Misconceptionåˆ†æ:")
    misconception_dist = train_df["Misconception"].value_counts()
    print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªèª¤æ¦‚å¿µæ•°: {len(misconception_dist)}")
    print(f"   NAä»¥å¤–ã®èª¤æ¦‚å¿µ: {(train_df['Misconception'] != 'NA').sum():,}ä»¶")

    print("\n   ä¸Šä½10ã®èª¤æ¦‚å¿µ:")
    for i, (misc, count) in enumerate(misconception_dist.head(10).items(), 1):
        percentage = count / len(train_df) * 100
        print(f"   {i:2d}. {misc}: {count:,}ä»¶ ({percentage:.1f}%)")

    # 6åˆ†é¡ã®æœ€çµ‚ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
    print(f"\nğŸ¯ 6åˆ†é¡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆã‚³ãƒ³ãƒšå½¢å¼ï¼‰:")
    print("   æœŸå¾…ã•ã‚Œã‚‹ã‚«ãƒ†ã‚´ãƒª:")
    expected_categories = [
        "True_Correct",
        "True_Neither",
        "True_Misconception",
        "False_Correct",
        "False_Neither",
        "False_Misconception",
    ]

    actual_categories = sorted(train_df["Category"].unique())
    print(f"   å®Ÿéš›ã®ã‚«ãƒ†ã‚´ãƒªæ•°: {len(actual_categories)}")

    for i, cat in enumerate(actual_categories):
        count = (train_df["Category"] == cat).sum()
        percentage = count / len(train_df) * 100
        status = "âœ…" if cat in expected_categories else "âš ï¸"
        print(f"   {i}: {status} {cat}: {count:,}ä»¶ ({percentage:.1f}%)")


def show_text_examples(train_df, test_df):
    """å®Ÿéš›ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆä¾‹ã‚’è¡¨ç¤º"""
    if train_df is None or test_df is None:
        return

    print("\nğŸ“ å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆä¾‹")
    print("=" * 60)

    def create_enhanced_text(row):
        """å¼·åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã‚’ä½œæˆ"""
        question = str(row["QuestionText"]) if pd.notna(row["QuestionText"]) else ""
        mc_answer = str(row["MC_Answer"]) if pd.notna(row["MC_Answer"]) else ""
        explanation = (
            str(row["StudentExplanation"])
            if pd.notna(row["StudentExplanation"])
            else ""
        )

        enhanced_text = f"Question: {question} Selected Answer: {mc_answer} Explanation: {explanation}"
        return enhanced_text

    # å„ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰1ä¾‹ãšã¤è¡¨ç¤º
    categories = train_df["Category"].unique()

    for i, category in enumerate(categories[:3]):  # æœ€åˆã®3ã‚«ãƒ†ã‚´ãƒª
        print(f"\nğŸ“‹ ä¾‹ {i+1}: ã‚«ãƒ†ã‚´ãƒª = {category}")
        print("-" * 40)

        sample = train_df[train_df["Category"] == category].iloc[0]
        enhanced_text = create_enhanced_text(sample)

        print(f"   è³ªå•ID: {sample['QuestionId']}")
        print(f"   é¸æŠç­”ãˆ: {sample['MC_Answer']}")
        print(f"   ã‚«ãƒ†ã‚´ãƒª: {sample['Category']}")
        print(f"   èª¤æ¦‚å¿µ: {sample['Misconception']}")
        print(f"\n   ğŸ“„ å®Œå…¨ãªå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ:")
        print(f"   {enhanced_text}")
        print(f"\n   ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(enhanced_text)}æ–‡å­—")

    # ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ
    train_df["enhanced_text"] = train_df.apply(create_enhanced_text, axis=1)
    text_lengths = train_df["enhanced_text"].str.len()

    print(f"\nğŸ“Š ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ:")
    print(f"   å¹³å‡: {text_lengths.mean():.1f}æ–‡å­—")
    print(f"   ä¸­å¤®å€¤: {text_lengths.median():.1f}æ–‡å­—")
    print(f"   æœ€å°: {text_lengths.min()}æ–‡å­—")
    print(f"   æœ€å¤§: {text_lengths.max()}æ–‡å­—")
    print(
        f"   512æ–‡å­—è¶…é: {(text_lengths > 512).sum():,}ä»¶ ({(text_lengths > 512).mean()*100:.1f}%)"
    )


def test_gemma_loading():
    """Gemmaãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ¤– Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)

    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    print("ğŸ’» ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
    print(f"   PyTorch: {torch.__version__}")
    print(f"   CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")

    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        total_memory = torch.cuda.get_device_properties(0).total_memory
        print(f"   GPUç·ãƒ¡ãƒ¢ãƒª: {total_memory / 1e9:.1f} GB")

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
        torch.cuda.empty_cache()
        allocated = torch.cuda.memory_allocated(0)
        cached = torch.cuda.memory_reserved(0)
        print(f"   GPUä½¿ç”¨ãƒ¡ãƒ¢ãƒª: {allocated / 1e9:.2f} GB")
        print(f"   GPUã‚­ãƒ£ãƒƒã‚·ãƒ¥: {cached / 1e9:.2f} GB")

    # Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ”¤ Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ†ã‚¹ãƒˆ:")
    try:
        print("   ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...")
        tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b")

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        print("   âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿æˆåŠŸ")
        print(f"   èªå½™ã‚µã‚¤ã‚º: {len(tokenizer):,}")

        # ãƒ†ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        test_text = "Question: What is 2+2? Selected Answer: 4 Explanation: I added 2 and 2 to get 4."
        tokens = tokenizer(
            test_text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512,
        )
        print(f"   ãƒ†ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–: {tokens['input_ids'].shape[1]}ãƒˆãƒ¼ã‚¯ãƒ³")

        return True, tokenizer

    except Exception as e:
        print(f"   âŒ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return False, None


def test_gemma_model(tokenizer_success, tokenizer):
    """Gemmaãƒ¢ãƒ‡ãƒ«æœ¬ä½“ã®ãƒ†ã‚¹ãƒˆ"""
    if not tokenizer_success:
        print("\nâŒ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å¤±æ•—ã®ãŸã‚ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return False

    print(f"\nğŸ§  Gemmaãƒ¢ãƒ‡ãƒ«æœ¬ä½“ãƒ†ã‚¹ãƒˆ:")

    try:
        print("   ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        model = AutoModelForSequenceClassification.from_pretrained(
            "google/gemma-2b",
            num_labels=6,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            low_cpu_mem_usage=True,
        )

        print("   âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
        print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model.num_parameters():,}")

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆGPUä½¿ç”¨æ™‚ï¼‰
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0)
            print(f"   ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¾ŒGPUä½¿ç”¨é‡: {allocated / 1e9:.2f} GB")

        # ç°¡å˜ãªæ¨è«–ãƒ†ã‚¹ãƒˆ
        print("   æ¨è«–ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        test_text = "Question: What is 2+2? Selected Answer: 4 Explanation: I added 2 and 2 to get 4."
        inputs = tokenizer(
            test_text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512,
        )

        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.softmax(outputs.logits, dim=-1)

        print(f"   âœ… æ¨è«–ãƒ†ã‚¹ãƒˆæˆåŠŸ")
        print(f"   å‡ºåŠ›å½¢çŠ¶: {predictions.shape}")
        print(f"   äºˆæ¸¬ç¢ºç‡ä¾‹: {predictions[0][:3].tolist()}")

        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        del model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return True

    except Exception as e:
        print(f"   âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        print(f"\nğŸ’¡ è§£æ±ºç­–:")
        print(f"   1. Hugging Faceèªè¨¼: huggingface-cli login")
        print(f"   2. ãƒ¡ãƒ¢ãƒªç¢ºä¿: ã‚ˆã‚Šå°ã•ãªãƒãƒƒãƒã‚µã‚¤ã‚ºä½¿ç”¨")
        print(f"   3. ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šç¢ºèª")
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ” MAPç«¶æŠ€ - ãƒ‡ãƒ¼ã‚¿ç¢ºèªã¨Gemmaãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    print("å®Ÿéš›ã®è¨“ç·´å‰ã®äº‹å‰ç¢ºèªã‚’è¡Œã„ã¾ã™\n")

    # 1. ãƒ‡ãƒ¼ã‚¿ç¢ºèª
    train_df, test_df, sample_submission = check_data_overview()

    if train_df is not None:
        # 2. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°åˆ†æ
        analyze_target_variables(train_df)

        # 3. ãƒ†ã‚­ã‚¹ãƒˆä¾‹è¡¨ç¤º
        show_text_examples(train_df, test_df)

    # 4. Gemmaãƒ†ã‚¹ãƒˆ
    tokenizer_success, tokenizer = test_gemma_loading()

    if tokenizer_success:
        model_success = test_gemma_model(tokenizer_success, tokenizer)
    else:
        model_success = False

    # 5. æœ€çµ‚çµæœ
    print(f"\nğŸ äº‹å‰ç¢ºèªçµæœ")
    print("=" * 60)
    print(f"   ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {'âœ…' if train_df is not None else 'âŒ'}")
    print(f"   Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼: {'âœ…' if tokenizer_success else 'âŒ'}")
    print(f"   Gemmaãƒ¢ãƒ‡ãƒ«: {'âœ…' if model_success else 'âŒ'}")

    if train_df is not None and tokenizer_success and model_success:
        print(f"\nğŸ‰ å…¨ã¦ã®äº‹å‰ç¢ºèªãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"   map_6class_model.py ã®å®Ÿè¡Œæº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚")
    else:
        print(f"\nâš ï¸  ã„ãã¤ã‹ã®å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚è§£æ±ºã—ã¦ã‹ã‚‰æœ¬å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

    return train_df is not None, tokenizer_success, model_success


if __name__ == "__main__":
    data_ok, tokenizer_ok, model_ok = main()
