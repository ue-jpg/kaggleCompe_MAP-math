{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e923ddc8",
   "metadata": {},
   "source": [
    "# 🎯 MAP Competition: DeBERTa-v3-xsmall for 6-Class Text Classification\n",
    "\n",
    "## Overview\n",
    "このノートブックは、数学的誤解（Math Misconception）分類コンペティション用のDeBERTa-v3-xsmallモデルを実装しています。\n",
    "\n",
    "### Target Classes (6分類)\n",
    "- **True_Correct**: 正解で正しい説明\n",
    "- **True_Neither**: 正解だが曖昧な説明\n",
    "- **True_Misconception**: 正解だが誤った概念の説明\n",
    "- **False_Correct**: 不正解だが正しい概念の説明\n",
    "- **False_Neither**: 不正解で曖昧な説明\n",
    "- **False_Misconception**: 不正解で誤った概念の説明\n",
    "\n",
    "### Model Architecture\n",
    "- **Base Model**: microsoft/deberta-v3-xsmall\n",
    "- **Parameters**: ~70M\n",
    "- **Max Length**: 512 tokens\n",
    "- **Evaluation Metric**: MAP@3\n",
    "\n",
    "### Competition Format\n",
    "- Input: QuestionText + MC_Answer + StudentExplanation\n",
    "- Output: Top 3 predicted categories (space-separated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6716e92",
   "metadata": {},
   "source": [
    "## 📦 Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dcd4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle環境で必要なライブラリをインストール\n",
    "# Kaggleでは通常、transformersとaccelerateが必要\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"パッケージをインストールする関数\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# 必要なライブラリのインストール\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"✅ transformers already installed: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"📦 Installing transformers...\")\n",
    "    install_package(\"transformers\")\n",
    "\n",
    "try:\n",
    "    import accelerate\n",
    "    print(f\"✅ accelerate already installed: {accelerate.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"📦 Installing accelerate...\")\n",
    "    install_package(\"accelerate>=0.26.0\")\n",
    "\n",
    "try:\n",
    "    import sentencepiece\n",
    "    print(f\"✅ sentencepiece already installed: {sentencepiece.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"📦 Installing sentencepiece...\")\n",
    "    install_package(\"sentencepiece\")\n",
    "\n",
    "print(\"🎉 All required libraries are ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a77d6",
   "metadata": {},
   "source": [
    "## 📚 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00896bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本ライブラリ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# 機械学習ライブラリ\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformersライブラリ\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "# Scikit-learnライブラリ\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 警告を非表示\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# デバッグ情報表示\n",
    "print(\"🔧 Environment Information:\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "# Kaggleのデータパス設定\n",
    "KAGGLE_INPUT_PATH = \"/kaggle/input\"\n",
    "if os.path.exists(KAGGLE_INPUT_PATH):\n",
    "    print(f\"📁 Kaggle environment detected: {KAGGLE_INPUT_PATH}\")\n",
    "    # MAP - Charting Student Math Misunderstandings コンペティション\n",
    "    DATA_PATH = \"/kaggle/input/map-charting-student-math-misunderstandings\"\n",
    "    print(f\"🎯 Competition data path: {DATA_PATH}\")\n",
    "else:\n",
    "    print(\"📁 Local environment detected\")\n",
    "    DATA_PATH = \".\"\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede0e958",
   "metadata": {},
   "source": [
    "## 🔧 Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d035a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathMisconceptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Math Misconception Dataset for PyTorch\n",
    "    \n",
    "    6分類タスク用のカスタムデータセット:\n",
    "    - True_Correct, True_Neither, True_Misconception\n",
    "    - False_Correct, False_Neither, False_Misconception\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list): テキストデータのリスト\n",
    "            labels (list): ラベルデータのリスト\n",
    "            tokenizer: DeBERTaトークナイザー\n",
    "            max_length (int): 最大トークン長\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # DeBERTaトークナイザーでテキストをエンコード\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "print(\"✅ MathMisconceptionDataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50be3bf",
   "metadata": {},
   "source": [
    "## 📊 Load and Prepare Competition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e746b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"コンペデータの読み込みと前処理（6分類形式）\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"📊 データ読み込みと前処理（6分類形式）\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Kaggleコンペティション: MAP - Charting Student Math Misunderstandings\n",
    "    # 正確なデータパス: /kaggle/input/map-charting-student-math-misunderstandings/\n",
    "    try:\n",
    "        # Kaggle環境でのパス\n",
    "        if os.path.exists(\"/kaggle/input\"):\n",
    "            # Kaggle環境\n",
    "            kaggle_data_path = \"/kaggle/input/map-charting-student-math-misunderstandings\"\n",
    "            train_path = f\"{kaggle_data_path}/train.csv\"\n",
    "            test_path = f\"{kaggle_data_path}/test.csv\"\n",
    "        else:\n",
    "            # ローカル環境 - map_dataフォルダを使用\n",
    "            train_path = f\"{DATA_PATH}/map_data/train.csv\"\n",
    "            test_path = f\"{DATA_PATH}/map_data/test.csv\"\n",
    "            \n",
    "        print(f\"📁 使用するパス:\")\n",
    "        print(f\"   Train: {train_path}\")\n",
    "        print(f\"   Test: {test_path}\")\n",
    "            \n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        \n",
    "        print(f\"✅ データ読み込み成功!\")\n",
    "        print(f\"📁 Train path: {train_path}\")\n",
    "        print(f\"📁 Test path: {test_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ データ読み込みエラー: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"📈 訓練データ形状: {train_df.shape}\")\n",
    "    print(f\"📈 テストデータ形状: {test_df.shape}\")\n",
    "    \n",
    "    # データ確認\n",
    "    print(\"\\n📋 訓練データの列:\")\n",
    "    print(train_df.columns.tolist())\n",
    "    \n",
    "    print(\"\\n📋 Category分布:\")\n",
    "    if 'Category' in train_df.columns:\n",
    "        print(train_df[\"Category\"].value_counts())\n",
    "    else:\n",
    "        print(\"❌ 'Category'列が見つかりません\")\n",
    "        return None, None\n",
    "\n",
    "    # NaN値の確認と除去\n",
    "    print(f\"\\n🧹 データクリーニング:\")\n",
    "    before_len = len(train_df)\n",
    "    \n",
    "    # 必要な列の存在確認\n",
    "    required_cols = [\"Category\", \"StudentExplanation\"]\n",
    "    missing_cols = [col for col in required_cols if col not in train_df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"❌ 必要な列が不足: {missing_cols}\")\n",
    "        return None, None\n",
    "    \n",
    "    train_df = train_df.dropna(subset=required_cols)\n",
    "    after_len = len(train_df)\n",
    "    print(f\"NaN除去: {before_len} -> {after_len} ({before_len - after_len}行削除)\")\n",
    "\n",
    "    # 強化されたテキスト特徴量の作成\n",
    "    def create_enhanced_text(row):\n",
    "        \"\"\"Question + MC_Answer + Explanation を結合した強化テキスト\"\"\"\n",
    "        question = str(row.get(\"QuestionText\", \"\")) if pd.notna(row.get(\"QuestionText\")) else \"\"\n",
    "        mc_answer = str(row.get(\"MC_Answer\", \"\")) if pd.notna(row.get(\"MC_Answer\")) else \"\"\n",
    "        explanation = str(row.get(\"StudentExplanation\", \"\")) if pd.notna(row.get(\"StudentExplanation\")) else \"\"\n",
    "        \n",
    "        # DeBERTa用の構造化テキスト\n",
    "        enhanced_text = f\"Question: {question} Selected Answer: {mc_answer} Explanation: {explanation}\"\n",
    "        return enhanced_text\n",
    "\n",
    "    print(\"\\n🔧 強化テキスト特徴量作成中...\")\n",
    "    train_df[\"enhanced_text\"] = train_df.apply(create_enhanced_text, axis=1)\n",
    "    test_df[\"enhanced_text\"] = test_df.apply(create_enhanced_text, axis=1)\n",
    "    \n",
    "    # 6つのカテゴリの確認\n",
    "    unique_categories = sorted(train_df[\"Category\"].unique())\n",
    "    print(f\"\\n📊 カテゴリ情報:\")\n",
    "    print(f\"ユニークなカテゴリ数: {len(unique_categories)}\")\n",
    "    print(\"カテゴリ詳細:\")\n",
    "    for i, cat in enumerate(unique_categories):\n",
    "        count = (train_df[\"Category\"] == cat).sum()\n",
    "        percentage = count / len(train_df) * 100\n",
    "        print(f\"  {i}: {cat} ({count:,}件, {percentage:.1f}%)\")\n",
    "    \n",
    "    # サンプルテキストの表示\n",
    "    print(f\"\\n📝 強化テキストサンプル:\")\n",
    "    sample_text = train_df[\"enhanced_text\"].iloc[0]\n",
    "    print(f\"Length: {len(sample_text)} characters\")\n",
    "    print(f\"Sample: {sample_text[:200]}...\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# データ読み込み実行\n",
    "train_df, test_df = load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63abc52a",
   "metadata": {},
   "source": [
    "## 🤖 Initialize DeBERTa Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb712f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(num_labels=6, model_name=\"microsoft/deberta-v3-xsmall\"):\n",
    "    \"\"\"DeBERTa-v3-xsmallモデルとトークナイザーの準備\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"🤖 DeBERTaモデル準備: {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # デバイス設定（KaggleのGPU環境に最適化）\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"🖥️  使用デバイス: {device}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"🚀 GPU情報: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"💾 GPU メモリ: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "    try:\n",
    "        # トークナイザーの読み込み\n",
    "        print(\"📝 DeBERTaトークナイザー読み込み中...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # パディングトークンの確認\n",
    "        print(f\"🔖 パディングトークン: {tokenizer.pad_token}\")\n",
    "        print(f\"📏 最大トークン長: {tokenizer.model_max_length}\")\n",
    "\n",
    "        # 6分類用モデルの読み込み\n",
    "        print(\"🧠 DeBERTaモデル読み込み中...\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        )\n",
    "\n",
    "        # デバイスに移動\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # モデル情報表示\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"✅ DeBERTaモデル読み込み完了!\")\n",
    "        print(f\"📊 分類クラス数: {num_labels}\")\n",
    "        print(f\"📈 総パラメータ数: {total_params:,}\")\n",
    "        print(f\"🎯 訓練可能パラメータ数: {trainable_params:,}\")\n",
    "        print(f\"💡 モデルサイズ: ~{total_params / 1e6:.1f}M parameters\")\n",
    "\n",
    "        return model, tokenizer, device\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ DeBERTaモデルの読み込みに失敗: {e}\")\n",
    "        print(\"\\n🔧 トラブルシューティング:\")\n",
    "        print(\"1. インターネット接続確認\")\n",
    "        print(\"2. Kaggle環境でのモデルダウンロード制限確認\")\n",
    "        print(\"3. メモリ不足の可能性確認\")\n",
    "        raise e\n",
    "\n",
    "# モデル初期化の実行\n",
    "if train_df is not None:\n",
    "    model, tokenizer, device = prepare_model(num_labels=6)\n",
    "else:\n",
    "    print(\"❌ データが読み込まれていないため、モデル初期化をスキップします\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b64ba",
   "metadata": {},
   "source": [
    "## ⚙️ Create Training Arguments and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf2eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle環境に最適化されたトレーニング設定\n",
    "def create_training_config(device, output_dir=\"./deberta_model\"):\n",
    "    \"\"\"Kaggle環境用のトレーニング設定\"\"\"\n",
    "    \n",
    "    # GPU利用可能性に基づく設定調整\n",
    "    if torch.cuda.is_available():\n",
    "        # GPU環境: より大きなバッチサイズとfp16使用\n",
    "        batch_size = 8  # Kaggle GPU制限に合わせて調整\n",
    "        gradient_accumulation = 4  # 実効バッチサイズ = 32\n",
    "        use_fp16 = True\n",
    "        epochs = 3  # Kaggle時間制限に合わせて調整\n",
    "    else:\n",
    "        # CPU環境: 小さなバッチサイズ\n",
    "        batch_size = 2\n",
    "        gradient_accumulation = 8\n",
    "        use_fp16 = False\n",
    "        epochs = 2\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation,\n",
    "        \n",
    "        # 学習率とオプティマイザー設定\n",
    "        learning_rate=3e-5,  # DeBERTa推奨値\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=200,\n",
    "        \n",
    "        # 評価とロギング\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        logging_steps=50,\n",
    "        \n",
    "        # モデル保存\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=400,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"map3\",\n",
    "        greater_is_better=True,\n",
    "        \n",
    "        # 最適化設定\n",
    "        fp16=use_fp16,\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=None,  # wandbなどのロギングを無効\n",
    "        \n",
    "        # その他\n",
    "        seed=42,\n",
    "        data_seed=42,\n",
    "    )\n",
    "    \n",
    "    print(\"⚙️ トレーニング設定:\")\n",
    "    print(f\"  📊 エポック数: {epochs}\")\n",
    "    print(f\"  📦 バッチサイズ: {batch_size}\")\n",
    "    print(f\"  🔄 勾配蓄積: {gradient_accumulation} (実効バッチ: {batch_size * gradient_accumulation})\")\n",
    "    print(f\"  📈 学習率: {training_args.learning_rate}\")\n",
    "    print(f\"  🎯 FP16使用: {use_fp16}\")\n",
    "    print(f\"  💾 出力ディレクトリ: {output_dir}\")\n",
    "    \n",
    "    return training_args\n",
    "\n",
    "# MAP@3メトリクス計算関数\n",
    "def compute_map3_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    MAP@3 (Mean Average Precision at 3) メトリクスの計算\n",
    "    コンペティションの評価指標\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # ソフトマックスで確率に変換\n",
    "    predictions = torch.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
    "    \n",
    "    map_scores = []\n",
    "    for i, true_label in enumerate(labels):\n",
    "        # 各サンプルの上位3つの予測を取得\n",
    "        top3_indices = np.argsort(predictions[i])[::-1][:3]\n",
    "        \n",
    "        # MAP計算: 正解が上位3つに含まれるかチェック\n",
    "        score = 0.0\n",
    "        for j, pred_idx in enumerate(top3_indices):\n",
    "            if pred_idx == true_label:\n",
    "                score = 1.0 / (j + 1)  # 1位=1.0, 2位=0.5, 3位=0.33\n",
    "                break\n",
    "        map_scores.append(score)\n",
    "    \n",
    "    # 平均計算\n",
    "    map3_score = np.mean(map_scores)\n",
    "    accuracy = accuracy_score(labels, np.argmax(predictions, axis=1))\n",
    "    \n",
    "    return {\n",
    "        \"map3\": map3_score,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"eval_loss\": float(np.mean([1 - score for score in map_scores]))  # 簡易損失\n",
    "    }\n",
    "\n",
    "print(\"✅ トレーニング設定とメトリクス関数が定義されました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb5ef60",
   "metadata": {},
   "source": [
    "## 🚀 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e056236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_df, model, tokenizer, device):\n",
    "    \"\"\"DeBERTaモデルのファインチューニング実行\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🚀 6分類モデル ファインチューニング開始\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ラベルエンコーディング\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_df[\"encoded_labels\"] = label_encoder.fit_transform(train_df[\"Category\"])\n",
    "    \n",
    "    print(\"🏷️ エンコードされたラベル:\")\n",
    "    for i, label in enumerate(label_encoder.classes_):\n",
    "        count = (train_df[\"encoded_labels\"] == i).sum()\n",
    "        print(f\"  {i}: {label} ({count:,}件)\")\n",
    "    \n",
    "    # データ分割（クラス不均衡に対応）\n",
    "    train_texts = train_df[\"enhanced_text\"].tolist()\n",
    "    train_labels = train_df[\"encoded_labels\"].tolist()\n",
    "    \n",
    "    print(f\"\\n📊 データ分割実行...\")\n",
    "    try:\n",
    "        # Stratified split（クラス比率を保持）\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            train_texts,\n",
    "            train_labels,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=train_labels,\n",
    "        )\n",
    "        print(\"✅ Stratified split適用\")\n",
    "    except ValueError as e:\n",
    "        print(f\"⚠️ Stratified splitに失敗、通常のsplitを使用: {e}\")\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            train_texts, train_labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "    \n",
    "    print(f\"📈 訓練データ: {len(X_train):,}件\")\n",
    "    print(f\"📈 検証データ: {len(X_val):,}件\")\n",
    "    \n",
    "    # データセット作成\n",
    "    print(f\"\\n🔧 PyTorchデータセット作成中...\")\n",
    "    train_dataset = MathMisconceptionDataset(\n",
    "        X_train, y_train, tokenizer, max_length=512\n",
    "    )\n",
    "    val_dataset = MathMisconceptionDataset(\n",
    "        X_val, y_val, tokenizer, max_length=512\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ 訓練データセット: {len(train_dataset)}サンプル\")\n",
    "    print(f\"✅ 検証データセット: {len(val_dataset)}サンプル\")\n",
    "    \n",
    "    # データコレーター（動的パディング）\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    # トレーニング設定\n",
    "    training_args = create_training_config(device)\n",
    "    \n",
    "    # Trainerの初期化\n",
    "    print(f\"\\n🎯 Trainer初期化中...\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_map3_metrics,\n",
    "    )\n",
    "    \n",
    "    # GPU使用時のメモリ使用量表示\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"🖥️ GPU初期メモリ使用量: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "    \n",
    "    # ファインチューニング実行\n",
    "    print(f\"\\n🚀 ファインチューニング開始!\")\n",
    "    print(f\"⏱️ 推定時間: {training_args.num_train_epochs * len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) // 60 + 1} 分\")\n",
    "    \n",
    "    try:\n",
    "        # 訓練実行\n",
    "        trainer.train()\n",
    "        \n",
    "        # GPU使用時の最終メモリ使用量\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"🖥️ GPU最終メモリ使用量: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "        \n",
    "        print(f\"✅ ファインチューニング完了!\")\n",
    "        \n",
    "        # 最終評価\n",
    "        print(f\"\\n📊 最終モデル評価:\")\n",
    "        eval_results = trainer.evaluate()\n",
    "        for key, value in eval_results.items():\n",
    "            print(f\"  📈 {key}: {value:.4f}\")\n",
    "        \n",
    "        return trainer, label_encoder\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ トレーニング中にエラーが発生: {e}\")\n",
    "        print(f\"💡 メモリ不足の可能性があります。バッチサイズを下げてください。\")\n",
    "        raise e\n",
    "\n",
    "# モデル訓練の実行\n",
    "if 'model' in locals() and 'tokenizer' in locals() and train_df is not None:\n",
    "    print(\"🎯 モデル訓練を開始します...\")\n",
    "    trainer, label_encoder = train_model(train_df, model, tokenizer, device)\n",
    "    print(\"🎉 モデル訓練完了!\")\n",
    "else:\n",
    "    print(\"❌ 必要な変数が準備されていません。前のセルを実行してください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad5c65f",
   "metadata": {},
   "source": [
    "## 🔮 Generate Predictions for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6978bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_predictions(trainer, tokenizer, test_df, label_encoder):\n",
    "    \"\"\"テストセットに対する予測生成（MAP@3形式）\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🔮 テストセット予測生成\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if test_df is None or len(test_df) == 0:\n",
    "        print(\"❌ テストデータが利用できません\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"📊 テストデータ: {len(test_df):,}件\")\n",
    "    \n",
    "    # テストデータの前処理\n",
    "    test_texts = test_df[\"enhanced_text\"].tolist()\n",
    "    \n",
    "    # ダミーラベル（予測には使用されない）\n",
    "    dummy_labels = [0] * len(test_texts)\n",
    "    \n",
    "    # テストデータセット作成\n",
    "    print(\"🔧 テストデータセット作成中...\")\n",
    "    test_dataset = MathMisconceptionDataset(\n",
    "        test_texts, dummy_labels, tokenizer, max_length=512\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ テストデータセット: {len(test_dataset)}サンプル\")\n",
    "    \n",
    "    # 予測実行\n",
    "    print(\"🔮 予測実行中...\")\n",
    "    try:\n",
    "        predictions = trainer.predict(test_dataset)\n",
    "        print(\"✅ 予測完了!\")\n",
    "        \n",
    "        # 確率に変換\n",
    "        probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1).numpy()\n",
    "        print(f\"📊 予測形状: {probs.shape}\")\n",
    "        \n",
    "        # 各サンプルで上位3つの予測を取得\n",
    "        print(\"🎯 TOP-3予測抽出中...\")\n",
    "        submission_predictions = []\n",
    "        \n",
    "        for i, prob in enumerate(probs):\n",
    "            # 確率の高い順に上位3つのインデックスを取得\n",
    "            top3_indices = np.argsort(prob)[::-1][:3]\n",
    "            \n",
    "            # インデックスを実際のラベル名に変換\n",
    "            top3_labels = [label_encoder.classes_[idx] for idx in top3_indices]\n",
    "            \n",
    "            # スペース区切りで結合（コンペ要求形式）\n",
    "            prediction_string = \" \".join(top3_labels)\n",
    "            submission_predictions.append(prediction_string)\n",
    "            \n",
    "            # 進捗表示（最初の5件）\n",
    "            if i < 5:\n",
    "                top3_probs = [prob[idx] for idx in top3_indices]\n",
    "                print(f\"  サンプル {i+1}: {prediction_string}\")\n",
    "                print(f\"    確率: {[f'{p:.3f}' for p in top3_probs]}\")\n",
    "        \n",
    "        print(f\"✅ TOP-3予測抽出完了: {len(submission_predictions)}件\")\n",
    "        \n",
    "        # 予測の統計情報\n",
    "        all_predictions = \" \".join(submission_predictions).split()\n",
    "        from collections import Counter\n",
    "        pred_counts = Counter(all_predictions)\n",
    "        \n",
    "        print(f\"\\n📈 予測統計:\")\n",
    "        print(f\"  予測に使用されたカテゴリ数: {len(pred_counts)}\")\n",
    "        for category, count in pred_counts.most_common():\n",
    "            percentage = count / (len(submission_predictions) * 3) * 100\n",
    "            print(f\"    {category}: {count}回 ({percentage:.1f}%)\")\n",
    "        \n",
    "        return submission_predictions\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 予測実行中にエラー: {e}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"🖥️ 現在のGPUメモリ使用量: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "        raise e\n",
    "\n",
    "# テスト予測の実行\n",
    "if 'trainer' in locals() and 'label_encoder' in locals() and test_df is not None:\n",
    "    print(\"🔮 テストセット予測を開始します...\")\n",
    "    test_predictions = generate_test_predictions(trainer, tokenizer, test_df, label_encoder)\n",
    "    print(\"🎉 テスト予測完了!\")\n",
    "else:\n",
    "    print(\"❌ 必要な変数が準備されていません。前のセルを実行してください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d0bc5",
   "metadata": {},
   "source": [
    "## 📤 Create Submission File\n",
    "\n",
    "最終ステップとして、Kaggleコンペティションに提出するためのCSVファイルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf432deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_file(test_df, predictions, output_path=\"submission.csv\"):\n",
    "    \"\"\"提出用CSVファイルの作成\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"📤 提出ファイル作成\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if test_df is None or predictions is None:\n",
    "        print(\"❌ テストデータまたは予測結果がありません\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"📊 提出データ: {len(predictions):,}件\")\n",
    "    \n",
    "    # 提出用データフレーム作成\n",
    "    submission_df = pd.DataFrame({\n",
    "        'QuestionId_Answer': test_df.index,  # またはtest_df['QuestionId_Answer'] if available\n",
    "        'Correct': predictions\n",
    "    })\n",
    "    \n",
    "    print(f\"✅ 提出データフレーム作成完了: {submission_df.shape}\")\n",
    "    print(f\"📝 列: {list(submission_df.columns)}\")\n",
    "    \n",
    "    # サンプル表示\n",
    "    print(f\"\\n📋 提出データサンプル:\")\n",
    "    print(submission_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # ファイル保存\n",
    "    try:\n",
    "        submission_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\n💾 提出ファイル保存完了: {output_path}\")\n",
    "        \n",
    "        # ファイルサイズ確認\n",
    "        import os\n",
    "        file_size = os.path.getsize(output_path)\n",
    "        print(f\"📏 ファイルサイズ: {file_size:,} bytes ({file_size/1024:.1f} KB)\")\n",
    "        \n",
    "        # 形式確認\n",
    "        check_df = pd.read_csv(output_path)\n",
    "        print(f\"✅ 提出ファイル検証: {check_df.shape}\")\n",
    "        print(f\"   必要列存在確認: {'QuestionId_Answer' in check_df.columns and 'Correct' in check_df.columns}\")\n",
    "        \n",
    "        # 予測形式チェック\n",
    "        sample_predictions = check_df['Correct'].head(5).tolist()\n",
    "        print(f\"🔍 予測形式サンプル:\")\n",
    "        for i, pred in enumerate(sample_predictions):\n",
    "            pred_parts = pred.split()\n",
    "            print(f\"   {i+1}: {pred} (要素数: {len(pred_parts)})\")\n",
    "        \n",
    "        return submission_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ファイル保存エラー: {e}\")\n",
    "        return None\n",
    "\n",
    "# 提出ファイルの作成\n",
    "if 'test_predictions' in locals() and test_df is not None:\n",
    "    print(\"📤 提出ファイルを作成します...\")\n",
    "    submission_df = create_submission_file(test_df, test_predictions, \"DeBERTa_v3_xsmall_submission.csv\")\n",
    "    \n",
    "    if submission_df is not None:\n",
    "        print(\"🎉 提出ファイル作成完了!\")\n",
    "        print(f\"📋 最終確認:\")\n",
    "        print(f\"   ファイル名: DeBERTa_v3_xsmall_submission.csv\")\n",
    "        print(f\"   データ件数: {len(submission_df):,}\")\n",
    "        print(f\"   予測形式: MAP@3 (各行に3つの予測をスペース区切り)\")\n",
    "        print(\"\\n🏆 Kaggleコンペティションに提出準備完了!\")\n",
    "    else:\n",
    "        print(\"❌ 提出ファイル作成に失敗しました\")\n",
    "else:\n",
    "    print(\"❌ 必要なデータが準備されていません。前のセルを実行してください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2cc21f",
   "metadata": {},
   "source": [
    "## 🎯 Summary & Next Steps\n",
    "\n",
    "### 📊 実行結果サマリー\n",
    "このノートブックでは以下を実装しました：\n",
    "\n",
    "1. **📚 データセット準備**: 数学の誤解分類データの前処理\n",
    "2. **🤖 モデル設定**: Microsoft DeBERTa-v3-xsmall (70M parameters)\n",
    "3. **⚡ 効率的な学習**: Hugging Face Transformersによる最適化\n",
    "4. **📈 評価指標**: MAP@3による性能測定\n",
    "5. **🔮 テスト予測**: TOP-3予測の生成\n",
    "6. **📤 提出準備**: Kaggle形式のCSVファイル作成\n",
    "\n",
    "### 🔧 使用技術\n",
    "- **モデル**: `microsoft/deberta-v3-xsmall`\n",
    "- **フレームワーク**: PyTorch + Transformers\n",
    "- **評価**: MAP@3 (Mean Average Precision at 3)\n",
    "- **最適化**: AdamW optimizer + Linear schedule\n",
    "\n",
    "### 🚀 改善のアイデア\n",
    "- データ拡張（同義語置換、パラフレーズ）\n",
    "- より大きなモデル（DeBERTa-large, RoBERTa-large）\n",
    "- アンサンブル手法（複数モデルの組み合わせ）\n",
    "- ハイパーパラメータチューニング\n",
    "- 交差検証による安定性向上\n",
    "\n",
    "### 📝 使用方法\n",
    "1. 全セルを順番に実行\n",
    "2. `DeBERTa_v3_xsmall_submission.csv`が生成される\n",
    "3. KaggleコンペティションにCSVファイルを提出\n",
    "\n",
    "**🏆 Good luck with your competition!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
