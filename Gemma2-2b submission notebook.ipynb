{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a285cc",
   "metadata": {
    "papermill": {
     "duration": 0.005473,
     "end_time": "2025-07-24T19:39:10.500740",
     "exception": false,
     "start_time": "2025-07-24T19:39:10.495267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final Model Overview: Gemma-2-2b-it with Improved Prompts + QLoRA\n",
    "**Date**: July 24, 2025\n",
    "\n",
    "## ü§î Analysis and Considerations\n",
    "\n",
    "### Performance Comparison with Other Models\n",
    "Despite using a significantly larger model (Gemma-2-2b-it with ~2.6B parameters), the performance improvement compared to smaller models is limited. For example, DeBERTa-v3-xsmall, which has approximately 1/100th the parameters, achieves MAP@3 scores around 0.93. This suggests that model size alone is not the primary factor limiting performance in this task.\n",
    "\n",
    "### Performance Ceiling Observations\n",
    "Examining the leaderboard, most submissions appear to plateau around 0.94 MAP@3 score, indicating a potential performance ceiling for this competition. This plateau suggests fundamental limitations that are not easily overcome by simply scaling model size or improving training techniques.\n",
    "\n",
    "**Resource Constraints and Scalability**: It's important to note that MAP@3 scores don't necessarily improve linearly with model parameter count. If the goal were to improve scores simply by scaling up model size, it would require significantly larger computational resources and associated costs, which are not feasible in my current environment.\n",
    "\n",
    "### Potential Underlying Issues\n",
    "\n",
    "#### Misconception Category Problems\n",
    "The current labeling scheme may be working against optimal performance. The Category:Misconception format creates several challenges:\n",
    "\n",
    "1. **Severe Class Imbalance**: Many misconception categories have only one or very few samples, making it nearly impossible for models to learn meaningful patterns for these rare classes.\n",
    "\n",
    "2. **Non-generalizable Labels**: Some misconception categories are highly specific and may not represent patterns that generalize well to unseen data.\n",
    "\n",
    "3. **Complex Multi-dimensional Classification**: The current format combines multiple classification dimensions (True/False for correctness + Correct/Misconception/Neither for understanding type) into a single label, potentially making the learning task unnecessarily complex.\n",
    "\n",
    "#### Proposed Alternative Approaches\n",
    "\n",
    "**Simplified Classification Approach:**\n",
    "Focus on the core understanding classification (Correct/Misconception/Neither) while excluding the True/False correctness dimension and specific misconception categories. This would:\n",
    "- Reduce class imbalance issues\n",
    "- Focus on the most generalizable aspects of student understanding\n",
    "- Allow better utilization of larger model capabilities\n",
    "\n",
    "**Two-Stage Pipeline Approach:**\n",
    "An alternative strategy could involve:\n",
    "1. Using a high-performance pre-trained LLM to identify unclear points, errors, or misconceptions in student explanations\n",
    "2. Applying a secondary classification model to categorize these identified issues\n",
    "\n",
    "This approach could potentially leverage the superior reasoning capabilities of large language models while avoiding the current labeling complexity issues.\n",
    "\n",
    "### Hypothesis\n",
    "The fundamental issue may not be model capacity but rather the labeling framework itself. Since our Gemma-2-2b model and other participants' larger models should theoretically have superior baseline LLM performance, addressing the labeling complexity could unlock significantly better prediction accuracy.\n",
    "\n",
    "**Note**: These observations are based on empirical results and leaderboard analysis, and represent hypotheses that would require further experimentation to validate.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Model Summary\n",
    "**Model Name**: `gemma-2-2b-improved-prompts-qlora`\n",
    "**Base Model**: google/gemma-2-2b-it (~2.6B parameters)\n",
    "**Training Method**: QLoRA (Quantized Low-Rank Adaptation)\n",
    "**Task**: 65-label classification for mathematical misconception detection\n",
    "\n",
    "## üìä Performance Results\n",
    "- **Final MAP@3 Score**: **0.9411** (94.11%)\n",
    "- **Final Accuracy**: 0.8894 (88.94%)\n",
    "- **Evaluation Loss**: 0.3691\n",
    "- **Training Epochs**: 3.0\n",
    "- **Evaluation Runtime**: 473.05 seconds\n",
    "- **Evaluation Speed**: 15.52 samples/second\n",
    "- **Evaluation Steps per Second**: 1.94\n",
    "- **Improvement from Baseline**: +1.1% (0.93 ‚Üí 0.9411)\n",
    "\n",
    "## üîß Technical Specifications\n",
    "\n",
    "### QLoRA Configuration\n",
    "- **Quantization**: 4-bit (nf4) with double quantization\n",
    "- **LoRA Rank (r)**: 16\n",
    "- **LoRA Alpha**: 32\n",
    "- **Target Modules**: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj\n",
    "- **Memory Reduction**: ~75% compared to full fine-tuning\n",
    "\n",
    "### Training Parameters\n",
    "- **Epochs**: 3\n",
    "- **Batch Size**: 8 (per device)\n",
    "- **Gradient Accumulation**: 4 steps (effective batch size: 32)\n",
    "- **Learning Rate**: 1e-4\n",
    "- **Max Token Length**: 1024\n",
    "- **Training Time**: ~8 hours on A100 GPU\n",
    "\n",
    "### Training Environment\n",
    "- **Platform**: Google Colab Pro\n",
    "- **GPU**: NVIDIA A100 (40GB VRAM)\n",
    "- **Compute Units Consumed**: 50 units (~$5.00 USD cost)\n",
    "- **Code Backup**: Training code preserved in input section for reference\n",
    "\n",
    "## üìù Prompt Engineering Improvements\n",
    "\n",
    "### Enhanced Prompt Structure\n",
    "Based on `final_compact_prompt.py` with the following optimizations:\n",
    "\n",
    "1. **Early Guidelines Placement**: Classification guidelines positioned at the beginning\n",
    "2. **Complete Label Coverage**: All 65 labels including False_Correct:NA\n",
    "3. **Clear Task Definition**: Explicit instruction for exact label selection\n",
    "4. **Structured Format**: Organized question-answer-explanation flow\n",
    "\n",
    "### Sample Prompt Template\n",
    "```\n",
    "You are an expert math educator analyzing student responses for mathematical misconceptions.\n",
    "\n",
    "Question: [Question Text]\n",
    "Correct Answer: [MC_Answer]\n",
    "Student's Explanation: [Student Explanation]\n",
    "\n",
    "CLASSIFICATION GUIDELINES:\n",
    "‚Ä¢ True_Correct:NA = Student demonstrates correct understanding\n",
    "‚Ä¢ False_Correct:NA = Student gives correct answer but for wrong reasons\n",
    "‚Ä¢ True_Neither:NA = Correct answer but unclear/incomplete reasoning\n",
    "‚Ä¢ False_Neither:NA = Incorrect answer but no specific misconception identified\n",
    "‚Ä¢ True_Misconception:[Type] = Correct answer but demonstrates specific misconception\n",
    "‚Ä¢ False_Misconception:[Type] = Incorrect answer with identifiable misconception\n",
    "\n",
    "TASK: Classify this student's response using EXACTLY ONE of these 65 labels:\n",
    "[Complete label list...]\n",
    "\n",
    "Classification:\n",
    "```\n",
    "\n",
    "## üéØ Key Improvements Over Baseline\n",
    "\n",
    "### 1. Prompt Optimization\n",
    "- **Token Efficiency**: ~741 tokens average (optimal for Gemma-2B)\n",
    "- **Label Completeness**: Full 65-label taxonomy support\n",
    "- **Context Structure**: Enhanced problem-solution-explanation flow\n",
    "\n",
    "### 2. QLoRA Benefits\n",
    "- **Memory Efficiency**: 75% reduction in GPU memory usage\n",
    "- **Training Stability**: Improved convergence with 4-bit quantization\n",
    "- **Parameter Efficiency**: Only ~1% of parameters trained (adapters)\n",
    "\n",
    "### 3. Architecture Enhancements\n",
    "- **Gradient Checkpointing**: Memory optimization for long sequences\n",
    "- **Group Batching**: Efficient processing of variable-length inputs\n",
    "- **Mixed Precision**: bf16 for QLoRA stability\n",
    "\n",
    "## üõ†Ô∏è Development Environment\n",
    "\n",
    "### AI-Assisted Development\n",
    "- **Primary Tool**: GitHub Copilot for code generation and documentation\n",
    "- **Language Support**: English documentation and code comments (**author is non-native English speaker**)\n",
    "- **Code Coverage**: Majority of implementation assisted by Copilot, including model training, data processing, and submission notebooks\n",
    "\n",
    "### Training Infrastructure\n",
    "- **Cloud Platform**: Google Colab Pro with premium GPU access\n",
    "- **Hardware**: NVIDIA A100 GPU (40GB VRAM) for efficient QLoRA training\n",
    "- **Cost Management**: 50 compute units consumed (~$5.00 USD total cost)\n",
    "- **Code Preservation**: Complete training codebase backed up in input section for reproducibility\n",
    "\n",
    "---\n",
    "**Model Created**: July 23, 2025\n",
    "**Framework**: Transformers + PEFT + QLoRA\n",
    "**Competition**: MAP - Charting Student Math Misunderstandings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6b3975",
   "metadata": {
    "papermill": {
     "duration": 0.003944,
     "end_time": "2025-07-24T19:39:10.509121",
     "exception": false,
     "start_time": "2025-07-24T19:39:10.505177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üì¶ Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "632b8d6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T19:39:10.519463Z",
     "iopub.status.busy": "2025-07-24T19:39:10.518652Z",
     "iopub.status.idle": "2025-07-24T19:39:47.630879Z",
     "shell.execute_reply": "2025-07-24T19:39:47.629828Z"
    },
    "papermill": {
     "duration": 37.123486,
     "end_time": "2025-07-24T19:39:47.636786",
     "exception": false,
     "start_time": "2025-07-24T19:39:10.513300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Installing required libraries for improved QLoRA model...\n",
      "‚úÖ transformers already installed: 4.52.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 19:39:30.397177: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753385970.624290      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753385970.693000      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ peft already installed: 0.15.2\n",
      "‚úÖ torch already installed: 2.6.0+cu124\n",
      "üéâ Essential libraries for improved QLoRA inference are ready!\n",
      "üìã Note: Minimal setup optimized for inference (no training dependencies)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries for improved QLoRA model\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install package using pip\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Required packages for the improved QLoRA model (inference only)\n",
    "required_packages = [\n",
    "    \"transformers>=4.35.0\",  # For model and tokenizer\n",
    "    \"peft>=0.8.0\",          # Required for QLoRA adapter loading\n",
    "    \"torch>=2.0.0\"          # PyTorch for tensor operations\n",
    "    # Note: bitsandbytes, accelerate, sentencepiece not needed for inference\n",
    "]\n",
    "\n",
    "print(\"üîß Installing required libraries for improved QLoRA model...\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        # Test import first\n",
    "        if \"transformers\" in package:\n",
    "            import transformers\n",
    "            print(f\"‚úÖ transformers already installed: {transformers.__version__}\")\n",
    "        elif \"peft\" in package:\n",
    "            import peft\n",
    "            print(f\"‚úÖ peft already installed: {peft.__version__}\")\n",
    "        elif \"torch\" in package:\n",
    "            import torch\n",
    "            print(f\"‚úÖ torch already installed: {torch.__version__}\")\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installing {package}...\")\n",
    "        install_package(package)\n",
    "\n",
    "print(\"üéâ Essential libraries for improved QLoRA inference are ready!\")\n",
    "print(\"üìã Note: Minimal setup optimized for inference (no training dependencies)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2984cc3a",
   "metadata": {
    "papermill": {
     "duration": 0.004338,
     "end_time": "2025-07-24T19:39:47.645902",
     "exception": false,
     "start_time": "2025-07-24T19:39:47.641564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üìö Import Dependencies and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b90ad2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T19:39:47.656887Z",
     "iopub.status.busy": "2025-07-24T19:39:47.656129Z",
     "iopub.status.idle": "2025-07-24T19:39:47.728941Z",
     "shell.execute_reply": "2025-07-24T19:39:47.727768Z"
    },
    "papermill": {
     "duration": 0.080246,
     "end_time": "2025-07-24T19:39:47.730621",
     "exception": false,
     "start_time": "2025-07-24T19:39:47.650375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Environment Information for Improved QLoRA Model:\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: False\n",
      "Running on CPU\n",
      "\n",
      "üöÄ Model Configuration:\n",
      "Base model: google/gemma-2-2b-it\n",
      "Number of labels: 65\n",
      "Max token length: 1024\n",
      "Improved model name: gemma-2-2b-improved-prompts-qlora\n",
      "\n",
      "üìÅ Kaggle environment detected: /kaggle/input\n",
      "üéØ Competition data path: /kaggle/input/map-charting-student-math-misunderstandings\n",
      "ü§ñ Improved QLoRA model path: /kaggle/input/gemma-2-2b-improved-prompts-qloraaaa/transformers/default/1/kaggle-ready-improved-qlora\n",
      "‚úÖ All dependencies imported and environment configured for improved QLoRA model!\n"
     ]
    }
   ],
   "source": [
    "# Essential libraries for improved QLoRA model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformers and PEFT libraries for improved QLoRA\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import PeftModel  # Required for QLoRA adapters\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Environment configuration\n",
    "print(\"üîß Environment Information for Improved QLoRA Model:\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Model configuration (matching training setup)\n",
    "MODEL_NAME = \"google/gemma-2-2b-it\"\n",
    "NUM_LABELS = 65  # Complete 65-label coverage\n",
    "MAX_LENGTH = 1024  # Optimized token length\n",
    "IMPROVED_MODEL_NAME = \"gemma-2-2b-improved-prompts-qlora\"\n",
    "\n",
    "print(f\"\\nüöÄ Model Configuration:\")\n",
    "print(f\"Base model: {MODEL_NAME}\")\n",
    "print(f\"Number of labels: {NUM_LABELS}\")\n",
    "print(f\"Max token length: {MAX_LENGTH}\")\n",
    "print(f\"Improved model name: {IMPROVED_MODEL_NAME}\")\n",
    "\n",
    "# Kaggle path configuration\n",
    "KAGGLE_INPUT_PATH = \"/kaggle/input\"\n",
    "if os.path.exists(KAGGLE_INPUT_PATH):\n",
    "    print(f\"\\nüìÅ Kaggle environment detected: {KAGGLE_INPUT_PATH}\")\n",
    "    # Competition data path\n",
    "    COMP_DATA_PATH = \"/kaggle/input/map-charting-student-math-misunderstandings\"\n",
    "    # Improved QLoRA model path (update this to your dataset name)\n",
    "    MODEL_DATA_PATH = \"/kaggle/input/gemma-2-2b-improved-prompts-qloraaaa/transformers/default/1/kaggle-ready-improved-qlora\"\n",
    "    print(f\"üéØ Competition data path: {COMP_DATA_PATH}\")\n",
    "    print(f\"ü§ñ Improved QLoRA model path: {MODEL_DATA_PATH}\")\n",
    "else:\n",
    "    print(f\"\\nüìÅ Local environment detected\")\n",
    "    COMP_DATA_PATH = r\"C:\\Users\\mouse\\Desktop\\NotDelete\\GitHub\\kaggleCompe_MAP-math\\map_data\"\n",
    "    # Local improved QLoRA model path\n",
    "    MODEL_DATA_PATH = r\"C:\\Users\\mouse\\Desktop\\NotDelete\\GitHub\\kaggleCompe_MAP-math\\colab\\colab„ÅßË®ìÁ∑¥„Åó„Å¶‰øùÂ≠ò\\kaggle-ready-improved-qlora\"\n",
    "\n",
    "print(\"‚úÖ All dependencies imported and environment configured for improved QLoRA model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a61eb4",
   "metadata": {
    "papermill": {
     "duration": 0.00424,
     "end_time": "2025-07-24T19:39:47.739753",
     "exception": false,
     "start_time": "2025-07-24T19:39:47.735513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üìù Define Improved Prompt Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd75fbac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T19:39:47.750741Z",
     "iopub.status.busy": "2025-07-24T19:39:47.750389Z",
     "iopub.status.idle": "2025-07-24T19:39:47.762298Z",
     "shell.execute_reply": "2025-07-24T19:39:47.761099Z"
    },
    "papermill": {
     "duration": 0.019515,
     "end_time": "2025-07-24T19:39:47.763726",
     "exception": false,
     "start_time": "2025-07-24T19:39:47.744211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Improved prompt functions implemented successfully!\n",
      "üìù Features: Enhanced structure from final_compact_prompt.py\n",
      "üéØ Support: Complete 65-label classification\n",
      "üìã Guidelines: Early classification guidelines placement\n",
      "‚ö° Optimization: Token efficiency and context structure\n",
      "\n",
      "üìä Sample labels (first 10 of 52):\n",
      "  1. False_Correct:NA\n",
      "  2. False_Misconception:Algebra Of Functional Expressions\n",
      "  3. False_Misconception:Algebra Vs Calculus\n",
      "  4. False_Misconception:Arithmetic Of Algebraic Expressions\n",
      "  5. False_Misconception:Confused About Infinity Or Undefined\n",
      "  6. False_Misconception:Confused By Notation\n",
      "  7. False_Misconception:Does Not Use Appropriate Formulas Or Procedures\n",
      "  8. False_Misconception:Graphical\n",
      "  9. False_Misconception:Incomplete\n",
      "  10. False_Misconception:Incorrect Definition\n",
      "  ...\n"
     ]
    }
   ],
   "source": [
    "def get_improved_compact_prompt(question, answer, explanation, all_labels):\n",
    "    \"\"\"\n",
    "    Enhanced prompt function based on final_compact_prompt.py\n",
    "    Optimized for 65-label classification with early guidelines placement\n",
    "    \"\"\"\n",
    "    labels_text = \"\\n\".join([f\"- {label}\" for label in all_labels])\n",
    "\n",
    "    prompt = f\"\"\"You are an expert math educator analyzing student responses for mathematical misconceptions.\n",
    "\n",
    "Question: {question}\n",
    "Correct Answer: {answer}\n",
    "Student's Explanation: {explanation}\n",
    "\n",
    "CLASSIFICATION GUIDELINES:\n",
    "‚Ä¢ True_Correct:NA = Student demonstrates correct understanding\n",
    "‚Ä¢ False_Correct:NA = Student gives correct answer but for wrong reasons\n",
    "‚Ä¢ True_Neither:NA = Correct answer but unclear/incomplete reasoning\n",
    "‚Ä¢ False_Neither:NA = Incorrect answer but no specific misconception identified\n",
    "‚Ä¢ True_Misconception:[Type] = Correct answer but demonstrates specific misconception\n",
    "‚Ä¢ False_Misconception:[Type] = Incorrect answer with identifiable misconception\n",
    "\n",
    "TASK: Classify this student's response using EXACTLY ONE of these {len(all_labels)} labels:\n",
    "\n",
    "{labels_text}\n",
    "\n",
    "Classification:\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def create_enhanced_text_with_improved_prompt(row, all_labels):\n",
    "    \"\"\"\n",
    "    Create enhanced text features using the improved prompt structure\n",
    "    Matches the training format for optimal model performance\n",
    "    \"\"\"\n",
    "    question = str(row[\"QuestionText\"]) if pd.notna(row[\"QuestionText\"]) else \"\"\n",
    "    mc_answer = str(row[\"MC_Answer\"]) if pd.notna(row[\"MC_Answer\"]) else \"\"\n",
    "    explanation = str(row[\"StudentExplanation\"]) if pd.notna(row[\"StudentExplanation\"]) else \"\"\n",
    "\n",
    "    # Use the improved prompt format from training\n",
    "    enhanced_text = get_improved_compact_prompt(question, mc_answer, explanation, all_labels)\n",
    "    return enhanced_text\n",
    "\n",
    "def get_default_labels():\n",
    "    \"\"\"\n",
    "    Get the complete 65-label set used during training\n",
    "    This ensures consistency between training and inference\n",
    "    \"\"\"\n",
    "    # These are the 65 labels that the improved model was trained on\n",
    "    default_labels = [\n",
    "        \"False_Correct:NA\", \"False_Misconception:Algebra Of Functional Expressions\",\n",
    "        \"False_Misconception:Algebra Vs Calculus\", \"False_Misconception:Arithmetic Of Algebraic Expressions\",\n",
    "        \"False_Misconception:Confused About Infinity Or Undefined\", \"False_Misconception:Confused By Notation\",\n",
    "        \"False_Misconception:Does Not Use Appropriate Formulas Or Procedures\",\n",
    "        \"False_Misconception:Graphical\", \"False_Misconception:Incomplete\",\n",
    "        \"False_Misconception:Incorrect Definition\", \"False_Misconception:Linear Extrapolation\",\n",
    "        \"False_Misconception:Logarithms\", \"False_Misconception:Numerical Error\",\n",
    "        \"False_Misconception:Operations\", \"False_Misconception:Other\",\n",
    "        \"False_Misconception:Overconstraining\", \"False_Misconception:Probability\",\n",
    "        \"False_Misconception:Properties Of Functions\", \"False_Misconception:Reasoning About Functions\",\n",
    "        \"False_Misconception:Reasoning About Graphs\", \"False_Misconception:Signed Numbers\",\n",
    "        \"False_Misconception:Slope\", \"False_Misconception:Symbol String Manipulation\",\n",
    "        \"False_Misconception:Trigonometry\", \"False_Misconception:Units\", \"False_Neither:NA\",\n",
    "        \"True_Correct:NA\", \"True_Misconception:Algebra Of Functional Expressions\",\n",
    "        \"True_Misconception:Algebra Vs Calculus\", \"True_Misconception:Arithmetic Of Algebraic Expressions\",\n",
    "        \"True_Misconception:Confused About Infinity Or Undefined\", \"True_Misconception:Confused By Notation\",\n",
    "        \"True_Misconception:Does Not Use Appropriate Formulas Or Procedures\",\n",
    "        \"True_Misconception:Graphical\", \"True_Misconception:Incomplete\",\n",
    "        \"True_Misconception:Incorrect Definition\", \"True_Misconception:Linear Extrapolation\",\n",
    "        \"True_Misconception:Logarithms\", \"True_Misconception:Numerical Error\",\n",
    "        \"True_Misconception:Operations\", \"True_Misconception:Other\",\n",
    "        \"True_Misconception:Overconstraining\", \"True_Misconception:Probability\",\n",
    "        \"True_Misconception:Properties Of Functions\", \"True_Misconception:Reasoning About Functions\",\n",
    "        \"True_Misconception:Reasoning About Graphs\", \"True_Misconception:Signed Numbers\",\n",
    "        \"True_Misconception:Slope\", \"True_Misconception:Symbol String Manipulation\",\n",
    "        \"True_Misconception:Trigonometry\", \"True_Misconception:Units\", \"True_Neither:NA\"\n",
    "    ]\n",
    "    return sorted(default_labels)  # Sort for consistency\n",
    "\n",
    "print(\"‚úÖ Improved prompt functions implemented successfully!\")\n",
    "print(\"üìù Features: Enhanced structure from final_compact_prompt.py\")\n",
    "print(\"üéØ Support: Complete 65-label classification\")\n",
    "print(\"üìã Guidelines: Early classification guidelines placement\")\n",
    "print(\"‚ö° Optimization: Token efficiency and context structure\")\n",
    "\n",
    "# Display sample labels\n",
    "sample_labels = get_default_labels()[:10]\n",
    "print(f\"\\nüìä Sample labels (first 10 of {len(get_default_labels())}):\")\n",
    "for i, label in enumerate(sample_labels, 1):\n",
    "    print(f\"  {i}. {label}\")\n",
    "print(\"  ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a493247",
   "metadata": {
    "papermill": {
     "duration": 0.004658,
     "end_time": "2025-07-24T19:39:47.773303",
     "exception": false,
     "start_time": "2025-07-24T19:39:47.768645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üîß Define Dataset Class for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "567f40f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T19:39:47.783997Z",
     "iopub.status.busy": "2025-07-24T19:39:47.783690Z",
     "iopub.status.idle": "2025-07-24T19:39:47.793069Z",
     "shell.execute_reply": "2025-07-24T19:39:47.791951Z"
    },
    "papermill": {
     "duration": 0.016713,
     "end_time": "2025-07-24T19:39:47.794661",
     "exception": false,
     "start_time": "2025-07-24T19:39:47.777948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Improved Math Misconception Dataset class defined successfully!\n",
      "üéØ Features: Optimized for enhanced prompt structure\n",
      "üìè Max length: 1024 tokens (matching training configuration)\n",
      "üîß Support: Complete 65-label classification\n",
      "üìä MAP@3: Inference-ready prediction format\n"
     ]
    }
   ],
   "source": [
    "class ImprovedMathMisconceptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Enhanced Math Misconception Dataset for PyTorch inference\n",
    "    Optimized for the improved prompt structure and 65-label classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts, tokenizer, max_length=1024):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list): Enhanced text data with improved prompts\n",
    "            tokenizer: Gemma tokenizer (compatible with QLoRA model)\n",
    "            max_length (int): Maximum token length (optimized for improved prompts)\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "\n",
    "        # Tokenize with improved prompt structure support\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "        }\n",
    "\n",
    "def compute_map3_metrics_inference(predictions, top_k=3):\n",
    "    \"\"\"\n",
    "    Compute MAP@3 style predictions for inference\n",
    "    Returns top-k predictions with confidence scores\n",
    "    \"\"\"\n",
    "    # Apply softmax to get probabilities\n",
    "    probs = torch.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
    "    \n",
    "    # Get top-k predictions for each sample\n",
    "    top_k_results = []\n",
    "    for prob in probs:\n",
    "        # Get indices of top-k predictions\n",
    "        top_k_indices = np.argsort(prob)[::-1][:top_k]\n",
    "        top_k_probs = prob[top_k_indices]\n",
    "        top_k_results.append((top_k_indices, top_k_probs))\n",
    "    \n",
    "    return top_k_results\n",
    "\n",
    "print(\"‚úÖ Improved Math Misconception Dataset class defined successfully!\")\n",
    "print(\"üéØ Features: Optimized for enhanced prompt structure\")\n",
    "print(\"üìè Max length: 1024 tokens (matching training configuration)\")\n",
    "print(\"üîß Support: Complete 65-label classification\")\n",
    "print(\"üìä MAP@3: Inference-ready prediction format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b043aa",
   "metadata": {
    "papermill": {
     "duration": 0.004414,
     "end_time": "2025-07-24T19:39:47.805220",
     "exception": false,
     "start_time": "2025-07-24T19:39:47.800806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ü§ñ Load Pre-trained Improved QLoRA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "841d67a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T19:39:47.816012Z",
     "iopub.status.busy": "2025-07-24T19:39:47.815695Z",
     "iopub.status.idle": "2025-07-24T19:39:51.693653Z",
     "shell.execute_reply": "2025-07-24T19:39:51.692104Z"
    },
    "papermill": {
     "duration": 3.885589,
     "end_time": "2025-07-24T19:39:51.695345",
     "exception": false,
     "start_time": "2025-07-24T19:39:47.809756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting improved QLoRA model loading...\n",
      "============================================================\n",
      "ü§ñ Loading Improved QLoRA Gemma Model (MAP@3: 0.9411)\n",
      "============================================================\n",
      "üìÅ Model path: /kaggle/input/gemma-2-2b-improved-prompts-qloraaaa/transformers/default/1/kaggle-ready-improved-qlora\n",
      "üìã Loading label mapping: /kaggle/input/gemma-2-2b-improved-prompts-qloraaaa/transformers/default/1/kaggle-ready-improved-qlora/label_mapping.json\n",
      "‚úÖ Label mapping loaded from model directory\n",
      "üìä Total labels: 65\n",
      "üéØ Sample labels:\n",
      "   0: False_Correct:NA\n",
      "   1: False_Misconception:Adding_across\n",
      "   2: False_Misconception:Adding_terms\n",
      "   3: False_Misconception:Additive\n",
      "   4: False_Misconception:Base_rate\n",
      "   ...\n",
      "\n",
      "üìù Loading Gemma tokenizer...\n",
      "‚úÖ Tokenizer loaded successfully\n",
      "üîñ Padding token: <pad>\n",
      "üìè Vocabulary size: 256,000\n",
      "\n",
      "üß† Loading merged improved model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0aa3cf16d849eea531166860bdcd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merged improved model loaded successfully!\n",
      "\n",
      "üìä Model Information:\n",
      "  üè∑Ô∏è Classification labels: 65\n",
      "  üìà Total parameters: 2,614,491,648\n",
      "  üí° Model type: Improved QLoRA Gemma-2-2b-it\n",
      "  üéØ Training MAP@3: 0.9411\n",
      "  ‚ö° Device: cpu\n",
      "\n",
      "üéâ Improved QLoRA model ready for inference!\n",
      "üìà Expected performance: MAP@3 ‚âà 0.9411 (training result)\n",
      "üî• Enhanced with optimized prompts and 65-label support\n"
     ]
    }
   ],
   "source": [
    "def load_improved_qlora_model():\n",
    "    \"\"\"\n",
    "    Load the pre-trained improved QLoRA model with enhanced performance\n",
    "    Supports both Kaggle and local environments\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ü§ñ Loading Improved QLoRA Gemma Model (MAP@3: 0.9411)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Verify model path\n",
    "        print(f\"üìÅ Model path: {MODEL_DATA_PATH}\")\n",
    "        \n",
    "        if not os.path.exists(MODEL_DATA_PATH):\n",
    "            print(f\"‚ùå Model path not found: {MODEL_DATA_PATH}\")\n",
    "            print(\"üí° In Kaggle environment, ensure the improved model dataset is properly uploaded\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # Load label mapping from improved model\n",
    "        label_file = os.path.join(MODEL_DATA_PATH, \"label_mapping.json\")\n",
    "        print(f\"üìã Loading label mapping: {label_file}\")\n",
    "        \n",
    "        # Try to load from model directory, fallback to default labels\n",
    "        try:\n",
    "            with open(label_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                label_mapping = json.load(f)\n",
    "            print(\"‚úÖ Label mapping loaded from model directory\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"‚ö†Ô∏è Label mapping not found in model directory, using default 65 labels\")\n",
    "            default_labels = get_default_labels()\n",
    "            label_mapping = {str(i): label for i, label in enumerate(default_labels)}\n",
    "        \n",
    "        print(f\"üìä Total labels: {len(label_mapping)}\")\n",
    "        \n",
    "        # Display sample labels\n",
    "        print(\"üéØ Sample labels:\")\n",
    "        for idx, label in list(label_mapping.items())[:5]:\n",
    "            print(f\"   {idx}: {label}\")\n",
    "        print(\"   ...\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        print(f\"\\nüìù Loading Gemma tokenizer...\")\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                MODEL_DATA_PATH,\n",
    "                local_files_only=True  # Kaggle offline support\n",
    "            )\n",
    "        except:\n",
    "            # Fallback to base model tokenizer\n",
    "            print(\"‚ö†Ô∏è Loading tokenizer from base model (fallback)\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "            \n",
    "        # Ensure padding token is set\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "        print(f\"‚úÖ Tokenizer loaded successfully\")\n",
    "        print(f\"üîñ Padding token: {tokenizer.pad_token}\")\n",
    "        print(f\"üìè Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "        \n",
    "        # Check if this is a merged model or PEFT model\n",
    "        config_path = os.path.join(MODEL_DATA_PATH, \"config.json\")\n",
    "        adapter_config_path = os.path.join(MODEL_DATA_PATH, \"adapter_config.json\")\n",
    "        \n",
    "        if os.path.exists(config_path) and not os.path.exists(adapter_config_path):\n",
    "            # This is a merged/unified model (kaggle-ready format)\n",
    "            print(f\"\\nüß† Loading merged improved model...\")\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                MODEL_DATA_PATH,\n",
    "                num_labels=len(label_mapping),\n",
    "                torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "                device_map=\"auto\" if device.type == \"cuda\" else None,\n",
    "                trust_remote_code=True,\n",
    "                local_files_only=True  # Kaggle offline support\n",
    "            )\n",
    "            print(\"‚úÖ Merged improved model loaded successfully!\")\n",
    "            \n",
    "        else:\n",
    "            # This is a PEFT model with adapters\n",
    "            print(f\"\\nüß† Loading base model and PEFT adapters...\")\n",
    "            \n",
    "            # Load base model first\n",
    "            base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                num_labels=len(label_mapping),\n",
    "                torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "                device_map=\"auto\" if device.type == \"cuda\" else None,\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "            \n",
    "            # Load PEFT adapters\n",
    "            model = PeftModel.from_pretrained(\n",
    "                base_model, \n",
    "                MODEL_DATA_PATH,\n",
    "                torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "            )\n",
    "            print(\"‚úÖ PEFT improved model loaded successfully!\")\n",
    "        \n",
    "        # Move to device if needed\n",
    "        if device.type == \"cpu\":\n",
    "            model = model.to(device)\n",
    "        \n",
    "        # Set to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Display model information\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"\\nüìä Model Information:\")\n",
    "        print(f\"  üè∑Ô∏è Classification labels: {model.config.num_labels}\")\n",
    "        print(f\"  üìà Total parameters: {total_params:,}\")\n",
    "        print(f\"  üí° Model type: Improved QLoRA Gemma-2-2b-it\")\n",
    "        print(f\"  üéØ Training MAP@3: 0.9411\")\n",
    "        print(f\"  ‚ö° Device: {device}\")\n",
    "        \n",
    "        return model, tokenizer, label_mapping\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model loading error: {e}\")\n",
    "        print(\"\\nüîß Troubleshooting for Kaggle environment:\")\n",
    "        print(\"1. Ensure improved QLoRA model is uploaded as Kaggle dataset\")\n",
    "        print(\"2. Verify MODEL_DATA_PATH matches your dataset name\") \n",
    "        print(\"3. Check if model files (config.json, model files) exist\")\n",
    "        print(\"4. Verify adapter files for PEFT models\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "\n",
    "# Load improved model\n",
    "print(\"üöÄ Starting improved QLoRA model loading...\")\n",
    "model, tokenizer, label_mapping = load_improved_qlora_model()\n",
    "\n",
    "if model is not None:\n",
    "    print(\"\\nüéâ Improved QLoRA model ready for inference!\")\n",
    "    print(\"üìà Expected performance: MAP@3 ‚âà 0.9411 (training result)\")\n",
    "    print(\"üî• Enhanced with optimized prompts and 65-label support\")\n",
    "else:\n",
    "    print(\"‚ùå Model loading failed. Please check the setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a297a",
   "metadata": {
    "papermill": {
     "duration": 0.00498,
     "end_time": "2025-07-24T19:39:51.705965",
     "exception": false,
     "start_time": "2025-07-24T19:39:51.700985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üìä Load and Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6cbf3ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T19:39:51.717919Z",
     "iopub.status.busy": "2025-07-24T19:39:51.717542Z",
     "iopub.status.idle": "2025-07-24T19:39:51.762415Z",
     "shell.execute_reply": "2025-07-24T19:39:51.761111Z"
    },
    "papermill": {
     "duration": 0.052849,
     "end_time": "2025-07-24T19:39:51.763986",
     "exception": false,
     "start_time": "2025-07-24T19:39:51.711137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading test data with improved prompt formatting...\n",
      "============================================================\n",
      "üìä Loading Test Data with Improved Prompts\n",
      "============================================================\n",
      "üìÅ Test data path: /kaggle/input/map-charting-student-math-misunderstandings/test.csv\n",
      "‚úÖ Test data loaded successfully!\n",
      "üìà Test data shape: (3, 5)\n",
      "\n",
      "üìã Test data columns:\n",
      "['row_id', 'QuestionId', 'QuestionText', 'MC_Answer', 'StudentExplanation']\n",
      "\n",
      "üìã Data sample:\n",
      "   row_id  QuestionId                                       QuestionText  \\\n",
      "0   36696       31772  What fraction of the shape is not shaded? Give...   \n",
      "1   36697       31772  What fraction of the shape is not shaded? Give...   \n",
      "2   36698       32835                      Which number is the greatest?   \n",
      "\n",
      "           MC_Answer                                 StudentExplanation  \n",
      "0  \\( \\frac{1}{3} \\)  I think that 1/3 is the answer, as it's the si...  \n",
      "1  \\( \\frac{3}{6} \\)  i think this answer is because 3 triangles are...  \n",
      "2          \\( 6.2 \\)     because the 2 makes it higher than the others.  \n",
      "\n",
      "üéØ Using 52 labels for improved prompts\n",
      "\n",
      "üîß Creating enhanced text features with improved prompts...\n",
      "   This process uses the same prompt structure as training\n",
      "‚úÖ Enhanced text creation completed in 0.00 seconds\n",
      "\n",
      "üìä Improved Prompt Text Statistics:\n",
      "   Average length: 2959 characters\n",
      "   Minimum length: 2850 characters\n",
      "   Maximum length: 3020 characters\n",
      "   Median length: 3008 characters\n",
      "   Estimated tokens (avg): 740 tokens\n",
      "   Prompts >3000 chars: 2 (66.7%)\n",
      "\n",
      "üìù Sample Enhanced Text with Improved Prompt:\n",
      "Length: 3008 characters\n",
      "Sample (first 500 chars):\n",
      "You are an expert math educator analyzing student responses for mathematical misconceptions.\n",
      "\n",
      "Question: What fraction of the shape is not shaded? Give your answer in its simplest form. [Image: A triangle split into 9 equal smaller triangles. 6 of them are shaded.]\n",
      "Correct Answer: \\( \\frac{1}{3} \\)\n",
      "Student's Explanation: I think that 1/3 is the answer, as it's the simplest form of 3/9.\n",
      "\n",
      "CLASSIFICATION GUIDELINES:\n",
      "‚Ä¢ True_Correct:NA = Student demonstrates correct understanding\n",
      "‚Ä¢ False_Correct:NA = ...\n",
      "\n",
      "‚úÖ Improved Prompt Validation:\n",
      "   Sample 1: Guidelines=True, Task=True, Labels=True\n",
      "   Sample 2: Guidelines=True, Task=True, Labels=True\n",
      "   Sample 3: Guidelines=True, Task=True, Labels=True\n",
      "\n",
      "üéâ Test data preparation completed!\n",
      "üìà Ready for inference: 3 samples\n",
      "üîß Enhanced with improved prompt structure\n",
      "üéØ Optimized for 65-label classification\n"
     ]
    }
   ],
   "source": [
    "def load_and_prepare_test_data_with_improved_prompts():\n",
    "    \"\"\"\n",
    "    Load test data and apply improved prompt structure\n",
    "    Matches the training format for optimal model performance\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä Loading Test Data with Improved Prompts\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Load test data\n",
    "        test_path = os.path.join(COMP_DATA_PATH, \"test.csv\")\n",
    "        print(f\"üìÅ Test data path: {test_path}\")\n",
    "        \n",
    "        if not os.path.exists(test_path):\n",
    "            print(f\"‚ùå Test data not found: {test_path}\")\n",
    "            return None\n",
    "        \n",
    "        test_df = pd.read_csv(test_path)\n",
    "        print(f\"‚úÖ Test data loaded successfully!\")\n",
    "        print(f\"üìà Test data shape: {test_df.shape}\")\n",
    "        \n",
    "        # Local environment testing: use smaller sample\n",
    "        is_local = not os.path.exists(\"/kaggle/input\")\n",
    "        if is_local:\n",
    "            print(\"üîß Local environment detected - using first 50 samples for testing\")\n",
    "            test_df = test_df.head(50).copy()\n",
    "            print(f\"üìä Sample data shape: {test_df.shape}\")\n",
    "        \n",
    "        # Display data information\n",
    "        print(f\"\\nüìã Test data columns:\")\n",
    "        print(test_df.columns.tolist())\n",
    "        \n",
    "        print(f\"\\nüìã Data sample:\")\n",
    "        print(test_df.head(3))\n",
    "        \n",
    "        # Get all labels for improved prompt generation\n",
    "        all_labels = get_default_labels()\n",
    "        print(f\"\\nüéØ Using {len(all_labels)} labels for improved prompts\")\n",
    "        \n",
    "        # Create enhanced text features using improved prompts\n",
    "        print(f\"\\nüîß Creating enhanced text features with improved prompts...\")\n",
    "        print(\"   This process uses the same prompt structure as training\")\n",
    "        \n",
    "        def create_improved_enhanced_text(row):\n",
    "            \"\"\"Create enhanced text using the exact improved prompt structure from training\"\"\"\n",
    "            return create_enhanced_text_with_improved_prompt(row, all_labels)\n",
    "        \n",
    "        # Apply improved prompt formatting\n",
    "        start_time = time.time()\n",
    "        test_df[\"enhanced_text\"] = test_df.apply(create_improved_enhanced_text, axis=1)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"‚úÖ Enhanced text creation completed in {end_time - start_time:.2f} seconds\")\n",
    "        \n",
    "        # Analyze text length statistics\n",
    "        text_lengths = test_df[\"enhanced_text\"].str.len()\n",
    "        print(f\"\\nüìä Improved Prompt Text Statistics:\")\n",
    "        print(f\"   Average length: {text_lengths.mean():.0f} characters\")\n",
    "        print(f\"   Minimum length: {text_lengths.min()} characters\")\n",
    "        print(f\"   Maximum length: {text_lengths.max()} characters\")\n",
    "        print(f\"   Median length: {text_lengths.median():.0f} characters\")\n",
    "        print(f\"   Estimated tokens (avg): {text_lengths.mean() / 4:.0f} tokens\")\n",
    "        \n",
    "        # Check for long prompts\n",
    "        long_prompts = (text_lengths > 3000).sum()\n",
    "        print(f\"   Prompts >3000 chars: {long_prompts} ({long_prompts/len(test_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Display sample improved prompt\n",
    "        print(f\"\\nüìù Sample Enhanced Text with Improved Prompt:\")\n",
    "        sample_text = test_df[\"enhanced_text\"].iloc[0]\n",
    "        print(f\"Length: {len(sample_text)} characters\")\n",
    "        print(f\"Sample (first 500 chars):\")\n",
    "        print(f\"{sample_text[:500]}...\")\n",
    "        \n",
    "        # Validate improved prompt structure\n",
    "        print(f\"\\n‚úÖ Improved Prompt Validation:\")\n",
    "        sample_prompts = test_df[\"enhanced_text\"].head(3)\n",
    "        for i, prompt in enumerate(sample_prompts):\n",
    "            has_guidelines = \"CLASSIFICATION GUIDELINES:\" in prompt\n",
    "            has_task = \"TASK: Classify this student's response\" in prompt\n",
    "            has_labels = len([label for label in all_labels if label in prompt]) > 50\n",
    "            print(f\"   Sample {i+1}: Guidelines={has_guidelines}, Task={has_task}, Labels={has_labels}\")\n",
    "        \n",
    "        return test_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test data loading error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load and prepare test data\n",
    "print(\"üìä Loading test data with improved prompt formatting...\")\n",
    "test_df = load_and_prepare_test_data_with_improved_prompts()\n",
    "\n",
    "if test_df is not None:\n",
    "    print(f\"\\nüéâ Test data preparation completed!\")\n",
    "    print(f\"üìà Ready for inference: {len(test_df):,} samples\")\n",
    "    print(f\"üîß Enhanced with improved prompt structure\")\n",
    "    print(f\"üéØ Optimized for 65-label classification\")\n",
    "else:\n",
    "    print(\"‚ùå Test data preparation failed. Please check the setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ffdf20",
   "metadata": {
    "papermill": {
     "duration": 0.00505,
     "end_time": "2025-07-24T19:39:51.774636",
     "exception": false,
     "start_time": "2025-07-24T19:39:51.769586",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üîÆ Generate Predictions with Improved Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bdc7180",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T19:39:51.786859Z",
     "iopub.status.busy": "2025-07-24T19:39:51.786555Z",
     "iopub.status.idle": "2025-07-24T19:43:24.886603Z",
     "shell.execute_reply": "2025-07-24T19:43:24.884807Z"
    },
    "papermill": {
     "duration": 213.122091,
     "end_time": "2025-07-24T19:43:24.901937",
     "exception": false,
     "start_time": "2025-07-24T19:39:51.779846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ Starting prediction generation with improved QLoRA model...\n",
      "üîß Using batch size: 2 (optimized for cpu)\n",
      "============================================================\n",
      "üîÆ Generating Predictions with Improved QLoRA Model\n",
      "============================================================\n",
      "üìä Test samples: 3\n",
      "üîß Batch size: 2\n",
      "üß† Model: Improved QLoRA Gemma-2-2b-it\n",
      "üéØ Expected performance: MAP@3 ‚âà 0.9411\n",
      "\n",
      "üîß Creating inference dataset...\n",
      "‚úÖ Test dataloader created: 2 batches\n",
      "\n",
      "üîÆ Starting inference with improved model...\n",
      "   Progress: 3/3 (100.0%) - 0.0 samples/sec\n",
      "‚úÖ Inference completed in 213.06 seconds\n",
      "‚ö° Average speed: 0.0 samples/second\n",
      "üìä Prediction tensor shape: (3, 65)\n",
      "\n",
      "üéØ Extracting TOP-3 predictions for MAP@3...\n",
      "  Sample 1:\n",
      "    Predictions: True_Correct:NA True_Neither:NA True_Misconception:Incomplete\n",
      "    Confidences: ['0.989', '0.008', '0.001']\n",
      "  Sample 2:\n",
      "    Predictions: False_Misconception:WNB False_Neither:NA False_Misconception:Incomplete\n",
      "    Confidences: ['0.993', '0.004', '0.001']\n",
      "  Sample 3:\n",
      "    Predictions: True_Neither:NA True_Correct:NA True_Misconception:Shorter_is_bigger\n",
      "    Confidences: ['0.842', '0.146', '0.004']\n",
      "‚úÖ TOP-3 prediction extraction completed: 3 samples\n",
      "\n",
      "üìà Prediction Analysis:\n",
      "   Average top prediction confidence: 0.941\n",
      "   Min confidence: 0.842\n",
      "   Max confidence: 0.993\n",
      "   Unique labels predicted: 7/65\n",
      "   Most frequent predictions:\n",
      "     True_Correct:NA: 2 times (22.2%)\n",
      "     True_Neither:NA: 2 times (22.2%)\n",
      "     True_Misconception:Incomplete: 1 times (11.1%)\n",
      "     False_Misconception:WNB: 1 times (11.1%)\n",
      "     False_Neither:NA: 1 times (11.1%)\n",
      "\n",
      "üéâ Prediction generation completed successfully!\n",
      "üìä Generated 3 TOP-3 predictions\n",
      "üèÜ Ready for Kaggle submission!\n"
     ]
    }
   ],
   "source": [
    "def generate_improved_predictions(model, tokenizer, test_df, label_mapping, batch_size=4):\n",
    "    \"\"\"\n",
    "    Generate predictions using the improved QLoRA model\n",
    "    Optimized for MAP@3 format with enhanced performance\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üîÆ Generating Predictions with Improved QLoRA Model\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if test_df is None or len(test_df) == 0:\n",
    "        print(\"‚ùå Test data not available\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìä Test samples: {len(test_df):,}\")\n",
    "    print(f\"üîß Batch size: {batch_size}\")\n",
    "    print(f\"üß† Model: Improved QLoRA Gemma-2-2b-it\")\n",
    "    print(f\"üéØ Expected performance: MAP@3 ‚âà 0.9411\")\n",
    "    \n",
    "    # Prepare test texts with improved prompts\n",
    "    test_texts = test_df[\"enhanced_text\"].tolist()\n",
    "    \n",
    "    # Create dataset with optimized parameters\n",
    "    print(f\"\\nüîß Creating inference dataset...\")\n",
    "    test_dataset = ImprovedMathMisconceptionDataset(\n",
    "        test_texts, tokenizer, max_length=MAX_LENGTH\n",
    "    )\n",
    "    \n",
    "    # Create dataloader with memory optimization\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Kaggle stability\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        pin_memory=False  # Memory optimization\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Test dataloader created: {len(test_dataloader)} batches\")\n",
    "    \n",
    "    # Start prediction\n",
    "    print(f\"\\nüîÆ Starting inference with improved model...\")\n",
    "    all_predictions = []\n",
    "    total_samples = 0\n",
    "    \n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for batch_idx, batch in enumerate(test_dataloader):\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(**batch)\n",
    "                predictions = outputs.logits\n",
    "                \n",
    "                # Move to CPU and collect\n",
    "                batch_predictions = predictions.cpu().numpy()\n",
    "                all_predictions.append(batch_predictions)\n",
    "                \n",
    "                total_samples += len(batch_predictions)\n",
    "                \n",
    "                # Progress reporting\n",
    "                if (batch_idx + 1) % 5 == 0 or (batch_idx + 1) == len(test_dataloader):\n",
    "                    elapsed = time.time() - start_time\n",
    "                    samples_per_sec = total_samples / elapsed if elapsed > 0 else 0\n",
    "                    print(f\"   Progress: {total_samples:,}/{len(test_df):,} \"\n",
    "                          f\"({total_samples/len(test_df)*100:.1f}%) \"\n",
    "                          f\"- {samples_per_sec:.1f} samples/sec\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"‚úÖ Inference completed in {total_time:.2f} seconds\")\n",
    "        print(f\"‚ö° Average speed: {len(test_df)/total_time:.1f} samples/second\")\n",
    "        \n",
    "        # Combine all predictions\n",
    "        all_predictions = np.vstack(all_predictions)\n",
    "        print(f\"üìä Prediction tensor shape: {all_predictions.shape}\")\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probs = torch.softmax(torch.tensor(all_predictions), dim=-1).numpy()\n",
    "        \n",
    "        # Generate TOP-3 predictions for MAP@3\n",
    "        print(f\"\\nüéØ Extracting TOP-3 predictions for MAP@3...\")\n",
    "        submission_predictions = []\n",
    "        \n",
    "        # Create index to label mapping\n",
    "        idx_to_label = {int(k): v for k, v in label_mapping.items()}\n",
    "        \n",
    "        # Confidence statistics\n",
    "        confidence_scores = []\n",
    "        \n",
    "        for i, prob in enumerate(probs):\n",
    "            # Get top-3 most confident predictions\n",
    "            top3_indices = np.argsort(prob)[::-1][:3]\n",
    "            top3_probs = prob[top3_indices]\n",
    "            \n",
    "            # Convert indices to labels\n",
    "            top3_labels = [idx_to_label[idx] for idx in top3_indices]\n",
    "            \n",
    "            # Store confidence for analysis\n",
    "            confidence_scores.append(top3_probs[0])  # Top prediction confidence\n",
    "            \n",
    "            # Format as space-separated string (Kaggle format)\n",
    "            prediction_string = \" \".join(top3_labels)\n",
    "            submission_predictions.append(prediction_string)\n",
    "            \n",
    "            # Show sample predictions\n",
    "            if i < 5:\n",
    "                print(f\"  Sample {i+1}:\")\n",
    "                print(f\"    Predictions: {prediction_string}\")\n",
    "                print(f\"    Confidences: {[f'{p:.3f}' for p in top3_probs]}\")\n",
    "        \n",
    "        print(f\"‚úÖ TOP-3 prediction extraction completed: {len(submission_predictions)} samples\")\n",
    "        \n",
    "        # Analyze prediction statistics\n",
    "        print(f\"\\nüìà Prediction Analysis:\")\n",
    "        \n",
    "        # Confidence statistics\n",
    "        avg_confidence = np.mean(confidence_scores)\n",
    "        min_confidence = np.min(confidence_scores)\n",
    "        max_confidence = np.max(confidence_scores)\n",
    "        print(f\"   Average top prediction confidence: {avg_confidence:.3f}\")\n",
    "        print(f\"   Min confidence: {min_confidence:.3f}\")\n",
    "        print(f\"   Max confidence: {max_confidence:.3f}\")\n",
    "        \n",
    "        # Label distribution\n",
    "        all_pred_labels = \" \".join(submission_predictions).split()\n",
    "        from collections import Counter\n",
    "        pred_counts = Counter(all_pred_labels)\n",
    "        \n",
    "        print(f\"   Unique labels predicted: {len(pred_counts)}/{len(label_mapping)}\")\n",
    "        print(f\"   Most frequent predictions:\")\n",
    "        for label, count in pred_counts.most_common(5):\n",
    "            percentage = count / (len(submission_predictions) * 3) * 100\n",
    "            print(f\"     {label}: {count} times ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Memory cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"üßπ GPU memory cleaned up\")\n",
    "        \n",
    "        return submission_predictions\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Prediction error: {e}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"üñ•Ô∏è Current GPU memory: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "        raise e\n",
    "\n",
    "# Generate predictions\n",
    "if model is not None and tokenizer is not None and test_df is not None:\n",
    "    print(\"üîÆ Starting prediction generation with improved QLoRA model...\")\n",
    "    \n",
    "    # Adjust batch size based on environment\n",
    "    if device.type == \"cpu\":\n",
    "        batch_size = 2  # Conservative for CPU\n",
    "    elif torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        batch_size = 4 if gpu_memory < 16 else 8  # Adaptive batch size\n",
    "    else:\n",
    "        batch_size = 4\n",
    "    \n",
    "    print(f\"üîß Using batch size: {batch_size} (optimized for {device})\")\n",
    "    \n",
    "    test_predictions = generate_improved_predictions(\n",
    "        model, tokenizer, test_df, label_mapping, batch_size\n",
    "    )\n",
    "    \n",
    "    if test_predictions is not None:\n",
    "        print(f\"\\nüéâ Prediction generation completed successfully!\")\n",
    "        print(f\"üìä Generated {len(test_predictions):,} TOP-3 predictions\")\n",
    "        print(f\"üèÜ Ready for Kaggle submission!\")\n",
    "    else:\n",
    "        print(\"‚ùå Prediction generation failed\")\n",
    "else:\n",
    "    print(\"‚ùå Required components not ready. Please check previous cells.\")\n",
    "    test_predictions = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e41de",
   "metadata": {
    "papermill": {
     "duration": 0.00552,
     "end_time": "2025-07-24T19:43:24.913609",
     "exception": false,
     "start_time": "2025-07-24T19:43:24.908089",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üì§ Create Kaggle Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22463bed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T19:43:24.927416Z",
     "iopub.status.busy": "2025-07-24T19:43:24.926963Z",
     "iopub.status.idle": "2025-07-24T19:43:24.972244Z",
     "shell.execute_reply": "2025-07-24T19:43:24.971080Z"
    },
    "papermill": {
     "duration": 0.054687,
     "end_time": "2025-07-24T19:43:24.973840",
     "exception": false,
     "start_time": "2025-07-24T19:43:24.919153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Creating Kaggle submission file with improved predictions...\n",
      "============================================================\n",
      "üì§ Creating Kaggle Submission File (Improved QLoRA)\n",
      "============================================================\n",
      "üìä Submission data: 3 samples\n",
      "ü§ñ Model: Improved QLoRA Gemma-2-2b-it (MAP@3: 0.9411)\n",
      "‚úÖ Using row_id from test data\n",
      "üìã Row ID range: 36696 to 36698\n",
      "\n",
      "üîß Using model predictions directly...\n",
      "üìù Sample predictions:\n",
      "  Sample 1: True_Correct:NA True_Neither:NA True_Misconception:Incomplete\n",
      "  Sample 2: False_Misconception:WNB False_Neither:NA False_Misconception:Incomplete\n",
      "  Sample 3: True_Neither:NA True_Correct:NA True_Misconception:Shorter_is_bigger\n",
      "\n",
      "‚úÖ Submission DataFrame created: (3, 2)\n",
      "üìù Columns: ['row_id', 'Category:Misconception']\n",
      "\n",
      "üìã Submission Sample:\n",
      " row_id                                                  Category:Misconception\n",
      "  36696           True_Correct:NA True_Neither:NA True_Misconception:Incomplete\n",
      "  36697 False_Misconception:WNB False_Neither:NA False_Misconception:Incomplete\n",
      "  36698    True_Neither:NA True_Correct:NA True_Misconception:Shorter_is_bigger\n",
      "\n",
      "üíæ Submission file saved: submission.csv\n",
      "üìè File size: 251 bytes (0.2 KB)\n",
      "‚úÖ File verification: (3, 2)\n",
      "   Required columns present: True\n",
      "\n",
      "üîç Prediction Format Validation:\n",
      "   Sample 1: 3 parts - True_Correct:NA True_Neither:NA True_Misconception:Incomplete\n",
      "   Sample 2: 3 parts - False_Misconception:WNB False_Neither:NA False_Misconception:Incomplete\n",
      "   Sample 3: 3 parts - True_Neither:NA True_Correct:NA True_Misconception:Shorter_is_bigger\n",
      "\n",
      "üìä Final Submission Summary:\n",
      "   ‚úÖ File: submission.csv\n",
      "   ‚úÖ Format: Kaggle MAP competition format\n",
      "   ‚úÖ Samples: 3\n",
      "   ‚úÖ Columns: ['row_id', 'Category:Misconception']\n",
      "   ‚úÖ Model: Improved QLoRA (Training MAP@3: 0.9411)\n",
      "   ‚úÖ Unique labels in submission: 7\n",
      "\n",
      "============================================================\n",
      "üèÜ IMPROVED QLORA SUBMISSION READY!\n",
      "============================================================\n",
      "ü§ñ Model: Improved QLoRA Gemma-2-2b-it\n",
      "üìà Training Performance: MAP@3 = 0.9411, Accuracy = 0.8894\n",
      "üéØ Enhancement: Optimized prompts + 65-label support\n",
      "üìä Submission: 3 predictions\n",
      "üìÅ File: submission.csv\n",
      "\n",
      "üîß Technical Details:\n",
      "   ‚Ä¢ QLoRA: 4-bit quantization, LoRA rank 16\n",
      "   ‚Ä¢ Prompt: Enhanced with early guidelines placement\n",
      "   ‚Ä¢ Labels: Complete 65-label coverage\n",
      "   ‚Ä¢ Format: Kaggle MAP@3 competition standard\n",
      "\n",
      "üìã Next Steps:\n",
      "   1. Download 'submission.csv'\n",
      "   2. Submit to MAP competition on Kaggle\n",
      "   3. Monitor leaderboard for performance\n",
      "\n",
      "üéØ Expected Performance:\n",
      "   Based on training results (MAP@3: 0.9411)\n",
      "   This should achieve competitive performance!\n",
      "\n",
      "üèÜ Good luck with your improved submission!\n"
     ]
    }
   ],
   "source": [
    "def create_improved_submission_file(test_df, predictions, output_path=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Create Kaggle submission file with improved QLoRA predictions\n",
    "    Properly formatted for MAP competition\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üì§ Creating Kaggle Submission File (Improved QLoRA)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if test_df is None or predictions is None:\n",
    "        print(\"‚ùå Test data or predictions not available\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìä Submission data: {len(predictions):,} samples\")\n",
    "    print(f\"ü§ñ Model: Improved QLoRA Gemma-2-2b-it (MAP@3: 0.9411)\")\n",
    "    \n",
    "    # Get row IDs from test data\n",
    "    if 'row_id' in test_df.columns:\n",
    "        row_ids = test_df['row_id'].tolist()\n",
    "        print(f\"‚úÖ Using row_id from test data\")\n",
    "    else:\n",
    "        # Estimate starting row_id for submission (typical competition format)\n",
    "        print(\"‚ö†Ô∏è row_id not found in test data, using estimated values\")\n",
    "        start_id = 36696  # Common starting ID for MAP competition\n",
    "        row_ids = list(range(start_id, start_id + len(test_df)))\n",
    "    \n",
    "    print(f\"üìã Row ID range: {min(row_ids)} to {max(row_ids)}\")\n",
    "    \n",
    "    # Use predictions as-is (already in correct format from model)\n",
    "    print(f\"\\nüîß Using model predictions directly...\")\n",
    "    \n",
    "    # Show sample predictions\n",
    "    print(f\"üìù Sample predictions:\")\n",
    "    for i, pred in enumerate(predictions[:5]):\n",
    "        print(f\"  Sample {i+1}: {pred}\")\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'row_id': row_ids,\n",
    "        'Category:Misconception': predictions\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n‚úÖ Submission DataFrame created: {submission_df.shape}\")\n",
    "    print(f\"üìù Columns: {list(submission_df.columns)}\")\n",
    "    \n",
    "    # Display sample of final submission\n",
    "    print(f\"\\nüìã Submission Sample:\")\n",
    "    print(submission_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Save to file\n",
    "    try:\n",
    "        submission_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nüíæ Submission file saved: {output_path}\")\n",
    "        \n",
    "        # File validation\n",
    "        file_size = os.path.getsize(output_path)\n",
    "        print(f\"üìè File size: {file_size:,} bytes ({file_size/1024:.1f} KB)\")\n",
    "        \n",
    "        # Load and verify format\n",
    "        check_df = pd.read_csv(output_path)\n",
    "        print(f\"‚úÖ File verification: {check_df.shape}\")\n",
    "        \n",
    "        # Check required columns\n",
    "        required_cols = ['row_id', 'Category:Misconception']\n",
    "        cols_present = all(col in check_df.columns for col in required_cols)\n",
    "        print(f\"   Required columns present: {cols_present}\")\n",
    "        \n",
    "        # Validate prediction format\n",
    "        sample_predictions = check_df['Category:Misconception'].head(5).tolist()\n",
    "        print(f\"\\nüîç Prediction Format Validation:\")\n",
    "        for i, pred in enumerate(sample_predictions):\n",
    "            pred_parts = pred.split()\n",
    "            has_three_parts = len(pred_parts) == 3\n",
    "            print(f\"   Sample {i+1}: {len(pred_parts)} parts - {pred}\")\n",
    "        \n",
    "        # Final summary\n",
    "        print(f\"\\nüìä Final Submission Summary:\")\n",
    "        print(f\"   ‚úÖ File: {output_path}\")\n",
    "        print(f\"   ‚úÖ Format: Kaggle MAP competition format\")\n",
    "        print(f\"   ‚úÖ Samples: {len(check_df):,}\")\n",
    "        print(f\"   ‚úÖ Columns: {list(check_df.columns)}\")\n",
    "        print(f\"   ‚úÖ Model: Improved QLoRA (Training MAP@3: 0.9411)\")\n",
    "        \n",
    "        # Analyze prediction distribution\n",
    "        all_prediction_labels = \" \".join(check_df['Category:Misconception']).split()\n",
    "        unique_labels = set(all_prediction_labels)\n",
    "        print(f\"   ‚úÖ Unique labels in submission: {len(unique_labels)}\")\n",
    "        \n",
    "        return submission_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå File save error: {e}\")\n",
    "        return None\n",
    "\n",
    "def display_submission_summary(submission_df):\n",
    "    \"\"\"Display comprehensive submission summary\"\"\"\n",
    "    if submission_df is None:\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üèÜ IMPROVED QLORA SUBMISSION READY!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"ü§ñ Model: Improved QLoRA Gemma-2-2b-it\")\n",
    "    print(f\"üìà Training Performance: MAP@3 = 0.9411, Accuracy = 0.8894\")\n",
    "    print(f\"üéØ Enhancement: Optimized prompts + 65-label support\")\n",
    "    print(f\"üìä Submission: {len(submission_df):,} predictions\")\n",
    "    print(f\"üìÅ File: submission.csv\")\n",
    "    \n",
    "    print(f\"\\nüîß Technical Details:\")\n",
    "    print(f\"   ‚Ä¢ QLoRA: 4-bit quantization, LoRA rank 16\")\n",
    "    print(f\"   ‚Ä¢ Prompt: Enhanced with early guidelines placement\")\n",
    "    print(f\"   ‚Ä¢ Labels: Complete 65-label coverage\")\n",
    "    print(f\"   ‚Ä¢ Format: Kaggle MAP@3 competition standard\")\n",
    "    \n",
    "    print(f\"\\nüìã Next Steps:\")\n",
    "    print(f\"   1. Download 'submission.csv'\")\n",
    "    print(f\"   2. Submit to MAP competition on Kaggle\")\n",
    "    print(f\"   3. Monitor leaderboard for performance\")\n",
    "    \n",
    "    print(f\"\\nüéØ Expected Performance:\")\n",
    "    print(f\"   Based on training results (MAP@3: 0.9411)\")\n",
    "    print(f\"   This should achieve competitive performance!\")\n",
    "    \n",
    "    print(\"\\nüèÜ Good luck with your improved submission!\")\n",
    "\n",
    "# Create submission file\n",
    "if test_predictions is not None and test_df is not None:\n",
    "    print(\"üì§ Creating Kaggle submission file with improved predictions...\")\n",
    "    \n",
    "    submission_df = create_improved_submission_file(\n",
    "        test_df, \n",
    "        test_predictions, \n",
    "        \"submission.csv\"\n",
    "    )\n",
    "    \n",
    "    if submission_df is not None:\n",
    "        display_submission_summary(submission_df)\n",
    "    else:\n",
    "        print(\"‚ùå Submission file creation failed\")\n",
    "else:\n",
    "    print(\"‚ùå Required data not available. Please run previous cells first.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12957508,
     "isSourceIdPinned": false,
     "sourceId": 104383,
     "sourceType": "competition"
    },
    {
     "datasetId": 7936952,
     "sourceId": 12568194,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 406873,
     "modelInstanceId": 387842,
     "sourceId": 486238,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 264.321356,
   "end_time": "2025-07-24T19:43:28.745500",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-24T19:39:04.424144",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0a29cbe05281408690cafac23ed5489e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8103a1c8296c48069cdffadef77283a3",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e4c4f10ad28e4c39bcaf5defea1afd11",
       "tabbable": null,
       "tooltip": null,
       "value": 3.0
      }
     },
     "3a51bde4092743c6ac70e426aeccce84": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "47c2a0dd89eb4b6e8181f70a78629585": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_79c805466ef84046b39dc21647548a54",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_ca04d107026c4b939f216190b5f62599",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
      }
     },
     "6f0aa3cf16d849eea531166860bdcd89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_47c2a0dd89eb4b6e8181f70a78629585",
        "IPY_MODEL_0a29cbe05281408690cafac23ed5489e",
        "IPY_MODEL_fd676e4f1fed475fac1033f87b5f67b3"
       ],
       "layout": "IPY_MODEL_9d6f39445f334d89a4394abeb9ac5038",
       "tabbable": null,
       "tooltip": null
      }
     },
     "79c805466ef84046b39dc21647548a54": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8103a1c8296c48069cdffadef77283a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9d6f39445f334d89a4394abeb9ac5038": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca04d107026c4b939f216190b5f62599": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ce6971b755bf46c5930471ae0ed6c082": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e4c4f10ad28e4c39bcaf5defea1afd11": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fd676e4f1fed475fac1033f87b5f67b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ce6971b755bf46c5930471ae0ed6c082",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_3a51bde4092743c6ac70e426aeccce84",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá3/3‚Äá[00:01&lt;00:00,‚Äá‚Äá2.00it/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
