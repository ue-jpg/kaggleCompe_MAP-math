{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dfa201a",
   "metadata": {},
   "source": [
    "# ğŸš€ Gemma-2-2b-it æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRA è¨“ç·´ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n",
    "\n",
    "## ğŸ“‹ æ¦‚è¦\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€**æ”¹è‰¯ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**ã¨**QLoRA (Quantized LoRA)** ã‚’ä½¿ç”¨ã—ã¦ã€MAP - Charting Student Math Misunderstandingsã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ç”¨ã®Gemma-2-2b-itãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«è¨“ç·´ã—ã¾ã™ã€‚\n",
    "\n",
    "### ğŸ†• ä¸»ãªæ”¹è‰¯ç‚¹\n",
    "1. **ğŸ“ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: `final_compact_prompt.py`ã®æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹é€ \n",
    "   - 65å€‹ã®å…¨ãƒ©ãƒ™ãƒ«å®Œå…¨å¯¾å¿œï¼ˆFalse_Correct:NAå«ã‚€ï¼‰\n",
    "   - åˆ†é¡ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã®æ—©æœŸé…ç½®\n",
    "   - å•é¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æœ€é©åŒ–\n",
    "   \n",
    "2. **âš¡ QLoRAæœ€é©åŒ–**: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã®å‘ä¸Š\n",
    "   - 4-bité‡å­åŒ–ã«ã‚ˆã‚‹å¤§å¹…ãªãƒ¡ãƒ¢ãƒªå‰Šæ¸›\n",
    "   - é«˜é€Ÿãªå‹¾é…è¨ˆç®—\n",
    "   - GPUä½¿ç”¨ç‡ã®æœ€é©åŒ–\n",
    "\n",
    "### ğŸ¯ ãƒ¢ãƒ‡ãƒ«ä»•æ§˜\n",
    "- **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«**: google/gemma-2-2b-it (~2.6B parameters)\n",
    "- **ã‚¿ã‚¹ã‚¯**: 65ãƒ©ãƒ™ãƒ«åˆ†é¡ (Category:Misconceptionå½¢å¼)\n",
    "- **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: QLoRA (Quantized Low-Rank Adaptation)\n",
    "- **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: final_compact_prompt.py ã®æ”¹è‰¯ç‰ˆ\n",
    "- **GPU**: T4/A100 å¯¾å¿œ\n",
    "\n",
    "### ğŸ’¾ ä¿å­˜å ´æ‰€\n",
    "- **æ–°ãƒ¢ãƒ‡ãƒ«å**: `gemma-2-2b-improved-prompts-qlora`\n",
    "- **Google Drive**: `/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models/`\n",
    "- **åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«**: zipãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾å¿œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b8a9e9",
   "metadata": {},
   "source": [
    "## ğŸ“ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82fb8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveãƒã‚¦ãƒ³ãƒˆ\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "import os\n",
    "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/map_data', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/logs', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/improved_models', exist_ok=True)\n",
    "\n",
    "print(\"âœ… Google Drive ãƒã‚¦ãƒ³ãƒˆå®Œäº†\")\n",
    "print(\"âœ… ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆå®Œäº†\")\n",
    "print(\"ğŸ“ æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: /content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb6cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Faceèªè¨¼\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97460a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRAè¨“ç·´ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install peft accelerate transformers datasets bitsandbytes -U -q\n",
    "!pip install scipy -U -q  # QLoRAã«å¿…è¦\n",
    "\n",
    "print(\"âœ… QLoRAå¯¾å¿œãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n",
    "print(\"ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿: transformers, peft, accelerate, bitsandbytes, datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc97449b",
   "metadata": {},
   "source": [
    "## ğŸ“š ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿ã¨è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573702eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import warnings\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
    "MODEL_NAME = \"google/gemma-2-2b-it\"\n",
    "NUM_LABELS = 65  # å…¨ãƒ©ãƒ™ãƒ«å¯¾å¿œ\n",
    "MAX_LENGTH = 512\n",
    "NEW_MODEL_NAME = \"gemma-2-2b-improved-prompts-qlora\"\n",
    "\n",
    "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ–¥ï¸ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "print(f\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "print(f\"ğŸš€ æ–°ãƒ¢ãƒ‡ãƒ«å: {NEW_MODEL_NAME}\")\n",
    "print(f\"ğŸ“Š å¯¾å¿œãƒ©ãƒ™ãƒ«æ•°: {NUM_LABELS}\")\n",
    "print(f\"ğŸ“ æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1350c255",
   "metadata": {},
   "source": [
    "## ğŸ“ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé–¢æ•°ã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee45a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_labels_from_data(train_df):\n",
    "    \"\"\"å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å…¨ãƒ©ãƒ™ãƒ«ã‚’å–å¾—ï¼ˆColabç”¨ã«ä¿®æ­£ï¼‰\"\"\"\n",
    "    labels = (\n",
    "        train_df[\"Category\"].astype(str)\n",
    "        + \":\"\n",
    "        + train_df[\"Misconception\"].fillna(\"NA\").astype(str)\n",
    "    )\n",
    "    return sorted(labels.unique())\n",
    "\n",
    "def get_improved_compact_prompt(question, answer, explanation, all_labels):\n",
    "    \"\"\"æ”¹è‰¯ç‰ˆã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ - final_compact_prompt.pyãƒ™ãƒ¼ã‚¹\"\"\"\n",
    "    \n",
    "    labels_text = \"\\n\".join([f\"- {label}\" for label in all_labels])\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert math educator analyzing student responses for mathematical misconceptions.\n",
    "\n",
    "Question: {question}\n",
    "Correct Answer: {answer}\n",
    "Student's Explanation: {explanation}\n",
    "\n",
    "CLASSIFICATION GUIDELINES:\n",
    "â€¢ True_Correct:NA = Student demonstrates correct understanding\n",
    "â€¢ False_Correct:NA = Student gives correct answer but for wrong reasons  \n",
    "â€¢ True_Neither:NA = Correct answer but unclear/incomplete reasoning\n",
    "â€¢ False_Neither:NA = Incorrect answer but no specific misconception identified\n",
    "â€¢ True_Misconception:[Type] = Correct answer but demonstrates specific misconception\n",
    "â€¢ False_Misconception:[Type] = Incorrect answer with identifiable misconception\n",
    "\n",
    "TASK: Classify this student's response using EXACTLY ONE of these {len(all_labels)} labels:\n",
    "\n",
    "{labels_text}\n",
    "\n",
    "Classification:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def create_enhanced_text_with_improved_prompt(row, all_labels):\n",
    "    \"\"\"æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆ\"\"\"\n",
    "    question = str(row[\"QuestionText\"]) if pd.notna(row[\"QuestionText\"]) else \"\"\n",
    "    mc_answer = str(row[\"MC_Answer\"]) if pd.notna(row[\"MC_Answer\"]) else \"\"\n",
    "    explanation = str(row[\"StudentExplanation\"]) if pd.notna(row[\"StudentExplanation\"]) else \"\"\n",
    "    \n",
    "    # æ”¹è‰¯ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼ã‚’ä½¿ç”¨\n",
    "    enhanced_text = get_improved_compact_prompt(question, mc_answer, explanation, all_labels)\n",
    "    return enhanced_text\n",
    "\n",
    "print(\"âœ… æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé–¢æ•°å®Ÿè£…å®Œäº†\")\n",
    "print(\"ğŸ“ final_compact_prompt.py ã®æ”¹è‰¯ç‰ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨\")\n",
    "print(\"ğŸ¯ ç‰¹å¾´: åˆ†é¡ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³æ—©æœŸé…ç½®ã€65ãƒ©ãƒ™ãƒ«å®Œå…¨å¯¾å¿œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28c7af",
   "metadata": {},
   "source": [
    "## ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æ”¹è‰¯å‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bab0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_improved_data():\n",
    "    \"\"\"æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ“Š æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ï¼ˆGoogle Driveå†…ï¼‰\n",
    "    train_data_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/map_data/train.csv\"\n",
    "\n",
    "    if not os.path.exists(train_data_path):\n",
    "        print(f\"âŒ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {train_data_path}\")\n",
    "        print(\"ğŸ’¡ Google Driveã®MyDrive/kaggleCompe_MAP-math/map_data/ã« train.csv ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "        train_df = pd.read_csv(train_data_path)\n",
    "        print(f\"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {train_df.shape}\")\n",
    "\n",
    "        # NaNå€¤é™¤å»\n",
    "        before_len = len(train_df)\n",
    "        train_df = train_df.dropna(subset=[\"Category\", \"StudentExplanation\", \"Misconception\"])\n",
    "        after_len = len(train_df)\n",
    "        print(f\"ğŸ§¹ NaNé™¤å»: {before_len} -> {after_len} ({before_len - after_len}è¡Œå‰Šé™¤)\")\n",
    "\n",
    "        # å…¨ãƒ©ãƒ™ãƒ«ã®å–å¾—ï¼ˆ65å€‹ï¼‰\n",
    "        all_labels = get_actual_labels_from_data(train_df)\n",
    "        print(f\"ğŸ“‹ å…¨ãƒ©ãƒ™ãƒ«æ•°: {len(all_labels)}\")\n",
    "        \n",
    "        # ä¸»è¦ãƒ©ãƒ™ãƒ«ã®ç¢ºèª\n",
    "        print(\"ğŸ¯ ä¸»è¦ãƒ©ãƒ™ãƒ«ä¾‹:\")\n",
    "        for i, label in enumerate(all_labels[:5]):\n",
    "            count = (train_df[\"Category\"].astype(str) + \":\" + train_df[\"Misconception\"].fillna(\"NA\").astype(str) == label).sum()\n",
    "            print(f\"  {i+1}. {label} ({count}ä»¶)\")\n",
    "        print(f\"  ... ({len(all_labels)-5}å€‹ã®è¿½åŠ ãƒ©ãƒ™ãƒ«)\")\n",
    "\n",
    "        # False_Correct:NAã®ç¢ºèª\n",
    "        false_correct_present = \"False_Correct:NA\" in all_labels\n",
    "        print(f\"âœ… False_Correct:NAå«æœ‰: {false_correct_present}\")\n",
    "\n",
    "        # æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆ\n",
    "        print(\"\\nğŸ”§ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆä¸­...\")\n",
    "        train_df[\"enhanced_text\"] = train_df.apply(\n",
    "            lambda row: create_enhanced_text_with_improved_prompt(row, all_labels), \n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # ãƒ©ãƒ™ãƒ«ä½œæˆï¼ˆCategory:Misconceptionå½¢å¼ï¼‰\n",
    "        train_df[\"full_label\"] = (\n",
    "            train_df[\"Category\"].astype(str) \n",
    "            + \":\" \n",
    "            + train_df[\"Misconception\"].fillna(\"NA\").astype(str)\n",
    "        )\n",
    "\n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ\n",
    "        text_lengths = train_df[\"enhanced_text\"].str.len()\n",
    "        print(f\"\\nğŸ“ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ:\")\n",
    "        print(f\"  å¹³å‡: {text_lengths.mean():.1f} æ–‡å­—\")\n",
    "        print(f\"  ä¸­å¤®å€¤: {text_lengths.median():.1f} æ–‡å­—\")\n",
    "        print(f\"  æœ€å¤§: {text_lengths.max()} æ–‡å­—\")\n",
    "        print(f\"  æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆå¹³å‡ï¼‰: {text_lengths.mean() / 4:.0f} ãƒˆãƒ¼ã‚¯ãƒ³\")\n",
    "        \n",
    "        # é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å‰²åˆ\n",
    "        long_prompts = (text_lengths > 2048).sum()\n",
    "        print(f\"  2048æ–‡å­—è¶…: {long_prompts} ({long_prompts/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "        return train_df, all_labels\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¾ãŸã¯å‰å‡¦ç†ã«å¤±æ•—: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Ÿè¡Œ\n",
    "train_df, all_labels = load_and_prepare_improved_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7acc2c4",
   "metadata": {},
   "source": [
    "## ğŸ¤– QLoRAå¯¾å¿œGemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gemma_model_with_qlora():\n",
    "    \"\"\"QLoRAè¨­å®šã§Gemma-2-2b-itãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ğŸ¤– QLoRAå¯¾å¿œGemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {MODEL_NAME}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # ğŸ”§ QLoRAç”¨é‡å­åŒ–è¨­å®š\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,                      # 4-bité‡å­åŒ–ã‚’æœ‰åŠ¹\n",
    "            bnb_4bit_use_double_quant=True,         # ãƒ€ãƒ–ãƒ«é‡å­åŒ–ã§ç²¾åº¦å‘ä¸Š\n",
    "            bnb_4bit_quant_type=\"nf4\",              # Normal Float 4-bit\n",
    "            bnb_4bit_compute_dtype=torch.float16,   # è¨ˆç®—ç²¾åº¦\n",
    "        )\n",
    "        \n",
    "        print(\"âš¡ QLoRAé‡å­åŒ–è¨­å®š:\")\n",
    "        print(\"  ğŸ”¸ 4-bité‡å­åŒ–: æœ‰åŠ¹\")\n",
    "        print(\"  ğŸ”¸ ãƒ€ãƒ–ãƒ«é‡å­åŒ–: æœ‰åŠ¹\")  \n",
    "        print(\"  ğŸ”¸ é‡å­åŒ–ã‚¿ã‚¤ãƒ—: nf4\")\n",
    "        print(\"  ğŸ”¸ è¨ˆç®—ç²¾åº¦: float16\")\n",
    "\n",
    "        # ğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿\n",
    "        print(\"\\nğŸ“ Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            print(\"ğŸ”§ ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’EOSãƒˆãƒ¼ã‚¯ãƒ³ã«è¨­å®š\")\n",
    "\n",
    "        print(f\"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "        print(f\"ğŸ”– ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³: {tokenizer.pad_token}\")\n",
    "        print(f\"ğŸ“ èªå½™ã‚µã‚¤ã‚º: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "        # ğŸ§  QLoRAå¯¾å¿œãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
    "        print(f\"\\nğŸ§  QLoRAå¯¾å¿œGemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "        config.num_labels = NUM_LABELS\n",
    "        config.problem_type = \"single_label_classification\"\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            config=config,\n",
    "            quantization_config=bnb_config,  # QLoRAé‡å­åŒ–è¨­å®š\n",
    "            device_map=\"auto\",               # è‡ªå‹•ãƒ‡ãƒã‚¤ã‚¹é…ç½®\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… QLoRAå¯¾å¿œGemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†!\")\n",
    "\n",
    "        # ğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"\\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±:\")\n",
    "        print(f\"  ğŸ“ˆ ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}\")\n",
    "        print(f\"  ğŸ“Š åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°: {NUM_LABELS}\")\n",
    "        print(f\"  ğŸ’¡ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: ~{total_params / 1e9:.2f}B parameters\")\n",
    "        print(f\"  âš¡ é‡å­åŒ–: 4-bit (ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç´„75%å‰Šæ¸›)\")\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ QLoRAãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# QLoRAå¯¾å¿œãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Ÿè¡Œ\n",
    "if train_df is not None:\n",
    "    model, tokenizer = load_gemma_model_with_qlora()\n",
    "else:\n",
    "    print(\"âŒ ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™\")\n",
    "    model, tokenizer = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34476115",
   "metadata": {},
   "source": [
    "## âš¡ QLoRAè¨­å®šã¨PEFTé©ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_qlora_to_model(model):\n",
    "    \"\"\"QLoRAè¨­å®šã¨PEFTé©ç”¨ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"âš¡ QLoRA (PEFT) è¨­å®šã¨é©ç”¨\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if model is None:\n",
    "        print(\"âŒ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        import peft\n",
    "        print(f\"ğŸ“¦ PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒãƒ¼ã‚¸ãƒ§ãƒ³: {peft.__version__}\")\n",
    "\n",
    "        # ğŸ”§ é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’QLoRAè¨“ç·´ç”¨ã«æº–å‚™\n",
    "        print(\"ğŸ”„ é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’QLoRAè¨“ç·´ç”¨ã«æº–å‚™ä¸­...\")\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        print(\"âœ… é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«æº–å‚™å®Œäº†\")\n",
    "\n",
    "        # âš¡ QLoRAè¨­å®šï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–ï¼‰\n",
    "        qlora_config = LoraConfig(\n",
    "            r=16,                           # LoRA attention dimension  \n",
    "            lora_alpha=32,                  # Alpha parameter for LoRA scaling\n",
    "            target_modules=[               # Gemma-2ã®ä¸»è¦ãªç·šå½¢å±¤ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "            ],\n",
    "            lora_dropout=0.05,             # Dropout probability for LoRA layers\n",
    "            bias=\"none\",                   # Bias type\n",
    "            task_type=\"SEQ_CLS\",           # Sequence Classification task\n",
    "            modules_to_save=[\"classifier\", \"score\"],  # åˆ†é¡ãƒ˜ãƒƒãƒ‰ã‚’ä¿å­˜\n",
    "        )\n",
    "\n",
    "        print(\"ğŸ“‹ QLoRAè¨­å®š:\")\n",
    "        print(f\"  ğŸ”¸ r (rank): {qlora_config.r}\")\n",
    "        print(f\"  ğŸ”¸ lora_alpha: {qlora_config.lora_alpha}\")\n",
    "        print(f\"  ğŸ”¸ target_modules: {qlora_config.target_modules}\")\n",
    "        print(f\"  ğŸ”¸ lora_dropout: {qlora_config.lora_dropout}\")\n",
    "        print(f\"  ğŸ”¸ task_type: {qlora_config.task_type}\")\n",
    "        print(f\"  ğŸ”¸ modules_to_save: {qlora_config.modules_to_save}\")\n",
    "\n",
    "        # ğŸš€ QLoRAãƒ¢ãƒ‡ãƒ«ä½œæˆ\n",
    "        print(\"\\nğŸ”„ QLoRAãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...\")\n",
    "        qlora_model = get_peft_model(model, qlora_config)\n",
    "\n",
    "        print(\"âœ… QLoRAãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†\")\n",
    "        qlora_model.print_trainable_parameters()\n",
    "\n",
    "        # ğŸ“Š ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª\n",
    "        if torch.cuda.is_available():\n",
    "            memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "            memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            print(f\"\\nğŸ’¾ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³:\")\n",
    "            print(f\"  ä½¿ç”¨ä¸­: {memory_used:.2f} GB\")\n",
    "            print(f\"  ç·å®¹é‡: {memory_total:.2f} GB\")\n",
    "            print(f\"  ä½¿ç”¨ç‡: {memory_used/memory_total*100:.1f}%\")\n",
    "\n",
    "        return qlora_model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ QLoRAé©ç”¨å¤±æ•—: {e}\")\n",
    "        return None\n",
    "\n",
    "# QLoRAé©ç”¨\n",
    "if model is not None:\n",
    "    qlora_model = apply_qlora_to_model(model)\n",
    "else:\n",
    "    print(\"âŒ ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€QLoRAã‚’é©ç”¨ã§ãã¾ã›ã‚“\")\n",
    "    qlora_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92dcddc",
   "metadata": {},
   "source": [
    "## ğŸ“ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedMathMisconceptionDataset(Dataset):\n",
    "    \"\"\"æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆ Math Misconception Dataset for PyTorch\"\"\"\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆé•·ã„ãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œï¼‰\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def compute_improved_map3_metrics(eval_pred):\n",
    "    \"\"\"æ”¹è‰¯ç‰ˆMAP@3ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = torch.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
    "\n",
    "    map_scores = []\n",
    "    for i, true_label in enumerate(labels):\n",
    "        top3_indices = np.argsort(predictions[i])[::-1][:3]\n",
    "        score = 0.0\n",
    "        for j, pred_idx in enumerate(top3_indices):\n",
    "            if pred_idx == true_label:\n",
    "                score = 1.0 / (j + 1)\n",
    "                break\n",
    "        map_scores.append(score)\n",
    "\n",
    "    map3_score = np.mean(map_scores)\n",
    "    accuracy = accuracy_score(labels, np.argmax(predictions, axis=1))\n",
    "\n",
    "    return {\n",
    "        \"map3\": map3_score, \n",
    "        \"accuracy\": accuracy,\n",
    "        \"map3_detailed\": f\"MAP@3: {map3_score:.4f}\"\n",
    "    }\n",
    "\n",
    "print(\"âœ… æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹å®šç¾©å®Œäº†\")\n",
    "print(\"ğŸ“Š MAP@3è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆæ”¹è‰¯ç‰ˆï¼‰å®šç¾©å®Œäº†\")\n",
    "print(\"ğŸ¯ ç‰¹å¾´: é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯¾å¿œã€è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f1337",
   "metadata": {},
   "source": [
    "## ğŸš€ QLoRAæœ€é©åŒ–è¨“ç·´å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33354d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_improved_qlora_model(train_df, qlora_model, tokenizer, all_labels):\n",
    "    \"\"\"æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRA ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸš€ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if train_df is None or qlora_model is None:\n",
    "        print(\"âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "        return None, None\n",
    "\n",
    "    # 65ãƒ©ãƒ™ãƒ«å¯¾å¿œã®ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_df[\"encoded_labels\"] = label_encoder.fit_transform(train_df[\"full_label\"])\n",
    "\n",
    "    print(f\"ğŸ“‹ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆãƒ©ãƒ™ãƒ«æƒ…å ±:\")\n",
    "    print(f\"  ç·ãƒ©ãƒ™ãƒ«æ•°: {len(label_encoder.classes_)}\")\n",
    "    print(f\"  ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ä¾‹ï¼ˆæœ€åˆã®5å€‹ï¼‰:\")\n",
    "    for i, label in enumerate(label_encoder.classes_[:5]):\n",
    "        count = (train_df[\"encoded_labels\"] == i).sum()\n",
    "        print(f\"    {i}: {label} ({count:,}ä»¶)\")\n",
    "    print(f\"    ... ({len(label_encoder.classes_)-5}å€‹ã®è¿½åŠ ãƒ©ãƒ™ãƒ«)\")\n",
    "\n",
    "    # è¨“ç·´/æ¤œè¨¼åˆ†å‰²\n",
    "    train_texts = train_df[\"enhanced_text\"].tolist()\n",
    "    train_labels = train_df[\"encoded_labels\"].tolist()\n",
    "\n",
    "    try:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            train_texts,\n",
    "            train_labels,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=train_labels,\n",
    "        )\n",
    "        print(\"âœ… Stratified splité©ç”¨\")\n",
    "    except ValueError:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            train_texts, train_labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "        print(\"âœ… Regular splité©ç”¨ï¼ˆå°‘æ•°ã‚¯ãƒ©ã‚¹ã®ãŸã‚ï¼‰\")\n",
    "\n",
    "    print(f\"ğŸ“Š æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²:\")\n",
    "    print(f\"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train):,}ä»¶\")\n",
    "    print(f\"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val):,}ä»¶\")\n",
    "\n",
    "    # æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
    "    train_dataset = ImprovedMathMisconceptionDataset(\n",
    "        X_train, y_train, tokenizer, max_length=MAX_LENGTH\n",
    "    )\n",
    "    val_dataset = ImprovedMathMisconceptionDataset(\n",
    "        X_val, y_val, tokenizer, max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "    # QLoRAæœ€é©åŒ–è¨“ç·´è¨­å®š\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"/content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/{NEW_MODEL_NAME}\",\n",
    "        num_train_epochs=3,                    # æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨æœ€é©ã‚¨ãƒãƒƒã‚¯æ•°\n",
    "        per_device_train_batch_size=2,         # QLoRAç”¨å°ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=16,        # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º32ã‚’ç¶­æŒ\n",
    "        warmup_steps=200,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"/content/drive/MyDrive/kaggleCompe_MAP-math/logs/{NEW_MODEL_NAME}\",\n",
    "        logging_steps=50,                      # ã‚ˆã‚Šé »ç¹ãªãƒ­ã‚°\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,                        # ã‚ˆã‚Šé »ç¹ãªè©•ä¾¡\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"map3\",\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\",\n",
    "        dataloader_pin_memory=False,\n",
    "        fp16=False,                           # QLoRAã§ã¯ç„¡åŠ¹\n",
    "        bf16=True,                            # QLoRAæ¨å¥¨è¨­å®š\n",
    "        optim=\"adamw_torch\",\n",
    "        learning_rate=1e-4,                   # QLoRAç”¨å­¦ç¿’ç‡\n",
    "        save_total_limit=3,\n",
    "        gradient_checkpointing=True,          # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–\n",
    "        dataloader_num_workers=0,             # QLoRAå®‰å®šåŒ–\n",
    "        group_by_length=True,                 # åŠ¹ç‡çš„ãƒãƒƒãƒãƒ³ã‚°\n",
    "    )\n",
    "\n",
    "    print(\"\\nğŸ“‹ QLoRAæœ€é©åŒ–è¨“ç·´è¨­å®š:\")\n",
    "    print(f\"  ğŸ”¸ ã‚¨ãƒãƒƒã‚¯æ•°: {training_args.num_train_epochs}\")\n",
    "    print(f\"  ğŸ”¸ per_device_batch_size: {training_args.per_device_train_batch_size}\")\n",
    "    print(f\"  ğŸ”¸ gradient_accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "    print(f\"  ğŸ”¸ å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "    print(f\"  ğŸ”¸ å­¦ç¿’ç‡: {training_args.learning_rate}\")\n",
    "    print(f\"  ğŸ”¸ ä¿å­˜å…ˆ: {training_args.output_dir}\")\n",
    "    print(f\"  ğŸ”¸ gradient_checkpointing: {training_args.gradient_checkpointing}\")\n",
    "    print(f\"  ğŸ”¸ group_by_length: {training_args.group_by_length}\")\n",
    "\n",
    "    # QLoRAå¯¾å¿œTrainerä½œæˆ\n",
    "    trainer = Trainer(\n",
    "        model=qlora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_improved_map3_metrics,\n",
    "    )\n",
    "\n",
    "    # è¨“ç·´å®Ÿè¡Œ\n",
    "    print(\"\\nğŸš€ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†\")\n",
    "\n",
    "        # æœ€çµ‚è©•ä¾¡\n",
    "        print(\"\\nğŸ“Š æœ€çµ‚è©•ä¾¡:\")\n",
    "        eval_results = trainer.evaluate()\n",
    "        for key, value in eval_results.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  ğŸ¯ {key}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"  ğŸ“‹ {key}: {value}\")\n",
    "\n",
    "        return trainer, label_encoder\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRA è¨“ç·´å®Ÿè¡Œ\n",
    "if train_df is not None and qlora_model is not None and all_labels is not None:\n",
    "    trainer, label_encoder = train_improved_qlora_model(train_df, qlora_model, tokenizer, all_labels)\n",
    "else:\n",
    "    print(\"âŒ è¨“ç·´ã«å¿…è¦ãªè¦ç´ ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    trainer, label_encoder = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680cb700",
   "metadata": {},
   "source": [
    "## ğŸ’¾ æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b5c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_improved_qlora_model(trainer, tokenizer, label_encoder, all_labels):\n",
    "    \"\"\"æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRA è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ’¾ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRA ãƒ¢ãƒ‡ãƒ«ä¿å­˜\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if trainer is None or tokenizer is None or label_encoder is None:\n",
    "        print(\"âŒ ä¿å­˜ã«å¿…è¦ãªè¦ç´ ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "        return False\n",
    "\n",
    "    save_base_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/improved_models\"\n",
    "    model_save_path = f\"{save_base_path}/{NEW_MODEL_NAME}-final\"\n",
    "\n",
    "    try:\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜\n",
    "        print(f\"ğŸ“ ä¿å­˜å…ˆ: {model_save_path}\")\n",
    "\n",
    "        # QLoRAãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆã‚¢ãƒ€ãƒ—ã‚¿ã®ã¿ï¼‰\n",
    "        trainer.save_model(model_save_path)\n",
    "        tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "        print(\"âœ… æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRAãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†\")\n",
    "\n",
    "        # æ”¹è‰¯ç‰ˆãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã®ä¿å­˜\n",
    "        label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "        label_file = f\"{model_save_path}/improved_label_mapping.json\"\n",
    "\n",
    "        with open(label_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(label_mapping, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(\"âœ… æ”¹è‰¯ç‰ˆãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ä¿å­˜å®Œäº†\")\n",
    "        print(f\"ğŸ“‹ ãƒ©ãƒ™ãƒ«ç·æ•°: {len(label_mapping)}\")\n",
    "        \n",
    "        # False_Correct:NAã®ç¢ºèª\n",
    "        false_correct_labels = [label for label in label_mapping.values() if \"False_Correct\" in label]\n",
    "        print(f\"âœ… False_Correcté–¢é€£ãƒ©ãƒ™ãƒ«: {len(false_correct_labels)}å€‹å«æœ‰\")\n",
    "\n",
    "        # æ”¹è‰¯ç‰ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜\n",
    "        metadata = {\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"improved_model_name\": NEW_MODEL_NAME,\n",
    "            \"num_labels\": len(all_labels),\n",
    "            \"max_length\": MAX_LENGTH,\n",
    "            \"task_type\": \"sequence_classification\",\n",
    "            \"architecture\": \"AutoModelForSequenceClassification\",\n",
    "            \"improvements\": {\n",
    "                \"prompt_optimization\": \"final_compact_prompt.py based\",\n",
    "                \"qlora_applied\": True,\n",
    "                \"quantization\": \"4-bit\",\n",
    "                \"label_coverage\": \"65 labels including False_Correct:NA\",\n",
    "                \"prompt_features\": [\n",
    "                    \"Early classification guidelines placement\",\n",
    "                    \"Enhanced context structure\", \n",
    "                    \"Comprehensive label coverage\",\n",
    "                    \"Optimized token efficiency\"\n",
    "                ]\n",
    "            },\n",
    "            \"qlora_config\": {\n",
    "                \"r\": 16,\n",
    "                \"lora_alpha\": 32,\n",
    "                \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "                \"task_type\": \"SEQ_CLS\",\n",
    "                \"quantization\": \"4-bit nf4\",\n",
    "                \"compute_dtype\": \"float16\"\n",
    "            },\n",
    "            \"training_info\": {\n",
    "                \"epochs\": 3,\n",
    "                \"batch_size\": 2,\n",
    "                \"gradient_accumulation\": 16,\n",
    "                \"learning_rate\": 1e-4,\n",
    "                \"optimization\": \"QLoRA + improved prompts\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        metadata_file = f\"{model_save_path}/improved_model_metadata.json\"\n",
    "        with open(metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(\"âœ… æ”¹è‰¯ç‰ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†\")\n",
    "\n",
    "        # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "        print(f\"\\nğŸ“‚ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
    "        for item in os.listdir(model_save_path):\n",
    "            item_path = os.path.join(model_save_path, item)\n",
    "            if os.path.isfile(item_path):\n",
    "                size_mb = os.path.getsize(item_path) / (1024 * 1024)\n",
    "                print(f\"  ğŸ“„ {item}: {size_mb:.2f} MB\")\n",
    "            else:\n",
    "                print(f\"  ğŸ“ {item}/\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return False\n",
    "\n",
    "# æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Ÿè¡Œ\n",
    "if trainer is not None and tokenizer is not None and label_encoder is not None and all_labels is not None:\n",
    "    save_success = save_improved_qlora_model(trainer, tokenizer, label_encoder, all_labels)\n",
    "\n",
    "    if save_success:\n",
    "        print(\"\\nğŸ‰ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRAãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†!\")\n",
    "        print(f\"ğŸ“ Google Driveä¿å­˜ãƒ‘ã‚¹: /content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/\")\n",
    "        print(f\"ğŸš€ æ–°ãƒ¢ãƒ‡ãƒ«å: {NEW_MODEL_NAME}\")\n",
    "        print(\"ğŸ”¥ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: Kaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æº–å‚™å®Œäº†\")\n",
    "    else:\n",
    "        print(\"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "else:\n",
    "    print(\"âŒ ä¿å­˜ã«å¿…è¦ãªè¦ç´ ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f28f605",
   "metadata": {},
   "source": [
    "## ğŸ“¦ æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«zipä½œæˆã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c10bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_improved_model_zip():\n",
    "    \"\"\"æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRAãƒ¢ãƒ‡ãƒ«ç”¨zipãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“¦ æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ç”¨zipãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        import zipfile\n",
    "\n",
    "        model_dir = f\"/content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/{NEW_MODEL_NAME}-final\"\n",
    "        zip_path = f\"/content/drive/MyDrive/kaggleCompe_MAP-math/{NEW_MODEL_NAME}.zip\"\n",
    "\n",
    "        if not os.path.exists(model_dir):\n",
    "            print(f\"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_dir}\")\n",
    "            return False\n",
    "\n",
    "        print(f\"ğŸ“ ã‚½ãƒ¼ã‚¹: {model_dir}\")\n",
    "        print(f\"ğŸ“¦ å‡ºåŠ›zip: {zip_path}\")\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for root, dirs, files in os.walk(model_dir):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, model_dir)\n",
    "                    zipf.write(file_path, arcname)\n",
    "                    print(f\"  â• {arcname}\")\n",
    "\n",
    "        # zipãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª\n",
    "        zip_size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
    "        print(f\"\\nâœ… æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«zipä½œæˆå®Œäº†!\")\n",
    "        print(f\"ğŸ“ zipã‚µã‚¤ã‚º: {zip_size_mb:.2f} MB\")\n",
    "        print(f\"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‘ã‚¹: {zip_path}\")\n",
    "\n",
    "        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã‚³ãƒ¼ãƒ‰è¡¨ç¤º\n",
    "        print(f\"\\nğŸ’¡ Colabã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯:\")\n",
    "        print(f\"```python\")\n",
    "        print(f\"from google.colab import files\")\n",
    "        print(f\"files.download('{zip_path}')\")\n",
    "        print(f\"```\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ zipä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return False\n",
    "\n",
    "# æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«zipä½œæˆå®Ÿè¡Œ\n",
    "zip_success = create_improved_model_zip()\n",
    "\n",
    "# è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "if zip_success:\n",
    "    print(\"\\nğŸ‰ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRAãƒ¢ãƒ‡ãƒ«zipä½œæˆå®Œäº†!\")\n",
    "    print(f\"ğŸ“¦ zipãƒ•ã‚¡ã‚¤ãƒ«: {NEW_MODEL_NAME}.zip\")\n",
    "    \n",
    "    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç¢ºèª\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        response = input(\"\\næ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ã‚’ä»Šã™ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã‹ï¼Ÿ (y/n): \")\n",
    "        if response.lower() == 'y':\n",
    "            files.download(f'/content/drive/MyDrive/kaggleCompe_MAP-math/{NEW_MODEL_NAME}.zip')\n",
    "            print(\"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹!\")\n",
    "        else:\n",
    "            print(\"ğŸ’¡ å¾Œã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆã¯ä¸Šè¨˜ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„\")\n",
    "    except:\n",
    "        print(\"ğŸ’¡ æ‰‹å‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n",
    "else:\n",
    "    print(\"âŒ zipä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07011cd5",
   "metadata": {},
   "source": [
    "## ğŸ¯ æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«å®Œäº†ã‚µãƒãƒªãƒ¼\n",
    "\n",
    "### âœ… å®Œäº†ã—ãŸæ”¹è‰¯ç‚¹\n",
    "1. **ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹è‰¯**: `final_compact_prompt.py`ãƒ™ãƒ¼ã‚¹\n",
    "   - 65å€‹ã®å…¨ãƒ©ãƒ™ãƒ«å®Œå…¨å¯¾å¿œï¼ˆFalse_Correct:NAå«ã‚€ï¼‰\n",
    "   - åˆ†é¡ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã®æ—©æœŸé…ç½®\n",
    "   - å•é¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æœ€é©åŒ–æ§‹é€ \n",
    "   - ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡ã®æ”¹å–„\n",
    "\n",
    "2. **âš¡ QLoRAæœ€é©åŒ–**: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨æ€§èƒ½å‘ä¸Š\n",
    "   - 4-bité‡å­åŒ–ã«ã‚ˆã‚‹å¤§å¹…ãªãƒ¡ãƒ¢ãƒªå‰Šæ¸›\n",
    "   - LoRA (r=16, alpha=32) ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "   - å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–\n",
    "   - ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨accumulationã®æœ€é©ãƒãƒ©ãƒ³ã‚¹\n",
    "\n",
    "3. **ğŸš€ è¨“ç·´åŠ¹ç‡åŒ–**:\n",
    "   - QLoRAå¯¾å¿œã®å°‚ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
    "   - MAP@3ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ”¹è‰¯ç‰ˆå®Ÿè£…\n",
    "   - ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ãƒãƒƒãƒãƒ³ã‚°ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–\n",
    "   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å¤§å¹…å‰Šæ¸›\n",
    "\n",
    "### ğŸ“ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«\n",
    "- **æ–°ãƒ¢ãƒ‡ãƒ«å**: `gemma-2-2b-improved-prompts-qlora`\n",
    "- **Google Drive**: `/content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/`\n",
    "- **zipå½¢å¼**: `gemma-2-2b-improved-prompts-qlora.zip`\n",
    "- **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿**: æ”¹è‰¯å†…å®¹ã®è©³ç´°è¨˜éŒ²ä»˜ã\n",
    "\n",
    "### ğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆKaggleæå‡ºæº–å‚™ï¼‰\n",
    "1. **Kaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ**: æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ã‚’Kaggleã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "2. **æ¨è«–ãƒ†ã‚¹ãƒˆ**: test.csvã§ã®æ¨è«–æ€§èƒ½ç¢ºèª\n",
    "3. **submission.csvç”Ÿæˆ**: MAP@3æœ€é©åŒ–ã«ã‚ˆã‚‹æå‡º\n",
    "4. **æ€§èƒ½æ¯”è¼ƒ**: å…ƒãƒ¢ãƒ‡ãƒ«ã¨ã®æ€§èƒ½å·®åˆ†æ\n",
    "\n",
    "### ğŸ¯ æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿æ–¹æ³•ï¼ˆKaggleç”¨ï¼‰\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "\n",
    "# QLoRAã‚¢ãƒ€ãƒ—ã‚¿èª­ã¿è¾¼ã¿\n",
    "model = PeftModel.from_pretrained(base_model, \"path/to/improved_model\")\n",
    "```\n",
    "\n",
    "### ğŸš€ ä¸»ãªæ”¹å–„åŠ¹æœæœŸå¾…å€¤\n",
    "- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: ç´„75%å‰Šæ¸›ï¼ˆQLoRAåŠ¹æœï¼‰\n",
    "- **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç²¾åº¦**: False_Correct:NAå¯¾å¿œã«ã‚ˆã‚‹åˆ†é¡ç²¾åº¦å‘ä¸Š\n",
    "- **è¨“ç·´åŠ¹ç‡**: æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ˆã‚‹å­¦ç¿’åæŸæ€§å‘ä¸Š\n",
    "- **æ¨è«–é€Ÿåº¦**: æœ€é©åŒ–ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³é•·ã«ã‚ˆã‚‹é«˜é€ŸåŒ–\n",
    "\n",
    "**ğŸ‰ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRAãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†!**\n",
    "**æ¬¡ã¯æå‡ºç”¨æ¨è«–ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã®æ€§èƒ½ãƒ†ã‚¹ãƒˆã§ã™ï¼**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
