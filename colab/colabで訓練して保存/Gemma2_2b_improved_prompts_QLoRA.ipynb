{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5dfa201a",
      "metadata": {
        "id": "5dfa201a"
      },
      "source": [
        "# ğŸš€ Gemma-2-2b-it æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRA è¨“ç·´ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n",
        "\n",
        "## ğŸ“‹ æ¦‚è¦\n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€**æ”¹è‰¯ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**ã¨**QLoRA (Quantized LoRA)** ã‚’ä½¿ç”¨ã—ã¦ã€MAP - Charting Student Math Misunderstandingsã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ç”¨ã®Gemma-2-2b-itãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«è¨“ç·´ã—ã¾ã™ã€‚\n",
        "\n",
        "### ğŸ†• ä¸»ãªæ”¹è‰¯ç‚¹\n",
        "1. **ğŸ“ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: `final_compact_prompt.py`ã®æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹é€ \n",
        "   - 65å€‹ã®å…¨ãƒ©ãƒ™ãƒ«å®Œå…¨å¯¾å¿œï¼ˆFalse_Correct:NAå«ã‚€ï¼‰\n",
        "   - åˆ†é¡ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã®æ—©æœŸé…ç½®\n",
        "   - å•é¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æœ€é©åŒ–\n",
        "   \n",
        "2. **âš¡ QLoRAæœ€é©åŒ–**: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã®å‘ä¸Š\n",
        "   - 4-bité‡å­åŒ–ã«ã‚ˆã‚‹å¤§å¹…ãªãƒ¡ãƒ¢ãƒªå‰Šæ¸›\n",
        "   - é«˜é€Ÿãªå‹¾é…è¨ˆç®—\n",
        "   - GPUä½¿ç”¨ç‡ã®æœ€é©åŒ–\n",
        "\n",
        "### ğŸ¯ ãƒ¢ãƒ‡ãƒ«ä»•æ§˜\n",
        "- **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«**: google/gemma-2-2b-it (~2.6B parameters)\n",
        "- **ã‚¿ã‚¹ã‚¯**: 65ãƒ©ãƒ™ãƒ«åˆ†é¡ (Category:Misconceptionå½¢å¼)\n",
        "- **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: QLoRA (Quantized Low-Rank Adaptation)\n",
        "- **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: final_compact_prompt.py ã®æ”¹è‰¯ç‰ˆ\n",
        "- **GPU**: T4/A100 å¯¾å¿œ\n",
        "\n",
        "### ğŸ’¾ ä¿å­˜å ´æ‰€\n",
        "- **æ–°ãƒ¢ãƒ‡ãƒ«å**: `gemma-2-2b-improved-prompts-qlora`\n",
        "- **Google Drive**: `/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models/`\n",
        "- **åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«**: zipãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾å¿œ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b8a9e9",
      "metadata": {
        "id": "45b8a9e9"
      },
      "source": [
        "## ğŸ“ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c82fb8b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c82fb8b5",
        "outputId": "e4886641-e656-41ca-93a6-b2025c2686de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… Google Drive ãƒã‚¦ãƒ³ãƒˆå®Œäº†\n",
            "âœ… ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆå®Œäº†\n",
            "ğŸ“ æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: /content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/\n"
          ]
        }
      ],
      "source": [
        "# Google Driveãƒã‚¦ãƒ³ãƒˆ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
        "import os\n",
        "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/map_data', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/logs', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/improved_models', exist_ok=True)\n",
        "\n",
        "print(\"âœ… Google Drive ãƒã‚¦ãƒ³ãƒˆå®Œäº†\")\n",
        "print(\"âœ… ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆå®Œäº†\")\n",
        "print(\"ğŸ“ æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: /content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92cb6cdb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92cb6cdb",
        "outputId": "ad6ff44c-bd6c-4b27-ea22-ae44d5247785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: read).\n",
            "The token `GemmaPractice` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `GemmaPractice`\n"
          ]
        }
      ],
      "source": [
        "# Hugging Faceèªè¨¼\n",
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97460a12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97460a12",
        "outputId": "c24ddb4b-3b7e-40a3-ae18-b07204c1982c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… QLoRAå¯¾å¿œãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\n",
            "ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿: transformers, peft, accelerate, bitsandbytes, datasets\n"
          ]
        }
      ],
      "source": [
        "# QLoRAè¨“ç·´ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip install peft accelerate transformers datasets bitsandbytes -U -q\n",
        "!pip install scipy -U -q  # QLoRAã«å¿…è¦\n",
        "\n",
        "print(\"âœ… QLoRAå¯¾å¿œãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n",
        "print(\"ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿: transformers, peft, accelerate, bitsandbytes, datasets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc97449b",
      "metadata": {
        "id": "bc97449b"
      },
      "source": [
        "## ğŸ“š ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿ã¨è¨­å®š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "573702eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "573702eb",
        "outputId": "f0a1877b-253b-4a17-cb1b-3b4b5d7bb887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ–¥ï¸ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: cuda\n",
            "ğŸ® GPU: NVIDIA A100-SXM4-40GB\n",
            "ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒª: 42.5 GB\n",
            "âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿å®Œäº†\n",
            "ğŸš€ æ–°ãƒ¢ãƒ‡ãƒ«å: gemma-2-2b-improved-prompts-qlora\n",
            "ğŸ“Š å¯¾å¿œãƒ©ãƒ™ãƒ«æ•°: 65\n",
            "ğŸ“ æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·: 1024\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import warnings\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
        "MODEL_NAME = \"google/gemma-2-2b-it\"\n",
        "NUM_LABELS = 65  # å…¨ãƒ©ãƒ™ãƒ«å¯¾å¿œ\n",
        "MAX_LENGTH = 1024\n",
        "NEW_MODEL_NAME = \"gemma-2-2b-improved-prompts-qlora\"\n",
        "\n",
        "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ğŸ–¥ï¸ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "print(f\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿å®Œäº†\")\n",
        "print(f\"ğŸš€ æ–°ãƒ¢ãƒ‡ãƒ«å: {NEW_MODEL_NAME}\")\n",
        "print(f\"ğŸ“Š å¯¾å¿œãƒ©ãƒ™ãƒ«æ•°: {NUM_LABELS}\")\n",
        "print(f\"ğŸ“ æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·: {MAX_LENGTH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1350c255",
      "metadata": {
        "id": "1350c255"
      },
      "source": [
        "## ğŸ“ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé–¢æ•°ã®å®Ÿè£…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ee45a50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ee45a50",
        "outputId": "37178aef-f774-4a7c-c96e-c4217e9c986b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé–¢æ•°å®Ÿè£…å®Œäº†\n",
            "ğŸ“ final_compact_prompt.py ã®æ”¹è‰¯ç‰ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨\n",
            "ğŸ¯ ç‰¹å¾´: åˆ†é¡ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³æ—©æœŸé…ç½®ã€65ãƒ©ãƒ™ãƒ«å®Œå…¨å¯¾å¿œ\n"
          ]
        }
      ],
      "source": [
        "def get_actual_labels_from_data(train_df):\n",
        "    \"\"\"å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å…¨ãƒ©ãƒ™ãƒ«ã‚’å–å¾—ï¼ˆColabç”¨ã«ä¿®æ­£ï¼‰\"\"\"\n",
        "    labels = (\n",
        "        train_df[\"Category\"].astype(str)\n",
        "        + \":\"\n",
        "        + train_df[\"Misconception\"].fillna(\"NA\").astype(str)\n",
        "    )\n",
        "    return sorted(labels.unique())\n",
        "\n",
        "def get_improved_compact_prompt(question, answer, explanation, all_labels):\n",
        "    \"\"\"æ”¹è‰¯ç‰ˆã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ - final_compact_prompt.pyãƒ™ãƒ¼ã‚¹\"\"\"\n",
        "\n",
        "    labels_text = \"\\n\".join([f\"- {label}\" for label in all_labels])\n",
        "\n",
        "    prompt = f\"\"\"You are an expert math educator analyzing student responses for mathematical misconceptions.\n",
        "\n",
        "Question: {question}\n",
        "Correct Answer: {answer}\n",
        "Student's Explanation: {explanation}\n",
        "\n",
        "CLASSIFICATION GUIDELINES:\n",
        "â€¢ True_Correct:NA = Student demonstrates correct understanding\n",
        "â€¢ False_Correct:NA = Student gives correct answer but for wrong reasons\n",
        "â€¢ True_Neither:NA = Correct answer but unclear/incomplete reasoning\n",
        "â€¢ False_Neither:NA = Incorrect answer but no specific misconception identified\n",
        "â€¢ True_Misconception:[Type] = Correct answer but demonstrates specific misconception\n",
        "â€¢ False_Misconception:[Type] = Incorrect answer with identifiable misconception\n",
        "\n",
        "TASK: Classify this student's response using EXACTLY ONE of these {len(all_labels)} labels:\n",
        "\n",
        "{labels_text}\n",
        "\n",
        "Classification:\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def create_enhanced_text_with_improved_prompt(row, all_labels):\n",
        "    \"\"\"æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆ\"\"\"\n",
        "    question = str(row[\"QuestionText\"]) if pd.notna(row[\"QuestionText\"]) else \"\"\n",
        "    mc_answer = str(row[\"MC_Answer\"]) if pd.notna(row[\"MC_Answer\"]) else \"\"\n",
        "    explanation = str(row[\"StudentExplanation\"]) if pd.notna(row[\"StudentExplanation\"]) else \"\"\n",
        "\n",
        "    # æ”¹è‰¯ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼ã‚’ä½¿ç”¨\n",
        "    enhanced_text = get_improved_compact_prompt(question, mc_answer, explanation, all_labels)\n",
        "    return enhanced_text\n",
        "\n",
        "print(\"âœ… æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé–¢æ•°å®Ÿè£…å®Œäº†\")\n",
        "print(\"ğŸ“ final_compact_prompt.py ã®æ”¹è‰¯ç‰ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨\")\n",
        "print(\"ğŸ¯ ç‰¹å¾´: åˆ†é¡ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³æ—©æœŸé…ç½®ã€65ãƒ©ãƒ™ãƒ«å®Œå…¨å¯¾å¿œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b28c7af",
      "metadata": {
        "id": "5b28c7af"
      },
      "source": [
        "## ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æ”¹è‰¯å‰å‡¦ç†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75bab0e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75bab0e9",
        "outputId": "feef3b6c-2bc7-4bde-e50b-85dcd66638f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸ“Š æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\n",
            "============================================================\n",
            "âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: (36696, 7)\n",
            "ğŸ§¹ NaNé™¤å»ã¯è¡Œã„ã¾ã›ã‚“\n",
            "ğŸ“‹ å…¨ãƒ©ãƒ™ãƒ«æ•°: 65\n",
            "ğŸ¯ ä¸»è¦ãƒ©ãƒ™ãƒ«ä¾‹:\n",
            "  1. False_Correct:NA (227ä»¶)\n",
            "  2. False_Misconception:Adding_across (306ä»¶)\n",
            "  3. False_Misconception:Adding_terms (97ä»¶)\n",
            "  4. False_Misconception:Additive (891ä»¶)\n",
            "  5. False_Misconception:Base_rate (22ä»¶)\n",
            "  ... (60å€‹ã®è¿½åŠ ãƒ©ãƒ™ãƒ«)\n",
            "âœ… False_Correct:NAå«æœ‰: True\n",
            "\n",
            "ğŸ”§ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆä¸­...\n",
            "\n",
            "ğŸ“ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ:\n",
            "  å¹³å‡: 3089.1 æ–‡å­—\n",
            "  ä¸­å¤®å€¤: 3080.0 æ–‡å­—\n",
            "  æœ€å¤§: 3550 æ–‡å­—\n",
            "  æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆå¹³å‡ï¼‰: 772 ãƒˆãƒ¼ã‚¯ãƒ³\n",
            "  2048æ–‡å­—è¶…: 36696 (100.0%)\n"
          ]
        }
      ],
      "source": [
        "def load_and_prepare_improved_data():\n",
        "    \"\"\"æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ğŸ“Š æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ï¼ˆGoogle Driveå†…ï¼‰\n",
        "    train_data_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/map_data/train.csv\"\n",
        "\n",
        "    if not os.path.exists(train_data_path):\n",
        "        print(f\"âŒ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {train_data_path}\")\n",
        "        print(\"ğŸ’¡ Google Driveã®MyDrive/kaggleCompe_MAP-math/map_data/ã« train.csv ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
        "        train_df = pd.read_csv(train_data_path)\n",
        "        print(f\"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {train_df.shape}\")\n",
        "\n",
        "        # NaNå€¤é™¤å» - ã“ã®è¡Œã‚’å‰Šé™¤ã¾ãŸã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ\n",
        "        # before_len = len(train_df)\n",
        "        # train_df = train_df.dropna(subset=[\"Category\", \"StudentExplanation\", \"Misconception\"])\n",
        "        # after_len = len(train_df)\n",
        "        # print(f\"ğŸ§¹ NaNé™¤å»: {before_len} -> {after_len} ({before_len - after_len}è¡Œå‰Šé™¤)\")\n",
        "        print(\"ğŸ§¹ NaNé™¤å»ã¯è¡Œã„ã¾ã›ã‚“\")\n",
        "\n",
        "\n",
        "        # å…¨ãƒ©ãƒ™ãƒ«ã®å–å¾—ï¼ˆ65å€‹ï¼‰\n",
        "        # NaNã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œã™ã‚‹ãŸã‚ã€fillna(\"NA\")ã‚’ä½¿ç”¨\n",
        "        all_labels = (\n",
        "            train_df[\"Category\"].astype(str)\n",
        "            + \":\"\n",
        "            + train_df[\"Misconception\"].fillna(\"NA\").astype(str)\n",
        "        ).unique()\n",
        "        all_labels = sorted(all_labels) # ã‚½ãƒ¼ãƒˆ\n",
        "\n",
        "        print(f\"ğŸ“‹ å…¨ãƒ©ãƒ™ãƒ«æ•°: {len(all_labels)}\")\n",
        "\n",
        "        # ä¸»è¦ãƒ©ãƒ™ãƒ«ã®ç¢ºèª\n",
        "        print(\"ğŸ¯ ä¸»è¦ãƒ©ãƒ™ãƒ«ä¾‹:\")\n",
        "        # NaNã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œã™ã‚‹ãŸã‚ã€fillna(\"NA\")ã‚’ä½¿ç”¨\n",
        "        full_labels_series = train_df[\"Category\"].astype(str) + \":\" + train_df[\"Misconception\"].fillna(\"NA\").astype(str)\n",
        "        for i, label in enumerate(all_labels[:5]):\n",
        "            count = (full_labels_series == label).sum()\n",
        "            print(f\"  {i+1}. {label} ({count}ä»¶)\")\n",
        "        print(f\"  ... ({len(all_labels)-5}å€‹ã®è¿½åŠ ãƒ©ãƒ™ãƒ«)\")\n",
        "\n",
        "        # False_Correct:NAã®ç¢ºèª\n",
        "        false_correct_present = \"False_Correct:NA\" in all_labels\n",
        "        print(f\"âœ… False_Correct:NAå«æœ‰: {false_correct_present}\")\n",
        "\n",
        "        # æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆ\n",
        "        print(\"\\nğŸ”§ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆä¸­...\")\n",
        "        train_df[\"enhanced_text\"] = train_df.apply(\n",
        "            lambda row: create_enhanced_text_with_improved_prompt(row, all_labels),\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # ãƒ©ãƒ™ãƒ«ä½œæˆï¼ˆCategory:Misconceptionå½¢å¼ï¼‰\n",
        "        # NaNã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œã™ã‚‹ãŸã‚ã€fillna(\"NA\")ã‚’ä½¿ç”¨\n",
        "        train_df[\"full_label\"] = (\n",
        "            train_df[\"Category\"].astype(str)\n",
        "            + \":\"\n",
        "            + train_df[\"Misconception\"].fillna(\"NA\").astype(str)\n",
        "        )\n",
        "\n",
        "        # ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ\n",
        "        text_lengths = train_df[\"enhanced_text\"].str.len()\n",
        "        print(f\"\\nğŸ“ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ:\")\n",
        "        print(f\"  å¹³å‡: {text_lengths.mean():.1f} æ–‡å­—\")\n",
        "        print(f\"  ä¸­å¤®å€¤: {text_lengths.median():.1f} æ–‡å­—\")\n",
        "        print(f\"  æœ€å¤§: {text_lengths.max()} æ–‡å­—\")\n",
        "        print(f\"  æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆå¹³å‡ï¼‰: {text_lengths.mean() / 4:.0f} ãƒˆãƒ¼ã‚¯ãƒ³\")\n",
        "\n",
        "        # é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å‰²åˆ\n",
        "        long_prompts = (text_lengths > 2048).sum()\n",
        "        print(f\"  2048æ–‡å­—è¶…: {long_prompts} ({long_prompts/len(train_df)*100:.1f}%)\")\n",
        "\n",
        "        return train_df, all_labels\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¾ãŸã¯å‰å‡¦ç†ã«å¤±æ•—: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Ÿè¡Œ\n",
        "train_df, all_labels = load_and_prepare_improved_data()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a91629d2",
        "outputId": "9d0dad94-9bab-40c1-f92a-ab18b75c2c58"
      },
      "source": [
        "# ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¡¨ç¤º\n",
        "if train_df is not None and all_labels is not None:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹:\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰æœ€åˆã®è¡Œã‚’å–å¾—\n",
        "    sample_row = train_df.iloc[0]\n",
        "\n",
        "    # æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ\n",
        "    sample_prompt = create_enhanced_text_with_improved_prompt(sample_row, all_labels)\n",
        "\n",
        "    # ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¡¨ç¤º\n",
        "    print(sample_prompt)\n",
        "else:\n",
        "    print(\"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¾ãŸã¯ãƒ©ãƒ™ãƒ«ãŒæº–å‚™ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚\")"
      ],
      "id": "a91629d2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹:\n",
            "============================================================\n",
            "You are an expert math educator analyzing student responses for mathematical misconceptions.\n",
            "\n",
            "Question: What fraction of the shape is not shaded? Give your answer in its simplest form. [Image: A triangle split into 9 equal smaller triangles. 6 of them are shaded.]\n",
            "Correct Answer: \\( \\frac{1}{3} \\)\n",
            "Student's Explanation: 0ne third is equal to tree nineth\n",
            "\n",
            "CLASSIFICATION GUIDELINES:\n",
            "â€¢ True_Correct:NA = Student demonstrates correct understanding\n",
            "â€¢ False_Correct:NA = Student gives correct answer but for wrong reasons  \n",
            "â€¢ True_Neither:NA = Correct answer but unclear/incomplete reasoning\n",
            "â€¢ False_Neither:NA = Incorrect answer but no specific misconception identified\n",
            "â€¢ True_Misconception:[Type] = Correct answer but demonstrates specific misconception\n",
            "â€¢ False_Misconception:[Type] = Incorrect answer with identifiable misconception\n",
            "\n",
            "TASK: Classify this student's response using EXACTLY ONE of these 65 labels:\n",
            "\n",
            "- False_Correct:NA\n",
            "- False_Misconception:Adding_across\n",
            "- False_Misconception:Adding_terms\n",
            "- False_Misconception:Additive\n",
            "- False_Misconception:Base_rate\n",
            "- False_Misconception:Certainty\n",
            "- False_Misconception:Definition\n",
            "- False_Misconception:Denominator-only_change\n",
            "- False_Misconception:Division\n",
            "- False_Misconception:Duplication\n",
            "- False_Misconception:Firstterm\n",
            "- False_Misconception:FlipChange\n",
            "- False_Misconception:Ignores_zeroes\n",
            "- False_Misconception:Incomplete\n",
            "- False_Misconception:Incorrect_equivalent_fraction_addition\n",
            "- False_Misconception:Interior\n",
            "- False_Misconception:Inverse_operation\n",
            "- False_Misconception:Inversion\n",
            "- False_Misconception:Irrelevant\n",
            "- False_Misconception:Longer_is_bigger\n",
            "- False_Misconception:Mult\n",
            "- False_Misconception:Multiplying_by_4\n",
            "- False_Misconception:Not_variable\n",
            "- False_Misconception:Positive\n",
            "- False_Misconception:Scale\n",
            "- False_Misconception:Shorter_is_bigger\n",
            "- False_Misconception:Subtraction\n",
            "- False_Misconception:SwapDividend\n",
            "- False_Misconception:Tacking\n",
            "- False_Misconception:Unknowable\n",
            "- False_Misconception:WNB\n",
            "- False_Misconception:Whole_numbers_larger\n",
            "- False_Misconception:Wrong_Fraction\n",
            "- False_Misconception:Wrong_Operation\n",
            "- False_Misconception:Wrong_fraction\n",
            "- False_Misconception:Wrong_term\n",
            "- False_Neither:NA\n",
            "- True_Correct:NA\n",
            "- True_Misconception:Adding_across\n",
            "- True_Misconception:Additive\n",
            "- True_Misconception:Base_rate\n",
            "- True_Misconception:Definition\n",
            "- True_Misconception:Denominator-only_change\n",
            "- True_Misconception:Division\n",
            "- True_Misconception:Duplication\n",
            "- True_Misconception:Firstterm\n",
            "- True_Misconception:FlipChange\n",
            "- True_Misconception:Incomplete\n",
            "- True_Misconception:Incorrect_equivalent_fraction_addition\n",
            "- True_Misconception:Inversion\n",
            "- True_Misconception:Irrelevant\n",
            "- True_Misconception:Longer_is_bigger\n",
            "- True_Misconception:Mult\n",
            "- True_Misconception:Multiplying_by_4\n",
            "- True_Misconception:Not_variable\n",
            "- True_Misconception:Positive\n",
            "- True_Misconception:Shorter_is_bigger\n",
            "- True_Misconception:Subtraction\n",
            "- True_Misconception:SwapDividend\n",
            "- True_Misconception:Tacking\n",
            "- True_Misconception:WNB\n",
            "- True_Misconception:Whole_numbers_larger\n",
            "- True_Misconception:Wrong_fraction\n",
            "- True_Misconception:Wrong_term\n",
            "- True_Neither:NA\n",
            "\n",
            "Classification:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7acc2c4",
      "metadata": {
        "id": "c7acc2c4"
      },
      "source": [
        "## ğŸ¤– QLoRAå¯¾å¿œGemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "235c53ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503,
          "referenced_widgets": [
            "968c315157ed4c8fbcdcd2f9a894af3a",
            "6f9de74e59f849dc83ccaf134a86ed65",
            "cb10e4b9afb34651b03f7c221cf1a78b",
            "6a835575a45d4c4dae6bd853c48b99b7",
            "def1157f519b48858a0f9ddc570814ae",
            "488ddc4ab48a497485839d0c6335fb6b",
            "46ff47ad96f941be95e9c213f647a458",
            "eea9e37f68124d49879ff3d3004e15bc",
            "6c898975af71432cb9c9598e7c4861b2",
            "d58eafb8911f4035836877d531e62ca6",
            "5a97e8e323f54998b6107e9a3e940cd5"
          ]
        },
        "id": "235c53ee",
        "outputId": "993bf3b5-5c7f-4918-ebf8-e820502bfcc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ¤– QLoRAå¯¾å¿œGemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: google/gemma-2-2b-it\n",
            "============================================================\n",
            "âš¡ QLoRAé‡å­åŒ–è¨­å®š:\n",
            "  ğŸ”¸ 4-bité‡å­åŒ–: æœ‰åŠ¹\n",
            "  ğŸ”¸ ãƒ€ãƒ–ãƒ«é‡å­åŒ–: æœ‰åŠ¹\n",
            "  ğŸ”¸ é‡å­åŒ–ã‚¿ã‚¤ãƒ—: nf4\n",
            "  ğŸ”¸ è¨ˆç®—ç²¾åº¦: float16\n",
            "\n",
            "ğŸ“ Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...\n",
            "âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†\n",
            "ğŸ”– ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³: <pad>\n",
            "ğŸ“ èªå½™ã‚µã‚¤ã‚º: 256,000\n",
            "\n",
            "ğŸ§  QLoRAå¯¾å¿œGemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "968c315157ed4c8fbcdcd2f9a894af3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at google/gemma-2-2b-it and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… QLoRAå¯¾å¿œGemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†!\n",
            "\n",
            "ğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±:\n",
            "  ğŸ“ˆ ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 1,602,353,664\n",
            "  ğŸ“Š åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°: 65\n",
            "  ğŸ’¡ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: ~1.60B parameters\n",
            "  âš¡ é‡å­åŒ–: 4-bit (ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç´„75%å‰Šæ¸›)\n"
          ]
        }
      ],
      "source": [
        "def load_gemma_model_with_qlora():\n",
        "    \"\"\"QLoRAè¨­å®šã§Gemma-2-2b-itãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"ğŸ¤– QLoRAå¯¾å¿œGemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {MODEL_NAME}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # ğŸ”§ QLoRAç”¨é‡å­åŒ–è¨­å®š\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,                      # 4-bité‡å­åŒ–ã‚’æœ‰åŠ¹\n",
        "            bnb_4bit_use_double_quant=True,         # ãƒ€ãƒ–ãƒ«é‡å­åŒ–ã§ç²¾åº¦å‘ä¸Š\n",
        "            bnb_4bit_quant_type=\"nf4\",              # Normal Float 4-bit\n",
        "            bnb_4bit_compute_dtype=torch.float16,   # è¨ˆç®—ç²¾åº¦\n",
        "        )\n",
        "\n",
        "        print(\"âš¡ QLoRAé‡å­åŒ–è¨­å®š:\")\n",
        "        print(\"  ğŸ”¸ 4-bité‡å­åŒ–: æœ‰åŠ¹\")\n",
        "        print(\"  ğŸ”¸ ãƒ€ãƒ–ãƒ«é‡å­åŒ–: æœ‰åŠ¹\")\n",
        "        print(\"  ğŸ”¸ é‡å­åŒ–ã‚¿ã‚¤ãƒ—: nf4\")\n",
        "        print(\"  ğŸ”¸ è¨ˆç®—ç²¾åº¦: float16\")\n",
        "\n",
        "        # ğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿\n",
        "        print(\"\\nğŸ“ Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "            print(\"ğŸ”§ ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’EOSãƒˆãƒ¼ã‚¯ãƒ³ã«è¨­å®š\")\n",
        "\n",
        "        print(f\"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†\")\n",
        "        print(f\"ğŸ”– ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³: {tokenizer.pad_token}\")\n",
        "        print(f\"ğŸ“ èªå½™ã‚µã‚¤ã‚º: {tokenizer.vocab_size:,}\")\n",
        "\n",
        "        # ğŸ§  QLoRAå¯¾å¿œãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
        "        print(f\"\\nğŸ§  QLoRAå¯¾å¿œGemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "\n",
        "        config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "        config.num_labels = NUM_LABELS\n",
        "        config.problem_type = \"single_label_classification\"\n",
        "\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            config=config,\n",
        "            quantization_config=bnb_config,  # QLoRAé‡å­åŒ–è¨­å®š\n",
        "            device_map=\"auto\",               # è‡ªå‹•ãƒ‡ãƒã‚¤ã‚¹é…ç½®\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "\n",
        "        print(f\"âœ… QLoRAå¯¾å¿œGemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†!\")\n",
        "\n",
        "        # ğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        print(f\"\\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±:\")\n",
        "        print(f\"  ğŸ“ˆ ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}\")\n",
        "        print(f\"  ğŸ“Š åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°: {NUM_LABELS}\")\n",
        "        print(f\"  ğŸ’¡ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: ~{total_params / 1e9:.2f}B parameters\")\n",
        "        print(f\"  âš¡ é‡å­åŒ–: 4-bit (ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç´„75%å‰Šæ¸›)\")\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ QLoRAãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# QLoRAå¯¾å¿œãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Ÿè¡Œ\n",
        "if train_df is not None:\n",
        "    model, tokenizer = load_gemma_model_with_qlora()\n",
        "else:\n",
        "    print(\"âŒ ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™\")\n",
        "    model, tokenizer = None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34476115",
      "metadata": {
        "id": "34476115"
      },
      "source": [
        "## âš¡ QLoRAè¨­å®šã¨PEFTé©ç”¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c45c13b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c45c13b6",
        "outputId": "4cd250cd-6811-4bde-a2b7-824267dd8c30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "âš¡ QLoRA (PEFT) è¨­å®šã¨é©ç”¨\n",
            "============================================================\n",
            "ğŸ“¦ PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒãƒ¼ã‚¸ãƒ§ãƒ³: 0.16.0\n",
            "ğŸ”„ é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’QLoRAè¨“ç·´ç”¨ã«æº–å‚™ä¸­...\n",
            "âœ… é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«æº–å‚™å®Œäº†\n",
            "ğŸ“‹ QLoRAè¨­å®š:\n",
            "  ğŸ”¸ r (rank): 16\n",
            "  ğŸ”¸ lora_alpha: 32\n",
            "  ğŸ”¸ target_modules: {'gate_proj', 'down_proj', 'o_proj', 'v_proj', 'q_proj', 'k_proj', 'up_proj'}\n",
            "  ğŸ”¸ lora_dropout: 0.05\n",
            "  ğŸ”¸ task_type: SEQ_CLS\n",
            "  ğŸ”¸ modules_to_save: ['classifier', 'score']\n",
            "\n",
            "ğŸ”„ QLoRAãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...\n",
            "âœ… QLoRAãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†\n",
            "trainable params: 20,916,480 || all params: 2,635,408,128 || trainable%: 0.7937\n",
            "\n",
            "ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³:\n",
            "  ä½¿ç”¨ä¸­: 7.42 GB\n",
            "  ç·å®¹é‡: 42.47 GB\n",
            "  ä½¿ç”¨ç‡: 17.5%\n"
          ]
        }
      ],
      "source": [
        "def apply_qlora_to_model(model):\n",
        "    \"\"\"QLoRAè¨­å®šã¨PEFTé©ç”¨ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"âš¡ QLoRA (PEFT) è¨­å®šã¨é©ç”¨\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if model is None:\n",
        "        print(\"âŒ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        import peft\n",
        "        print(f\"ğŸ“¦ PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒãƒ¼ã‚¸ãƒ§ãƒ³: {peft.__version__}\")\n",
        "\n",
        "        # ğŸ”§ é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’QLoRAè¨“ç·´ç”¨ã«æº–å‚™\n",
        "        print(\"ğŸ”„ é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’QLoRAè¨“ç·´ç”¨ã«æº–å‚™ä¸­...\")\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "        print(\"âœ… é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«æº–å‚™å®Œäº†\")\n",
        "\n",
        "        # âš¡ QLoRAè¨­å®šï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–ï¼‰\n",
        "        qlora_config = LoraConfig(\n",
        "            r=16,                           # LoRA attention dimension\n",
        "            lora_alpha=32,                  # Alpha parameter for LoRA scaling\n",
        "            target_modules=[               # Gemma-2ã®ä¸»è¦ãªç·šå½¢å±¤ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ\n",
        "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "            ],\n",
        "            lora_dropout=0.05,             # Dropout probability for LoRA layers\n",
        "            bias=\"none\",                   # Bias type\n",
        "            task_type=\"SEQ_CLS\",           # Sequence Classification task\n",
        "            modules_to_save=[\"classifier\", \"score\"],  # åˆ†é¡ãƒ˜ãƒƒãƒ‰ã‚’ä¿å­˜\n",
        "        )\n",
        "\n",
        "        print(\"ğŸ“‹ QLoRAè¨­å®š:\")\n",
        "        print(f\"  ğŸ”¸ r (rank): {qlora_config.r}\")\n",
        "        print(f\"  ğŸ”¸ lora_alpha: {qlora_config.lora_alpha}\")\n",
        "        print(f\"  ğŸ”¸ target_modules: {qlora_config.target_modules}\")\n",
        "        print(f\"  ğŸ”¸ lora_dropout: {qlora_config.lora_dropout}\")\n",
        "        print(f\"  ğŸ”¸ task_type: {qlora_config.task_type}\")\n",
        "        print(f\"  ğŸ”¸ modules_to_save: {qlora_config.modules_to_save}\")\n",
        "\n",
        "        # ğŸš€ QLoRAãƒ¢ãƒ‡ãƒ«ä½œæˆ\n",
        "        print(\"\\nğŸ”„ QLoRAãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...\")\n",
        "        qlora_model = get_peft_model(model, qlora_config)\n",
        "\n",
        "        print(\"âœ… QLoRAãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†\")\n",
        "        qlora_model.print_trainable_parameters()\n",
        "\n",
        "        # ğŸ“Š ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª\n",
        "        if torch.cuda.is_available():\n",
        "            memory_used = torch.cuda.memory_allocated() / 1e9\n",
        "            memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "            print(f\"\\nğŸ’¾ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³:\")\n",
        "            print(f\"  ä½¿ç”¨ä¸­: {memory_used:.2f} GB\")\n",
        "            print(f\"  ç·å®¹é‡: {memory_total:.2f} GB\")\n",
        "            print(f\"  ä½¿ç”¨ç‡: {memory_used/memory_total*100:.1f}%\")\n",
        "\n",
        "        return qlora_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ QLoRAé©ç”¨å¤±æ•—: {e}\")\n",
        "        return None\n",
        "\n",
        "# QLoRAé©ç”¨\n",
        "if model is not None:\n",
        "    qlora_model = apply_qlora_to_model(model)\n",
        "else:\n",
        "    print(\"âŒ ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€QLoRAã‚’é©ç”¨ã§ãã¾ã›ã‚“\")\n",
        "    qlora_model = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e92dcddc",
      "metadata": {
        "id": "e92dcddc"
      },
      "source": [
        "## ğŸ“ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "987b9556",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "987b9556",
        "outputId": "c089a338-b23a-4180-97d8-6d6d51fd40af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹å®šç¾©å®Œäº†\n",
            "ğŸ“Š MAP@3è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆæ”¹è‰¯ç‰ˆï¼‰å®šç¾©å®Œäº†\n",
            "ğŸ¯ ç‰¹å¾´: é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯¾å¿œã€è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n"
          ]
        }
      ],
      "source": [
        "class ImprovedMathMisconceptionDataset(Dataset):\n",
        "    \"\"\"æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆ Math Misconception Dataset for PyTorch\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆé•·ã„ãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œï¼‰\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "def compute_improved_map3_metrics(eval_pred):\n",
        "    \"\"\"æ”¹è‰¯ç‰ˆMAP@3ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = torch.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
        "\n",
        "    map_scores = []\n",
        "    for i, true_label in enumerate(labels):\n",
        "        top3_indices = np.argsort(predictions[i])[::-1][:3]\n",
        "        score = 0.0\n",
        "        for j, pred_idx in enumerate(top3_indices):\n",
        "            if pred_idx == true_label:\n",
        "                score = 1.0 / (j + 1)\n",
        "                break\n",
        "        map_scores.append(score)\n",
        "\n",
        "    map3_score = np.mean(map_scores)\n",
        "    accuracy = accuracy_score(labels, np.argmax(predictions, axis=1))\n",
        "\n",
        "    return {\n",
        "        \"map3\": map3_score,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"map3_detailed\": f\"MAP@3: {map3_score:.4f}\"\n",
        "    }\n",
        "\n",
        "print(\"âœ… æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹å®šç¾©å®Œäº†\")\n",
        "print(\"ğŸ“Š MAP@3è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆæ”¹è‰¯ç‰ˆï¼‰å®šç¾©å®Œäº†\")\n",
        "print(\"ğŸ¯ ç‰¹å¾´: é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯¾å¿œã€è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a2f1337",
      "metadata": {
        "id": "8a2f1337"
      },
      "source": [
        "## ğŸš€ QLoRAæœ€é©åŒ–è¨“ç·´å®Ÿè¡Œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33354d93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "33354d93",
        "outputId": "88640941-426a-4192-c1fe-5d479d77988c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸš€ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹\n",
            "============================================================\n",
            "ğŸ“‹ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆãƒ©ãƒ™ãƒ«æƒ…å ±:\n",
            "  ç·ãƒ©ãƒ™ãƒ«æ•°: 65\n",
            "  ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ä¾‹ï¼ˆæœ€åˆã®5å€‹ï¼‰:\n",
            "    0: False_Correct:NA (227ä»¶)\n",
            "    1: False_Misconception:Adding_across (306ä»¶)\n",
            "    2: False_Misconception:Adding_terms (97ä»¶)\n",
            "    3: False_Misconception:Additive (891ä»¶)\n",
            "    4: False_Misconception:Base_rate (22ä»¶)\n",
            "    ... (60å€‹ã®è¿½åŠ ãƒ©ãƒ™ãƒ«)\n",
            "âœ… Regular splité©ç”¨ï¼ˆå°‘æ•°ã‚¯ãƒ©ã‚¹ã®ãŸã‚ï¼‰\n",
            "ğŸ“Š æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²:\n",
            "  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 29,356ä»¶\n",
            "  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 7,340ä»¶\n",
            "\n",
            "ğŸ“‹ QLoRAæœ€é©åŒ–è¨“ç·´è¨­å®š:\n",
            "  ğŸ”¸ ã‚¨ãƒãƒƒã‚¯æ•°: 3\n",
            "  ğŸ”¸ per_device_batch_size: 8\n",
            "  ğŸ”¸ gradient_accumulation: 4\n",
            "  ğŸ”¸ å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: 32\n",
            "  ğŸ”¸ å­¦ç¿’ç‡: 0.0001\n",
            "  ğŸ”¸ ä¿å­˜å…ˆ: /content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/gemma-2-2b-improved-prompts-qlora\n",
            "  ğŸ”¸ gradient_checkpointing: True\n",
            "  ğŸ”¸ group_by_length: True\n",
            "\n",
            "ğŸš€ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2754' max='2754' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2754/2754 7:36:45, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Map3</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Map3 Detailed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.274500</td>\n",
              "      <td>1.091040</td>\n",
              "      <td>0.778088</td>\n",
              "      <td>0.670027</td>\n",
              "      <td>MAP@3: 0.7781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.606400</td>\n",
              "      <td>0.573929</td>\n",
              "      <td>0.890100</td>\n",
              "      <td>0.800136</td>\n",
              "      <td>MAP@3: 0.8901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.562860</td>\n",
              "      <td>0.896094</td>\n",
              "      <td>0.813760</td>\n",
              "      <td>MAP@3: 0.8961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.443900</td>\n",
              "      <td>0.442795</td>\n",
              "      <td>0.915985</td>\n",
              "      <td>0.846185</td>\n",
              "      <td>MAP@3: 0.9160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.353500</td>\n",
              "      <td>0.390200</td>\n",
              "      <td>0.922525</td>\n",
              "      <td>0.856267</td>\n",
              "      <td>MAP@3: 0.9225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.347800</td>\n",
              "      <td>0.411833</td>\n",
              "      <td>0.928270</td>\n",
              "      <td>0.868392</td>\n",
              "      <td>MAP@3: 0.9283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.326900</td>\n",
              "      <td>0.369123</td>\n",
              "      <td>0.933401</td>\n",
              "      <td>0.875749</td>\n",
              "      <td>MAP@3: 0.9334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.341200</td>\n",
              "      <td>0.337746</td>\n",
              "      <td>0.935945</td>\n",
              "      <td>0.879428</td>\n",
              "      <td>MAP@3: 0.9359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.316900</td>\n",
              "      <td>0.360213</td>\n",
              "      <td>0.932811</td>\n",
              "      <td>0.872343</td>\n",
              "      <td>MAP@3: 0.9328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.178700</td>\n",
              "      <td>0.388439</td>\n",
              "      <td>0.936944</td>\n",
              "      <td>0.883243</td>\n",
              "      <td>MAP@3: 0.9369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.169900</td>\n",
              "      <td>0.387700</td>\n",
              "      <td>0.938783</td>\n",
              "      <td>0.885150</td>\n",
              "      <td>MAP@3: 0.9388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.173700</td>\n",
              "      <td>0.369085</td>\n",
              "      <td>0.941076</td>\n",
              "      <td>0.889373</td>\n",
              "      <td>MAP@3: 0.9411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.172100</td>\n",
              "      <td>0.372057</td>\n",
              "      <td>0.940872</td>\n",
              "      <td>0.888828</td>\n",
              "      <td>MAP@3: 0.9409</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†\n",
            "\n",
            "ğŸ“Š æœ€çµ‚è©•ä¾¡:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='918' max='918' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [918/918 07:52]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ğŸ¯ eval_loss: 0.3691\n",
            "  ğŸ¯ eval_map3: 0.9411\n",
            "  ğŸ¯ eval_accuracy: 0.8894\n",
            "  ğŸ“‹ eval_map3_detailed: MAP@3: 0.9411\n",
            "  ğŸ¯ eval_runtime: 473.0461\n",
            "  ğŸ¯ eval_samples_per_second: 15.5160\n",
            "  ğŸ¯ eval_steps_per_second: 1.9410\n",
            "  ğŸ¯ epoch: 3.0000\n"
          ]
        }
      ],
      "source": [
        "def train_improved_qlora_model(train_df, qlora_model, tokenizer, all_labels):\n",
        "    \"\"\"æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRA ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ğŸš€ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if train_df is None or qlora_model is None:\n",
        "        print(\"âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
        "        return None, None\n",
        "\n",
        "    # 65ãƒ©ãƒ™ãƒ«å¯¾å¿œã®ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
        "    label_encoder = LabelEncoder()\n",
        "    train_df[\"encoded_labels\"] = label_encoder.fit_transform(train_df[\"full_label\"])\n",
        "\n",
        "    print(f\"ğŸ“‹ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆãƒ©ãƒ™ãƒ«æƒ…å ±:\")\n",
        "    print(f\"  ç·ãƒ©ãƒ™ãƒ«æ•°: {len(label_encoder.classes_)}\")\n",
        "    print(f\"  ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ä¾‹ï¼ˆæœ€åˆã®5å€‹ï¼‰:\")\n",
        "    for i, label in enumerate(label_encoder.classes_[:5]):\n",
        "        count = (train_df[\"encoded_labels\"] == i).sum()\n",
        "        print(f\"    {i}: {label} ({count:,}ä»¶)\")\n",
        "    print(f\"    ... ({len(label_encoder.classes_)-5}å€‹ã®è¿½åŠ ãƒ©ãƒ™ãƒ«)\")\n",
        "\n",
        "    # è¨“ç·´/æ¤œè¨¼åˆ†å‰²\n",
        "    train_texts = train_df[\"enhanced_text\"].tolist()\n",
        "    train_labels = train_df[\"encoded_labels\"].tolist()\n",
        "\n",
        "    try:\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            train_texts,\n",
        "            train_labels,\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=train_labels,\n",
        "        )\n",
        "        print(\"âœ… Stratified splité©ç”¨\")\n",
        "    except ValueError:\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            train_texts, train_labels, test_size=0.2, random_state=42\n",
        "        )\n",
        "        print(\"âœ… Regular splité©ç”¨ï¼ˆå°‘æ•°ã‚¯ãƒ©ã‚¹ã®ãŸã‚ï¼‰\")\n",
        "\n",
        "    print(f\"ğŸ“Š æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²:\")\n",
        "    print(f\"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train):,}ä»¶\")\n",
        "    print(f\"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val):,}ä»¶\")\n",
        "\n",
        "    # æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
        "    train_dataset = ImprovedMathMisconceptionDataset(\n",
        "        X_train, y_train, tokenizer, max_length=MAX_LENGTH\n",
        "    )\n",
        "    val_dataset = ImprovedMathMisconceptionDataset(\n",
        "        X_val, y_val, tokenizer, max_length=MAX_LENGTH\n",
        "    )\n",
        "\n",
        "    # QLoRAæœ€é©åŒ–è¨“ç·´è¨­å®š\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"/content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/{NEW_MODEL_NAME}\",\n",
        "        num_train_epochs=3,                    # æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨æœ€é©ã‚¨ãƒãƒƒã‚¯æ•°\n",
        "        per_device_train_batch_size=8,         # QLoRAç”¨å°ãƒãƒƒãƒã‚µã‚¤ã‚º (8ã«å¤‰æ›´)\n",
        "        per_device_eval_batch_size=8,          # è©•ä¾¡ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚‚åˆã‚ã›ã¦å¤‰æ›´\n",
        "        gradient_accumulation_steps=4,         # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º32ã‚’ç¶­æŒ (4ã«å¤‰æ›´)\n",
        "        warmup_steps=200,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f\"/content/drive/MyDrive/kaggleCompe_MAP-math/logs/{NEW_MODEL_NAME}\",\n",
        "        logging_steps=50,                      # ã‚ˆã‚Šé »ç¹ãªãƒ­ã‚°\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=200,                        # ã‚ˆã‚Šé »ç¹ãªè©•ä¾¡\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=200,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"map3\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\",\n",
        "        dataloader_pin_memory=False,\n",
        "        fp16=False,                           # QLoRAã§ã¯ç„¡åŠ¹\n",
        "        bf16=True,                            # QLoRAæ¨å¥¨è¨­å®š\n",
        "        optim=\"adamw_torch\",\n",
        "        learning_rate=1e-4,                   # QLoRAç”¨å­¦ç¿’ç‡\n",
        "        save_total_limit=3,\n",
        "        gradient_checkpointing=True,          # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–\n",
        "        dataloader_num_workers=0,             # QLoRAå®‰å®šåŒ–\n",
        "        group_by_length=True,                 # åŠ¹ç‡çš„ãƒãƒƒãƒãƒ³ã‚°\n",
        "    )\n",
        "\n",
        "    print(\"\\nğŸ“‹ QLoRAæœ€é©åŒ–è¨“ç·´è¨­å®š:\")\n",
        "    print(f\"  ğŸ”¸ ã‚¨ãƒãƒƒã‚¯æ•°: {training_args.num_train_epochs}\")\n",
        "    print(f\"  ğŸ”¸ per_device_batch_size: {training_args.per_device_train_batch_size}\")\n",
        "    print(f\"  ğŸ”¸ gradient_accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "    print(f\"  ğŸ”¸ å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "    print(f\"  ğŸ”¸ å­¦ç¿’ç‡: {training_args.learning_rate}\")\n",
        "    print(f\"  ğŸ”¸ ä¿å­˜å…ˆ: {training_args.output_dir}\")\n",
        "    print(f\"  ğŸ”¸ gradient_checkpointing: {training_args.gradient_checkpointing}\")\n",
        "    print(f\"  ğŸ”¸ group_by_length: {training_args.group_by_length}\")\n",
        "\n",
        "    # QLoRAå¯¾å¿œTrainerä½œæˆ\n",
        "    trainer = Trainer(\n",
        "        model=qlora_model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "        compute_metrics=compute_improved_map3_metrics,\n",
        "    )\n",
        "\n",
        "    # è¨“ç·´å®Ÿè¡Œ\n",
        "    print(\"\\nğŸš€ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...\")\n",
        "    try:\n",
        "        trainer.train()\n",
        "        print(\"âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†\")\n",
        "\n",
        "        # æœ€çµ‚è©•ä¾¡\n",
        "        print(\"\\nğŸ“Š æœ€çµ‚è©•ä¾¡:\")\n",
        "        eval_results = trainer.evaluate()\n",
        "        for key, value in eval_results.items():\n",
        "            if isinstance(value, float):\n",
        "                print(f\"  ğŸ¯ {key}: {value:.4f}\")\n",
        "            else:\n",
        "                print(f\"  ğŸ“‹ {key}: {value}\")\n",
        "\n",
        "        return trainer, label_encoder\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRA è¨“ç·´å®Ÿè¡Œ\n",
        "if train_df is not None and qlora_model is not None and all_labels is not None:\n",
        "    trainer, label_encoder = train_improved_qlora_model(train_df, qlora_model, tokenizer, all_labels)\n",
        "else:\n",
        "    print(\"âŒ è¨“ç·´ã«å¿…è¦ãªè¦ç´ ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
        "    trainer, label_encoder = None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "680cb700",
      "metadata": {
        "id": "680cb700"
      },
      "source": [
        "## ğŸ’¾ æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a6b5c39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a6b5c39",
        "outputId": "57f6a4a8-1bc6-4981-d5cc-a79d0e0146db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ’¾ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRA ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
            "============================================================\n",
            "ğŸ“ ä¿å­˜å…ˆ: /content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/gemma-2-2b-improved-prompts-qlora-final\n",
            "âœ… æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRAãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†\n",
            "âœ… æ”¹è‰¯ç‰ˆãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ä¿å­˜å®Œäº†\n",
            "ğŸ“‹ ãƒ©ãƒ™ãƒ«ç·æ•°: 65\n",
            "âœ… False_Correcté–¢é€£ãƒ©ãƒ™ãƒ«: 1å€‹å«æœ‰\n",
            "âœ… æ”¹è‰¯ç‰ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†\n",
            "\n",
            "ğŸ“‚ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\n",
            "  ğŸ“„ README.md: 0.00 MB\n",
            "  ğŸ“„ adapter_model.safetensors: 79.84 MB\n",
            "  ğŸ“„ adapter_config.json: 0.00 MB\n",
            "  ğŸ“„ chat_template.jinja: 0.00 MB\n",
            "  ğŸ“„ tokenizer_config.json: 0.04 MB\n",
            "  ğŸ“„ special_tokens_map.json: 0.00 MB\n",
            "  ğŸ“„ tokenizer.model: 4.04 MB\n",
            "  ğŸ“„ tokenizer.json: 32.77 MB\n",
            "  ğŸ“„ training_args.bin: 0.01 MB\n",
            "  ğŸ“„ improved_label_mapping.json: 0.00 MB\n",
            "  ğŸ“„ improved_model_metadata.json: 0.00 MB\n",
            "\n",
            "ğŸ‰ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRAãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†!\n",
            "ğŸ“ Google Driveä¿å­˜ãƒ‘ã‚¹: /content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/\n",
            "ğŸš€ æ–°ãƒ¢ãƒ‡ãƒ«å: gemma-2-2b-improved-prompts-qlora\n",
            "ğŸ”¥ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: Kaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æº–å‚™å®Œäº†\n"
          ]
        }
      ],
      "source": [
        "def save_improved_qlora_model(trainer, tokenizer, label_encoder, all_labels):\n",
        "    \"\"\"æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRA è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ğŸ’¾ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRA ãƒ¢ãƒ‡ãƒ«ä¿å­˜\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if trainer is None or tokenizer is None or label_encoder is None:\n",
        "        print(\"âŒ ä¿å­˜ã«å¿…è¦ãªè¦ç´ ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
        "        return False\n",
        "\n",
        "    save_base_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/improved_models\"\n",
        "    model_save_path = f\"{save_base_path}/{NEW_MODEL_NAME}-final\"\n",
        "\n",
        "    try:\n",
        "        # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜\n",
        "        print(f\"ğŸ“ ä¿å­˜å…ˆ: {model_save_path}\")\n",
        "\n",
        "        # QLoRAãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆã‚¢ãƒ€ãƒ—ã‚¿ã®ã¿ï¼‰\n",
        "        trainer.save_model(model_save_path)\n",
        "        tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "        print(\"âœ… æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRAãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†\")\n",
        "\n",
        "        # æ”¹è‰¯ç‰ˆãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã®ä¿å­˜\n",
        "        label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
        "        label_file = f\"{model_save_path}/improved_label_mapping.json\"\n",
        "\n",
        "        with open(label_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(label_mapping, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(\"âœ… æ”¹è‰¯ç‰ˆãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ä¿å­˜å®Œäº†\")\n",
        "        print(f\"ğŸ“‹ ãƒ©ãƒ™ãƒ«ç·æ•°: {len(label_mapping)}\")\n",
        "\n",
        "        # False_Correct:NAã®ç¢ºèª\n",
        "        false_correct_labels = [label for label in label_mapping.values() if \"False_Correct\" in label]\n",
        "        print(f\"âœ… False_Correcté–¢é€£ãƒ©ãƒ™ãƒ«: {len(false_correct_labels)}å€‹å«æœ‰\")\n",
        "\n",
        "        # æ”¹è‰¯ç‰ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜\n",
        "        metadata = {\n",
        "            \"model_name\": MODEL_NAME,\n",
        "            \"improved_model_name\": NEW_MODEL_NAME,\n",
        "            \"num_labels\": len(all_labels),\n",
        "            \"max_length\": MAX_LENGTH,\n",
        "            \"task_type\": \"sequence_classification\",\n",
        "            \"architecture\": \"AutoModelForSequenceClassification\",\n",
        "            \"improvements\": {\n",
        "                \"prompt_optimization\": \"final_compact_prompt.py based\",\n",
        "                \"qlora_applied\": True,\n",
        "                \"quantization\": \"4-bit\",\n",
        "                \"label_coverage\": \"65 labels including False_Correct:NA\",\n",
        "                \"prompt_features\": [\n",
        "                    \"Early classification guidelines placement\",\n",
        "                    \"Enhanced context structure\",\n",
        "                    \"Comprehensive label coverage\",\n",
        "                    \"Optimized token efficiency\"\n",
        "                ]\n",
        "            },\n",
        "            \"qlora_config\": {\n",
        "                \"r\": 16,\n",
        "                \"lora_alpha\": 32,\n",
        "                \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                \"task_type\": \"SEQ_CLS\",\n",
        "                \"quantization\": \"4-bit nf4\",\n",
        "                \"compute_dtype\": \"float16\"\n",
        "            },\n",
        "            \"training_info\": {\n",
        "                \"epochs\": 3,\n",
        "                \"batch_size\": 2,\n",
        "                \"gradient_accumulation\": 16,\n",
        "                \"learning_rate\": 1e-4,\n",
        "                \"optimization\": \"QLoRA + improved prompts\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        metadata_file = f\"{model_save_path}/improved_model_metadata.json\"\n",
        "        with open(metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(\"âœ… æ”¹è‰¯ç‰ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†\")\n",
        "\n",
        "        # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
        "        print(f\"\\nğŸ“‚ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
        "        for item in os.listdir(model_save_path):\n",
        "            item_path = os.path.join(model_save_path, item)\n",
        "            if os.path.isfile(item_path):\n",
        "                size_mb = os.path.getsize(item_path) / (1024 * 1024)\n",
        "                print(f\"  ğŸ“„ {item}: {size_mb:.2f} MB\")\n",
        "            else:\n",
        "                print(f\"  ğŸ“ {item}/\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "        return False\n",
        "\n",
        "# æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Ÿè¡Œ\n",
        "if trainer is not None and tokenizer is not None and label_encoder is not None and all_labels is not None:\n",
        "    save_success = save_improved_qlora_model(trainer, tokenizer, label_encoder, all_labels)\n",
        "\n",
        "    if save_success:\n",
        "        print(\"\\nğŸ‰ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRAãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†!\")\n",
        "        print(f\"ğŸ“ Google Driveä¿å­˜ãƒ‘ã‚¹: /content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/\")\n",
        "        print(f\"ğŸš€ æ–°ãƒ¢ãƒ‡ãƒ«å: {NEW_MODEL_NAME}\")\n",
        "        print(\"ğŸ”¥ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: Kaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æº–å‚™å®Œäº†\")\n",
        "    else:\n",
        "        print(\"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
        "else:\n",
        "    print(\"âŒ ä¿å­˜ã«å¿…è¦ãªè¦ç´ ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f28f605",
      "metadata": {
        "id": "8f28f605"
      },
      "source": [
        "## ğŸ“¦ æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«zipä½œæˆã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c10bc69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "0c10bc69",
        "outputId": "981233d7-6fe7-4221-c5e5-585ff1894983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ“¦ æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ç”¨zipãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n",
            "============================================================\n",
            "ğŸ“ ã‚½ãƒ¼ã‚¹: /content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/gemma-2-2b-improved-prompts-qlora-final\n",
            "ğŸ“¦ å‡ºåŠ›zip: /content/drive/MyDrive/kaggleCompe_MAP-math/gemma-2-2b-improved-prompts-qlora.zip\n",
            "  â• README.md\n",
            "  â• adapter_model.safetensors\n",
            "  â• adapter_config.json\n",
            "  â• chat_template.jinja\n",
            "  â• tokenizer_config.json\n",
            "  â• special_tokens_map.json\n",
            "  â• tokenizer.model\n",
            "  â• tokenizer.json\n",
            "  â• training_args.bin\n",
            "  â• improved_label_mapping.json\n",
            "  â• improved_model_metadata.json\n",
            "\n",
            "âœ… æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«zipä½œæˆå®Œäº†!\n",
            "ğŸ“ zipã‚µã‚¤ã‚º: 81.08 MB\n",
            "ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‘ã‚¹: /content/drive/MyDrive/kaggleCompe_MAP-math/gemma-2-2b-improved-prompts-qlora.zip\n",
            "\n",
            "ğŸ’¡ Colabã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯:\n",
            "```python\n",
            "from google.colab import files\n",
            "files.download('/content/drive/MyDrive/kaggleCompe_MAP-math/gemma-2-2b-improved-prompts-qlora.zip')\n",
            "```\n",
            "\n",
            "ğŸ‰ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRAãƒ¢ãƒ‡ãƒ«zipä½œæˆå®Œäº†!\n",
            "ğŸ“¦ zipãƒ•ã‚¡ã‚¤ãƒ«: gemma-2-2b-improved-prompts-qlora.zip\n",
            "\n",
            "æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ã‚’ä»Šã™ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã‹ï¼Ÿ (y/n): y\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5e05ab96-285b-4422-9dfc-1dd722205e41\", \"gemma-2-2b-improved-prompts-qlora.zip\", 85017142)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹!\n"
          ]
        }
      ],
      "source": [
        "def create_improved_model_zip():\n",
        "    \"\"\"æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRAãƒ¢ãƒ‡ãƒ«ç”¨zipãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ğŸ“¦ æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ç”¨zipãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        import zipfile\n",
        "\n",
        "        model_dir = f\"/content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/{NEW_MODEL_NAME}-final\"\n",
        "        zip_path = f\"/content/drive/MyDrive/kaggleCompe_MAP-math/{NEW_MODEL_NAME}.zip\"\n",
        "\n",
        "        if not os.path.exists(model_dir):\n",
        "            print(f\"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_dir}\")\n",
        "            return False\n",
        "\n",
        "        print(f\"ğŸ“ ã‚½ãƒ¼ã‚¹: {model_dir}\")\n",
        "        print(f\"ğŸ“¦ å‡ºåŠ›zip: {zip_path}\")\n",
        "\n",
        "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for root, dirs, files in os.walk(model_dir):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    arcname = os.path.relpath(file_path, model_dir)\n",
        "                    zipf.write(file_path, arcname)\n",
        "                    print(f\"  â• {arcname}\")\n",
        "\n",
        "        # zipãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª\n",
        "        zip_size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
        "        print(f\"\\nâœ… æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«zipä½œæˆå®Œäº†!\")\n",
        "        print(f\"ğŸ“ zipã‚µã‚¤ã‚º: {zip_size_mb:.2f} MB\")\n",
        "        print(f\"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‘ã‚¹: {zip_path}\")\n",
        "\n",
        "        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã‚³ãƒ¼ãƒ‰è¡¨ç¤º\n",
        "        print(f\"\\nğŸ’¡ Colabã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯:\")\n",
        "        print(f\"```python\")\n",
        "        print(f\"from google.colab import files\")\n",
        "        print(f\"files.download('{zip_path}')\")\n",
        "        print(f\"```\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ zipä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "        return False\n",
        "\n",
        "# æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«zipä½œæˆå®Ÿè¡Œ\n",
        "zip_success = create_improved_model_zip()\n",
        "\n",
        "# è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "if zip_success:\n",
        "    print(\"\\nğŸ‰ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRAãƒ¢ãƒ‡ãƒ«zipä½œæˆå®Œäº†!\")\n",
        "    print(f\"ğŸ“¦ zipãƒ•ã‚¡ã‚¤ãƒ«: {NEW_MODEL_NAME}.zip\")\n",
        "\n",
        "    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç¢ºèª\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        response = input(\"\\næ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ã‚’ä»Šã™ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã‹ï¼Ÿ (y/n): \")\n",
        "        if response.lower() == 'y':\n",
        "            files.download(f'/content/drive/MyDrive/kaggleCompe_MAP-math/{NEW_MODEL_NAME}.zip')\n",
        "            print(\"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹!\")\n",
        "        else:\n",
        "            print(\"ğŸ’¡ å¾Œã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆã¯ä¸Šè¨˜ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„\")\n",
        "    except:\n",
        "        print(\"ğŸ’¡ æ‰‹å‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n",
        "else:\n",
        "    print(\"âŒ zipä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07011cd5",
      "metadata": {
        "id": "07011cd5"
      },
      "source": [
        "## ğŸ¯ æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«å®Œäº†ã‚µãƒãƒªãƒ¼\n",
        "\n",
        "### âœ… å®Œäº†ã—ãŸæ”¹è‰¯ç‚¹\n",
        "1. **ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹è‰¯**: `final_compact_prompt.py`ãƒ™ãƒ¼ã‚¹\n",
        "   - 65å€‹ã®å…¨ãƒ©ãƒ™ãƒ«å®Œå…¨å¯¾å¿œï¼ˆFalse_Correct:NAå«ã‚€ï¼‰\n",
        "   - åˆ†é¡ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã®æ—©æœŸé…ç½®\n",
        "   - å•é¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æœ€é©åŒ–æ§‹é€ \n",
        "   - ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡ã®æ”¹å–„\n",
        "\n",
        "2. **âš¡ QLoRAæœ€é©åŒ–**: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨æ€§èƒ½å‘ä¸Š\n",
        "   - 4-bité‡å­åŒ–ã«ã‚ˆã‚‹å¤§å¹…ãªãƒ¡ãƒ¢ãƒªå‰Šæ¸›\n",
        "   - LoRA (r=16, alpha=32) ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
        "   - å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–\n",
        "   - ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨accumulationã®æœ€é©ãƒãƒ©ãƒ³ã‚¹\n",
        "\n",
        "3. **ğŸš€ è¨“ç·´åŠ¹ç‡åŒ–**:\n",
        "   - QLoRAå¯¾å¿œã®å°‚ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
        "   - MAP@3ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ”¹è‰¯ç‰ˆå®Ÿè£…\n",
        "   - ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ãƒãƒƒãƒãƒ³ã‚°ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–\n",
        "   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å¤§å¹…å‰Šæ¸›\n",
        "\n",
        "### ğŸ“ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«\n",
        "- **æ–°ãƒ¢ãƒ‡ãƒ«å**: `gemma-2-2b-improved-prompts-qlora`\n",
        "- **Google Drive**: `/content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/`\n",
        "- **zipå½¢å¼**: `gemma-2-2b-improved-prompts-qlora.zip`\n",
        "- **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿**: æ”¹è‰¯å†…å®¹ã®è©³ç´°è¨˜éŒ²ä»˜ã\n",
        "\n",
        "### ğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆKaggleæå‡ºæº–å‚™ï¼‰\n",
        "1. **Kaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ**: æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ã‚’Kaggleã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
        "2. **æ¨è«–ãƒ†ã‚¹ãƒˆ**: test.csvã§ã®æ¨è«–æ€§èƒ½ç¢ºèª\n",
        "3. **submission.csvç”Ÿæˆ**: MAP@3æœ€é©åŒ–ã«ã‚ˆã‚‹æå‡º\n",
        "4. **æ€§èƒ½æ¯”è¼ƒ**: å…ƒãƒ¢ãƒ‡ãƒ«ã¨ã®æ€§èƒ½å·®åˆ†æ\n",
        "\n",
        "### ğŸ¯ æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿æ–¹æ³•ï¼ˆKaggleç”¨ï¼‰\n",
        "```python\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\"google/gemma-2-2b-it\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
        "\n",
        "# QLoRAã‚¢ãƒ€ãƒ—ã‚¿èª­ã¿è¾¼ã¿\n",
        "model = PeftModel.from_pretrained(base_model, \"path/to/improved_model\")\n",
        "```\n",
        "\n",
        "### ğŸš€ ä¸»ãªæ”¹å–„åŠ¹æœæœŸå¾…å€¤\n",
        "- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: ç´„75%å‰Šæ¸›ï¼ˆQLoRAåŠ¹æœï¼‰\n",
        "- **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç²¾åº¦**: False_Correct:NAå¯¾å¿œã«ã‚ˆã‚‹åˆ†é¡ç²¾åº¦å‘ä¸Š\n",
        "- **è¨“ç·´åŠ¹ç‡**: æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ˆã‚‹å­¦ç¿’åæŸæ€§å‘ä¸Š\n",
        "- **æ¨è«–é€Ÿåº¦**: æœ€é©åŒ–ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³é•·ã«ã‚ˆã‚‹é«˜é€ŸåŒ–\n",
        "\n",
        "**ğŸ‰ æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + QLoRAãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†!**\n",
        "**æ¬¡ã¯æå‡ºç”¨æ¨è«–ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã®æ€§èƒ½ãƒ†ã‚¹ãƒˆã§ã™ï¼**"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "968c315157ed4c8fbcdcd2f9a894af3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f9de74e59f849dc83ccaf134a86ed65",
              "IPY_MODEL_cb10e4b9afb34651b03f7c221cf1a78b",
              "IPY_MODEL_6a835575a45d4c4dae6bd853c48b99b7"
            ],
            "layout": "IPY_MODEL_def1157f519b48858a0f9ddc570814ae"
          }
        },
        "6f9de74e59f849dc83ccaf134a86ed65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_488ddc4ab48a497485839d0c6335fb6b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_46ff47ad96f941be95e9c213f647a458",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "cb10e4b9afb34651b03f7c221cf1a78b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eea9e37f68124d49879ff3d3004e15bc",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c898975af71432cb9c9598e7c4861b2",
            "value": 2
          }
        },
        "6a835575a45d4c4dae6bd853c48b99b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d58eafb8911f4035836877d531e62ca6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5a97e8e323f54998b6107e9a3e940cd5",
            "value": "â€‡2/2â€‡[00:05&lt;00:00,â€‡â€‡2.23s/it]"
          }
        },
        "def1157f519b48858a0f9ddc570814ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "488ddc4ab48a497485839d0c6335fb6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46ff47ad96f941be95e9c213f647a458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eea9e37f68124d49879ff3d3004e15bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c898975af71432cb9c9598e7c4861b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d58eafb8911f4035836877d531e62ca6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a97e8e323f54998b6107e9a3e940cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}