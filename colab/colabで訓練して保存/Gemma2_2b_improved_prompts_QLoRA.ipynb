{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dfa201a",
   "metadata": {},
   "source": [
    "# 🚀 Gemma-2-2b-it 改良プロンプト + QLoRA 訓練ノートブック\n",
    "\n",
    "## 📋 概要\n",
    "このノートブックは、**改良されたプロンプト**と**QLoRA (Quantized LoRA)** を使用して、MAP - Charting Student Math Misunderstandingsコンペティション用のGemma-2-2b-itモデルを効率的に訓練します。\n",
    "\n",
    "### 🆕 主な改良点\n",
    "1. **📝 改良プロンプト**: `final_compact_prompt.py`の最適化されたプロンプト構造\n",
    "   - 65個の全ラベル完全対応（False_Correct:NA含む）\n",
    "   - 分類ガイドラインの早期配置\n",
    "   - 問題コンテキストの最適化\n",
    "   \n",
    "2. **⚡ QLoRA最適化**: メモリ効率とトレーニング速度の向上\n",
    "   - 4-bit量子化による大幅なメモリ削減\n",
    "   - 高速な勾配計算\n",
    "   - GPU使用率の最適化\n",
    "\n",
    "### 🎯 モデル仕様\n",
    "- **ベースモデル**: google/gemma-2-2b-it (~2.6B parameters)\n",
    "- **タスク**: 65ラベル分類 (Category:Misconception形式)\n",
    "- **ファインチューニング**: QLoRA (Quantized Low-Rank Adaptation)\n",
    "- **プロンプト**: final_compact_prompt.py の改良版\n",
    "- **GPU**: T4/A100 対応\n",
    "\n",
    "### 💾 保存場所\n",
    "- **新モデル名**: `gemma-2-2b-improved-prompts-qlora`\n",
    "- **Google Drive**: `/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models/`\n",
    "- **圧縮ファイル**: zipダウンロード対応"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b8a9e9",
   "metadata": {},
   "source": [
    "## 📁 環境セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82fb8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveマウント\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 改良版モデル用ディレクトリ作成\n",
    "import os\n",
    "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/map_data', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/logs', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/improved_models', exist_ok=True)\n",
    "\n",
    "print(\"✅ Google Drive マウント完了\")\n",
    "print(\"✅ ディレクトリ作成完了\")\n",
    "print(\"📁 改良版モデル保存用ディレクトリ: /content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb6cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face認証\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97460a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA訓練用ライブラリのインストール\n",
    "!pip install peft accelerate transformers datasets bitsandbytes -U -q\n",
    "!pip install scipy -U -q  # QLoRAに必要\n",
    "\n",
    "print(\"✅ QLoRA対応ライブラリインストール完了\")\n",
    "print(\"📦 インストール済み: transformers, peft, accelerate, bitsandbytes, datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc97449b",
   "metadata": {},
   "source": [
    "## 📚 ライブラリ読み込みと設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573702eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import warnings\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 改良版モデル設定\n",
    "MODEL_NAME = \"google/gemma-2-2b-it\"\n",
    "NUM_LABELS = 65  # 全ラベル対応\n",
    "MAX_LENGTH = 512\n",
    "NEW_MODEL_NAME = \"gemma-2-2b-improved-prompts-qlora\"\n",
    "\n",
    "# デバイス設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🖥️ 使用デバイス: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"💾 GPU メモリ: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "print(f\"✅ ライブラリ読み込み完了\")\n",
    "print(f\"🚀 新モデル名: {NEW_MODEL_NAME}\")\n",
    "print(f\"📊 対応ラベル数: {NUM_LABELS}\")\n",
    "print(f\"📏 最大トークン長: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1350c255",
   "metadata": {},
   "source": [
    "## 📝 改良プロンプト関数の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee45a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_labels_from_data(train_df):\n",
    "    \"\"\"実際のデータから全ラベルを取得（Colab用に修正）\"\"\"\n",
    "    labels = (\n",
    "        train_df[\"Category\"].astype(str)\n",
    "        + \":\"\n",
    "        + train_df[\"Misconception\"].fillna(\"NA\").astype(str)\n",
    "    )\n",
    "    return sorted(labels.unique())\n",
    "\n",
    "def get_improved_compact_prompt(question, answer, explanation, all_labels):\n",
    "    \"\"\"改良版コンパクトプロンプト - final_compact_prompt.pyベース\"\"\"\n",
    "    \n",
    "    labels_text = \"\\n\".join([f\"- {label}\" for label in all_labels])\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert math educator analyzing student responses for mathematical misconceptions.\n",
    "\n",
    "Question: {question}\n",
    "Correct Answer: {answer}\n",
    "Student's Explanation: {explanation}\n",
    "\n",
    "CLASSIFICATION GUIDELINES:\n",
    "• True_Correct:NA = Student demonstrates correct understanding\n",
    "• False_Correct:NA = Student gives correct answer but for wrong reasons  \n",
    "• True_Neither:NA = Correct answer but unclear/incomplete reasoning\n",
    "• False_Neither:NA = Incorrect answer but no specific misconception identified\n",
    "• True_Misconception:[Type] = Correct answer but demonstrates specific misconception\n",
    "• False_Misconception:[Type] = Incorrect answer with identifiable misconception\n",
    "\n",
    "TASK: Classify this student's response using EXACTLY ONE of these {len(all_labels)} labels:\n",
    "\n",
    "{labels_text}\n",
    "\n",
    "Classification:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def create_enhanced_text_with_improved_prompt(row, all_labels):\n",
    "    \"\"\"改良プロンプトを使用したテキスト特徴量作成\"\"\"\n",
    "    question = str(row[\"QuestionText\"]) if pd.notna(row[\"QuestionText\"]) else \"\"\n",
    "    mc_answer = str(row[\"MC_Answer\"]) if pd.notna(row[\"MC_Answer\"]) else \"\"\n",
    "    explanation = str(row[\"StudentExplanation\"]) if pd.notna(row[\"StudentExplanation\"]) else \"\"\n",
    "    \n",
    "    # 改良されたプロンプト形式を使用\n",
    "    enhanced_text = get_improved_compact_prompt(question, mc_answer, explanation, all_labels)\n",
    "    return enhanced_text\n",
    "\n",
    "print(\"✅ 改良プロンプト関数実装完了\")\n",
    "print(\"📝 final_compact_prompt.py の改良版プロンプトを使用\")\n",
    "print(\"🎯 特徴: 分類ガイドライン早期配置、65ラベル完全対応\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28c7af",
   "metadata": {},
   "source": [
    "## 📊 データ読み込みと改良前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bab0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_improved_data():\n",
    "    \"\"\"改良プロンプトを使用したデータ読み込みと前処理\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"📊 改良プロンプト版 データ読み込みと前処理\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # データパス（Google Drive内）\n",
    "    train_data_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/map_data/train.csv\"\n",
    "\n",
    "    if not os.path.exists(train_data_path):\n",
    "        print(f\"❌ データが見つかりません: {train_data_path}\")\n",
    "        print(\"💡 Google DriveのMyDrive/kaggleCompe_MAP-math/map_data/に train.csv をアップロードしてください\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        # データ読み込み\n",
    "        train_df = pd.read_csv(train_data_path)\n",
    "        print(f\"✅ 訓練データ読み込み: {train_df.shape}\")\n",
    "\n",
    "        # NaN値除去\n",
    "        before_len = len(train_df)\n",
    "        train_df = train_df.dropna(subset=[\"Category\", \"StudentExplanation\", \"Misconception\"])\n",
    "        after_len = len(train_df)\n",
    "        print(f\"🧹 NaN除去: {before_len} -> {after_len} ({before_len - after_len}行削除)\")\n",
    "\n",
    "        # 全ラベルの取得（65個）\n",
    "        all_labels = get_actual_labels_from_data(train_df)\n",
    "        print(f\"📋 全ラベル数: {len(all_labels)}\")\n",
    "        \n",
    "        # 主要ラベルの確認\n",
    "        print(\"🎯 主要ラベル例:\")\n",
    "        for i, label in enumerate(all_labels[:5]):\n",
    "            count = (train_df[\"Category\"].astype(str) + \":\" + train_df[\"Misconception\"].fillna(\"NA\").astype(str) == label).sum()\n",
    "            print(f\"  {i+1}. {label} ({count}件)\")\n",
    "        print(f\"  ... ({len(all_labels)-5}個の追加ラベル)\")\n",
    "\n",
    "        # False_Correct:NAの確認\n",
    "        false_correct_present = \"False_Correct:NA\" in all_labels\n",
    "        print(f\"✅ False_Correct:NA含有: {false_correct_present}\")\n",
    "\n",
    "        # 改良プロンプトを使用したテキスト特徴量作成\n",
    "        print(\"\\n🔧 改良プロンプトによるテキスト特徴量作成中...\")\n",
    "        train_df[\"enhanced_text\"] = train_df.apply(\n",
    "            lambda row: create_enhanced_text_with_improved_prompt(row, all_labels), \n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # ラベル作成（Category:Misconception形式）\n",
    "        train_df[\"full_label\"] = (\n",
    "            train_df[\"Category\"].astype(str) \n",
    "            + \":\" \n",
    "            + train_df[\"Misconception\"].fillna(\"NA\").astype(str)\n",
    "        )\n",
    "\n",
    "        # テキスト長統計\n",
    "        text_lengths = train_df[\"enhanced_text\"].str.len()\n",
    "        print(f\"\\n📝 改良プロンプトのテキスト長統計:\")\n",
    "        print(f\"  平均: {text_lengths.mean():.1f} 文字\")\n",
    "        print(f\"  中央値: {text_lengths.median():.1f} 文字\")\n",
    "        print(f\"  最大: {text_lengths.max()} 文字\")\n",
    "        print(f\"  推定トークン数（平均）: {text_lengths.mean() / 4:.0f} トークン\")\n",
    "        \n",
    "        # 長いプロンプトの割合\n",
    "        long_prompts = (text_lengths > 2048).sum()\n",
    "        print(f\"  2048文字超: {long_prompts} ({long_prompts/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "        return train_df, all_labels\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ データ読み込みまたは前処理に失敗: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# データ読み込み実行\n",
    "train_df, all_labels = load_and_prepare_improved_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7acc2c4",
   "metadata": {},
   "source": [
    "## 🤖 QLoRA対応Gemmaモデル読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gemma_model_with_qlora():\n",
    "    \"\"\"QLoRA設定でGemma-2-2b-itモデルとトークナイザーを読み込み\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"🤖 QLoRA対応Gemmaモデル読み込み: {MODEL_NAME}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # 🔧 QLoRA用量子化設定\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,                      # 4-bit量子化を有効\n",
    "            bnb_4bit_use_double_quant=True,         # ダブル量子化で精度向上\n",
    "            bnb_4bit_quant_type=\"nf4\",              # Normal Float 4-bit\n",
    "            bnb_4bit_compute_dtype=torch.float16,   # 計算精度\n",
    "        )\n",
    "        \n",
    "        print(\"⚡ QLoRA量子化設定:\")\n",
    "        print(\"  🔸 4-bit量子化: 有効\")\n",
    "        print(\"  🔸 ダブル量子化: 有効\")  \n",
    "        print(\"  🔸 量子化タイプ: nf4\")\n",
    "        print(\"  🔸 計算精度: float16\")\n",
    "\n",
    "        # 📝 トークナイザー読み込み\n",
    "        print(\"\\n📝 Gemmaトークナイザー読み込み中...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            print(\"🔧 パディングトークンをEOSトークンに設定\")\n",
    "\n",
    "        print(f\"✅ トークナイザー読み込み完了\")\n",
    "        print(f\"🔖 パディングトークン: {tokenizer.pad_token}\")\n",
    "        print(f\"📏 語彙サイズ: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "        # 🧠 QLoRA対応モデル読み込み\n",
    "        print(f\"\\n🧠 QLoRA対応Gemmaモデル読み込み中...\")\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "        config.num_labels = NUM_LABELS\n",
    "        config.problem_type = \"single_label_classification\"\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            config=config,\n",
    "            quantization_config=bnb_config,  # QLoRA量子化設定\n",
    "            device_map=\"auto\",               # 自動デバイス配置\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        print(f\"✅ QLoRA対応Gemmaモデル読み込み完了!\")\n",
    "\n",
    "        # 📊 モデル情報表示\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"\\n📊 モデル情報:\")\n",
    "        print(f\"  📈 総パラメータ数: {total_params:,}\")\n",
    "        print(f\"  📊 分類クラス数: {NUM_LABELS}\")\n",
    "        print(f\"  💡 モデルサイズ: ~{total_params / 1e9:.2f}B parameters\")\n",
    "        print(f\"  ⚡ 量子化: 4-bit (メモリ使用量約75%削減)\")\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ QLoRAモデル読み込み失敗: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# QLoRA対応モデル読み込み実行\n",
    "if train_df is not None:\n",
    "    model, tokenizer = load_gemma_model_with_qlora()\n",
    "else:\n",
    "    print(\"❌ データが読み込まれていないため、モデル読み込みをスキップします\")\n",
    "    model, tokenizer = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34476115",
   "metadata": {},
   "source": [
    "## ⚡ QLoRA設定とPEFT適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_qlora_to_model(model):\n",
    "    \"\"\"QLoRA設定とPEFT適用（メモリ効率化）\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"⚡ QLoRA (PEFT) 設定と適用\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if model is None:\n",
    "        print(\"❌ ベースモデルが準備されていません\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        import peft\n",
    "        print(f\"📦 PEFTライブラリバージョン: {peft.__version__}\")\n",
    "\n",
    "        # 🔧 量子化モデルをQLoRA訓練用に準備\n",
    "        print(\"🔄 量子化モデルをQLoRA訓練用に準備中...\")\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        print(\"✅ 量子化モデル準備完了\")\n",
    "\n",
    "        # ⚡ QLoRA設定（メモリ効率重視）\n",
    "        qlora_config = LoraConfig(\n",
    "            r=16,                           # LoRA attention dimension  \n",
    "            lora_alpha=32,                  # Alpha parameter for LoRA scaling\n",
    "            target_modules=[               # Gemma-2の主要な線形層をターゲット\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "            ],\n",
    "            lora_dropout=0.05,             # Dropout probability for LoRA layers\n",
    "            bias=\"none\",                   # Bias type\n",
    "            task_type=\"SEQ_CLS\",           # Sequence Classification task\n",
    "            modules_to_save=[\"classifier\", \"score\"],  # 分類ヘッドを保存\n",
    "        )\n",
    "\n",
    "        print(\"📋 QLoRA設定:\")\n",
    "        print(f\"  🔸 r (rank): {qlora_config.r}\")\n",
    "        print(f\"  🔸 lora_alpha: {qlora_config.lora_alpha}\")\n",
    "        print(f\"  🔸 target_modules: {qlora_config.target_modules}\")\n",
    "        print(f\"  🔸 lora_dropout: {qlora_config.lora_dropout}\")\n",
    "        print(f\"  🔸 task_type: {qlora_config.task_type}\")\n",
    "        print(f\"  🔸 modules_to_save: {qlora_config.modules_to_save}\")\n",
    "\n",
    "        # 🚀 QLoRAモデル作成\n",
    "        print(\"\\n🔄 QLoRAモデル作成中...\")\n",
    "        qlora_model = get_peft_model(model, qlora_config)\n",
    "\n",
    "        print(\"✅ QLoRAモデル作成完了\")\n",
    "        qlora_model.print_trainable_parameters()\n",
    "\n",
    "        # 📊 メモリ使用量確認\n",
    "        if torch.cuda.is_available():\n",
    "            memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "            memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            print(f\"\\n💾 GPU メモリ使用状況:\")\n",
    "            print(f\"  使用中: {memory_used:.2f} GB\")\n",
    "            print(f\"  総容量: {memory_total:.2f} GB\")\n",
    "            print(f\"  使用率: {memory_used/memory_total*100:.1f}%\")\n",
    "\n",
    "        return qlora_model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ QLoRA適用失敗: {e}\")\n",
    "        return None\n",
    "\n",
    "# QLoRA適用\n",
    "if model is not None:\n",
    "    qlora_model = apply_qlora_to_model(model)\n",
    "else:\n",
    "    print(\"❌ モデルが読み込まれていないため、QLoRAを適用できません\")\n",
    "    qlora_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92dcddc",
   "metadata": {},
   "source": [
    "## 📝 改良プロンプト版データセット作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedMathMisconceptionDataset(Dataset):\n",
    "    \"\"\"改良プロンプト版 Math Misconception Dataset for PyTorch\"\"\"\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 改良プロンプトのトークン化（長いテキスト対応）\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def compute_improved_map3_metrics(eval_pred):\n",
    "    \"\"\"改良版MAP@3メトリクスの計算\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = torch.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
    "\n",
    "    map_scores = []\n",
    "    for i, true_label in enumerate(labels):\n",
    "        top3_indices = np.argsort(predictions[i])[::-1][:3]\n",
    "        score = 0.0\n",
    "        for j, pred_idx in enumerate(top3_indices):\n",
    "            if pred_idx == true_label:\n",
    "                score = 1.0 / (j + 1)\n",
    "                break\n",
    "        map_scores.append(score)\n",
    "\n",
    "    map3_score = np.mean(map_scores)\n",
    "    accuracy = accuracy_score(labels, np.argmax(predictions, axis=1))\n",
    "\n",
    "    return {\n",
    "        \"map3\": map3_score, \n",
    "        \"accuracy\": accuracy,\n",
    "        \"map3_detailed\": f\"MAP@3: {map3_score:.4f}\"\n",
    "    }\n",
    "\n",
    "print(\"✅ 改良プロンプト版データセットクラス定義完了\")\n",
    "print(\"📊 MAP@3評価メトリクス（改良版）定義完了\")\n",
    "print(\"🎯 特徴: 長いプロンプト対応、詳細メトリクス\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f1337",
   "metadata": {},
   "source": [
    "## 🚀 QLoRA最適化訓練実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33354d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_improved_qlora_model(train_df, qlora_model, tokenizer, all_labels):\n",
    "    \"\"\"改良プロンプト + QLoRA モデルのファインチューニング実行\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🚀 改良プロンプト + QLoRA ファインチューニング開始\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if train_df is None or qlora_model is None:\n",
    "        print(\"❌ 訓練データまたはモデルが準備されていません\")\n",
    "        return None, None\n",
    "\n",
    "    # 65ラベル対応のラベルエンコーディング\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_df[\"encoded_labels\"] = label_encoder.fit_transform(train_df[\"full_label\"])\n",
    "\n",
    "    print(f\"📋 改良プロンプト版ラベル情報:\")\n",
    "    print(f\"  総ラベル数: {len(label_encoder.classes_)}\")\n",
    "    print(f\"  エンコード例（最初の5個）:\")\n",
    "    for i, label in enumerate(label_encoder.classes_[:5]):\n",
    "        count = (train_df[\"encoded_labels\"] == i).sum()\n",
    "        print(f\"    {i}: {label} ({count:,}件)\")\n",
    "    print(f\"    ... ({len(label_encoder.classes_)-5}個の追加ラベル)\")\n",
    "\n",
    "    # 訓練/検証分割\n",
    "    train_texts = train_df[\"enhanced_text\"].tolist()\n",
    "    train_labels = train_df[\"encoded_labels\"].tolist()\n",
    "\n",
    "    try:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            train_texts,\n",
    "            train_labels,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=train_labels,\n",
    "        )\n",
    "        print(\"✅ Stratified split適用\")\n",
    "    except ValueError:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            train_texts, train_labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "        print(\"✅ Regular split適用（少数クラスのため）\")\n",
    "\n",
    "    print(f\"📊 改良プロンプト版データ分割:\")\n",
    "    print(f\"  訓練データ: {len(X_train):,}件\")\n",
    "    print(f\"  検証データ: {len(X_val):,}件\")\n",
    "\n",
    "    # 改良プロンプト版データセット作成\n",
    "    train_dataset = ImprovedMathMisconceptionDataset(\n",
    "        X_train, y_train, tokenizer, max_length=MAX_LENGTH\n",
    "    )\n",
    "    val_dataset = ImprovedMathMisconceptionDataset(\n",
    "        X_val, y_val, tokenizer, max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "    # QLoRA最適化訓練設定\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"/content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/{NEW_MODEL_NAME}\",\n",
    "        num_train_epochs=3,                    # 改良プロンプト用最適エポック数\n",
    "        per_device_train_batch_size=2,         # QLoRA用小バッチサイズ\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=16,        # 実効バッチサイズ32を維持\n",
    "        warmup_steps=200,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"/content/drive/MyDrive/kaggleCompe_MAP-math/logs/{NEW_MODEL_NAME}\",\n",
    "        logging_steps=50,                      # より頻繁なログ\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,                        # より頻繁な評価\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"map3\",\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\",\n",
    "        dataloader_pin_memory=False,\n",
    "        fp16=False,                           # QLoRAでは無効\n",
    "        bf16=True,                            # QLoRA推奨設定\n",
    "        optim=\"adamw_torch\",\n",
    "        learning_rate=1e-4,                   # QLoRA用学習率\n",
    "        save_total_limit=3,\n",
    "        gradient_checkpointing=True,          # メモリ効率化\n",
    "        dataloader_num_workers=0,             # QLoRA安定化\n",
    "        group_by_length=True,                 # 効率的バッチング\n",
    "    )\n",
    "\n",
    "    print(\"\\n📋 QLoRA最適化訓練設定:\")\n",
    "    print(f\"  🔸 エポック数: {training_args.num_train_epochs}\")\n",
    "    print(f\"  🔸 per_device_batch_size: {training_args.per_device_train_batch_size}\")\n",
    "    print(f\"  🔸 gradient_accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "    print(f\"  🔸 実効バッチサイズ: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "    print(f\"  🔸 学習率: {training_args.learning_rate}\")\n",
    "    print(f\"  🔸 保存先: {training_args.output_dir}\")\n",
    "    print(f\"  🔸 gradient_checkpointing: {training_args.gradient_checkpointing}\")\n",
    "    print(f\"  🔸 group_by_length: {training_args.group_by_length}\")\n",
    "\n",
    "    # QLoRA対応Trainer作成\n",
    "    trainer = Trainer(\n",
    "        model=qlora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_improved_map3_metrics,\n",
    "    )\n",
    "\n",
    "    # 訓練実行\n",
    "    print(\"\\n🚀 改良プロンプト + QLoRA ファインチューニング開始...\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"✅ ファインチューニング完了\")\n",
    "\n",
    "        # 最終評価\n",
    "        print(\"\\n📊 最終評価:\")\n",
    "        eval_results = trainer.evaluate()\n",
    "        for key, value in eval_results.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  🎯 {key}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"  📋 {key}: {value}\")\n",
    "\n",
    "        return trainer, label_encoder\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ファインチューニング中にエラー: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# 改良プロンプト + QLoRA 訓練実行\n",
    "if train_df is not None and qlora_model is not None and all_labels is not None:\n",
    "    trainer, label_encoder = train_improved_qlora_model(train_df, qlora_model, tokenizer, all_labels)\n",
    "else:\n",
    "    print(\"❌ 訓練に必要な要素が準備されていません\")\n",
    "    trainer, label_encoder = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680cb700",
   "metadata": {},
   "source": [
    "## 💾 改良版モデル保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b5c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_improved_qlora_model(trainer, tokenizer, label_encoder, all_labels):\n",
    "    \"\"\"改良プロンプト + QLoRA 訓練済みモデルの保存\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"💾 改良プロンプト + QLoRA モデル保存\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if trainer is None or tokenizer is None or label_encoder is None:\n",
    "        print(\"❌ 保存に必要な要素が準備されていません\")\n",
    "        return False\n",
    "\n",
    "    save_base_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/improved_models\"\n",
    "    model_save_path = f\"{save_base_path}/{NEW_MODEL_NAME}-final\"\n",
    "\n",
    "    try:\n",
    "        # モデルとトークナイザーの保存\n",
    "        print(f\"📁 保存先: {model_save_path}\")\n",
    "\n",
    "        # QLoRAモデル保存（アダプタのみ）\n",
    "        trainer.save_model(model_save_path)\n",
    "        tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "        print(\"✅ 改良プロンプト + QLoRAモデル保存完了\")\n",
    "\n",
    "        # 改良版ラベルマッピングの保存\n",
    "        label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "        label_file = f\"{model_save_path}/improved_label_mapping.json\"\n",
    "\n",
    "        with open(label_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(label_mapping, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(\"✅ 改良版ラベルマッピング保存完了\")\n",
    "        print(f\"📋 ラベル総数: {len(label_mapping)}\")\n",
    "        \n",
    "        # False_Correct:NAの確認\n",
    "        false_correct_labels = [label for label in label_mapping.values() if \"False_Correct\" in label]\n",
    "        print(f\"✅ False_Correct関連ラベル: {len(false_correct_labels)}個含有\")\n",
    "\n",
    "        # 改良版メタデータの保存\n",
    "        metadata = {\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"improved_model_name\": NEW_MODEL_NAME,\n",
    "            \"num_labels\": len(all_labels),\n",
    "            \"max_length\": MAX_LENGTH,\n",
    "            \"task_type\": \"sequence_classification\",\n",
    "            \"architecture\": \"AutoModelForSequenceClassification\",\n",
    "            \"improvements\": {\n",
    "                \"prompt_optimization\": \"final_compact_prompt.py based\",\n",
    "                \"qlora_applied\": True,\n",
    "                \"quantization\": \"4-bit\",\n",
    "                \"label_coverage\": \"65 labels including False_Correct:NA\",\n",
    "                \"prompt_features\": [\n",
    "                    \"Early classification guidelines placement\",\n",
    "                    \"Enhanced context structure\", \n",
    "                    \"Comprehensive label coverage\",\n",
    "                    \"Optimized token efficiency\"\n",
    "                ]\n",
    "            },\n",
    "            \"qlora_config\": {\n",
    "                \"r\": 16,\n",
    "                \"lora_alpha\": 32,\n",
    "                \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "                \"task_type\": \"SEQ_CLS\",\n",
    "                \"quantization\": \"4-bit nf4\",\n",
    "                \"compute_dtype\": \"float16\"\n",
    "            },\n",
    "            \"training_info\": {\n",
    "                \"epochs\": 3,\n",
    "                \"batch_size\": 2,\n",
    "                \"gradient_accumulation\": 16,\n",
    "                \"learning_rate\": 1e-4,\n",
    "                \"optimization\": \"QLoRA + improved prompts\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        metadata_file = f\"{model_save_path}/improved_model_metadata.json\"\n",
    "        with open(metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(\"✅ 改良版メタデータ保存完了\")\n",
    "\n",
    "        # 保存されたファイルの確認\n",
    "        print(f\"\\n📂 保存されたファイル:\")\n",
    "        for item in os.listdir(model_save_path):\n",
    "            item_path = os.path.join(model_save_path, item)\n",
    "            if os.path.isfile(item_path):\n",
    "                size_mb = os.path.getsize(item_path) / (1024 * 1024)\n",
    "                print(f\"  📄 {item}: {size_mb:.2f} MB\")\n",
    "            else:\n",
    "                print(f\"  📁 {item}/\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ モデル保存中にエラー: {e}\")\n",
    "        return False\n",
    "\n",
    "# 改良版モデル保存実行\n",
    "if trainer is not None and tokenizer is not None and label_encoder is not None and all_labels is not None:\n",
    "    save_success = save_improved_qlora_model(trainer, tokenizer, label_encoder, all_labels)\n",
    "\n",
    "    if save_success:\n",
    "        print(\"\\n🎉 改良プロンプト + QLoRAモデル保存完了!\")\n",
    "        print(f\"📁 Google Drive保存パス: /content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/\")\n",
    "        print(f\"🚀 新モデル名: {NEW_MODEL_NAME}\")\n",
    "        print(\"🔥 次のステップ: Kaggleデータセットとしてアップロード準備完了\")\n",
    "    else:\n",
    "        print(\"❌ モデル保存に失敗しました\")\n",
    "else:\n",
    "    print(\"❌ 保存に必要な要素が準備されていません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f28f605",
   "metadata": {},
   "source": [
    "## 📦 改良版モデルzip作成とダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c10bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_improved_model_zip():\n",
    "    \"\"\"改良プロンプト + QLoRAモデル用zipファイル作成\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"📦 改良版モデル用zipファイル作成\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        import zipfile\n",
    "\n",
    "        model_dir = f\"/content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/{NEW_MODEL_NAME}-final\"\n",
    "        zip_path = f\"/content/drive/MyDrive/kaggleCompe_MAP-math/{NEW_MODEL_NAME}.zip\"\n",
    "\n",
    "        if not os.path.exists(model_dir):\n",
    "            print(f\"❌ モデルディレクトリが見つかりません: {model_dir}\")\n",
    "            return False\n",
    "\n",
    "        print(f\"📁 ソース: {model_dir}\")\n",
    "        print(f\"📦 出力zip: {zip_path}\")\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for root, dirs, files in os.walk(model_dir):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, model_dir)\n",
    "                    zipf.write(file_path, arcname)\n",
    "                    print(f\"  ➕ {arcname}\")\n",
    "\n",
    "        # zipファイルサイズ確認\n",
    "        zip_size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
    "        print(f\"\\n✅ 改良版モデルzip作成完了!\")\n",
    "        print(f\"📏 zipサイズ: {zip_size_mb:.2f} MB\")\n",
    "        print(f\"📥 ダウンロードパス: {zip_path}\")\n",
    "\n",
    "        # ダウンロード用コード表示\n",
    "        print(f\"\\n💡 Colabでダウンロードするには:\")\n",
    "        print(f\"```python\")\n",
    "        print(f\"from google.colab import files\")\n",
    "        print(f\"files.download('{zip_path}')\")\n",
    "        print(f\"```\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ zip作成中にエラー: {e}\")\n",
    "        return False\n",
    "\n",
    "# 改良版モデルzip作成実行\n",
    "zip_success = create_improved_model_zip()\n",
    "\n",
    "# 自動ダウンロード\n",
    "if zip_success:\n",
    "    print(\"\\n🎉 改良プロンプト + QLoRAモデルzip作成完了!\")\n",
    "    print(f\"📦 zipファイル: {NEW_MODEL_NAME}.zip\")\n",
    "    \n",
    "    # ダウンロード確認\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        response = input(\"\\n改良版モデルを今すぐダウンロードしますか？ (y/n): \")\n",
    "        if response.lower() == 'y':\n",
    "            files.download(f'/content/drive/MyDrive/kaggleCompe_MAP-math/{NEW_MODEL_NAME}.zip')\n",
    "            print(\"✅ ダウンロード開始!\")\n",
    "        else:\n",
    "            print(\"💡 後でダウンロードする場合は上記のコードを使用してください\")\n",
    "    except:\n",
    "        print(\"💡 手動でダウンロードしてください\")\n",
    "else:\n",
    "    print(\"❌ zip作成に失敗しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07011cd5",
   "metadata": {},
   "source": [
    "## 🎯 改良版モデル完了サマリー\n",
    "\n",
    "### ✅ 完了した改良点\n",
    "1. **📝 プロンプト改良**: `final_compact_prompt.py`ベース\n",
    "   - 65個の全ラベル完全対応（False_Correct:NA含む）\n",
    "   - 分類ガイドラインの早期配置\n",
    "   - 問題コンテキストの最適化構造\n",
    "   - トークン効率の改善\n",
    "\n",
    "2. **⚡ QLoRA最適化**: メモリ効率と性能向上\n",
    "   - 4-bit量子化による大幅なメモリ削減\n",
    "   - LoRA (r=16, alpha=32) による効率的ファインチューニング\n",
    "   - 勾配チェックポイントによるメモリ最適化\n",
    "   - バッチサイズとaccumulationの最適バランス\n",
    "\n",
    "3. **🚀 訓練効率化**:\n",
    "   - QLoRA対応の専用データセット作成\n",
    "   - MAP@3メトリクス改良版実装\n",
    "   - グルーピングバッチングによる効率化\n",
    "   - メモリ使用量の大幅削減\n",
    "\n",
    "### 📁 保存されたファイル\n",
    "- **新モデル名**: `gemma-2-2b-improved-prompts-qlora`\n",
    "- **Google Drive**: `/content/drive/MyDrive/kaggleCompe_MAP-math/improved_models/`\n",
    "- **zip形式**: `gemma-2-2b-improved-prompts-qlora.zip`\n",
    "- **メタデータ**: 改良内容の詳細記録付き\n",
    "\n",
    "### 🔄 次のステップ（Kaggle提出準備）\n",
    "1. **Kaggleデータセット作成**: 改良版モデルをKaggleにアップロード\n",
    "2. **推論テスト**: test.csvでの推論性能確認\n",
    "3. **submission.csv生成**: MAP@3最適化による提出\n",
    "4. **性能比較**: 元モデルとの性能差分析\n",
    "\n",
    "### 🎯 改良版モデルの読み込み方法（Kaggle用）\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# ベースモデル読み込み\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "\n",
    "# QLoRAアダプタ読み込み\n",
    "model = PeftModel.from_pretrained(base_model, \"path/to/improved_model\")\n",
    "```\n",
    "\n",
    "### 🚀 主な改善効果期待値\n",
    "- **メモリ使用量**: 約75%削減（QLoRA効果）\n",
    "- **プロンプト精度**: False_Correct:NA対応による分類精度向上\n",
    "- **訓練効率**: 改良プロンプトによる学習収束性向上\n",
    "- **推論速度**: 最適化されたトークン長による高速化\n",
    "\n",
    "**🎉 改良プロンプト + QLoRAモデル訓練完了!**\n",
    "**次は提出用推論ノートブックでの性能テストです！**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
