カグルでの提出テストがおわったので実際にColabで訓練をおこなう

Gemma2-2bを訓練したがMAPスコアが何故か訓練中まったく更新されなくて、訓練は完了したが意味のないモデルが完成した
訓練自体は3時間程度で終わった

Geminiに調査してもらっておそらくPEFTに問題があるっぽい
PEFTのライブラリをアップデートしたら多分治った
調査の際に多分関係ないところ、多分 bf16=Falseにした（もともとTrue）、とかもちょっといじったから変わった

再度訓練しなおし、訓練時間は3時間程度とでているので前回と同じ
no_misconceptional 訓練時間は3時間弱、mapスコアは0.93だった

peftで訓練したモデルには保存時にアダプター？がついており
カグルアップロードのときにはそらをさらにベースモデルと統合する必要があった

kaggleに提出したらmap0.71だった、noMisなので妥当か
自分はmisconceptionの内容を考慮していないのでその分スコアが下がったのだと思う

次のステップはプロンプトエンジニアリングとQLoRA、最終的にはmisconceptionのラベルまでいれる必要がある

プロンプト改良かつQLoRA適用版で訓練を開始した
訓練時間は６時間ぐらいらしい　バッチサイズ8
