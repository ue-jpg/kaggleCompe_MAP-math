#!/usr/bin/env python
"""
æ”¹è‰¯ç‰ˆQLoRAãƒ¢ãƒ‡ãƒ«ã‚’Kaggleã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç’°å¢ƒç”¨ã«çµ±åˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
QLoRAã‚¢ãƒ€ãƒ—ã‚¿ã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ¼ã‚¸ã—ã¦å˜ä¸€ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
"""
import os
import json
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import PeftModel


def merge_improved_qlora_model_for_kaggle():
    """æ”¹è‰¯ç‰ˆQLoRAãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ã¦Kaggleç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
    print("ğŸš€ æ”¹è‰¯ç‰ˆQLoRAâ†’Kaggleç”¨çµ±åˆãƒ¢ãƒ‡ãƒ«ä½œæˆ")
    print("=" * 60)

    # ãƒ‘ã‚¹è¨­å®š
    IMPROVED_MODEL_PATH = r"C:\Users\mouse\Desktop\NotDelete\GitHub\kaggleCompe_MAP-math\colab\colabã§è¨“ç·´ã—ã¦ä¿å­˜\gemma-2-2b-improved-prompts-qlora"
    OUTPUT_PATH = r"C:\Users\mouse\Desktop\NotDelete\GitHub\kaggleCompe_MAP-math\colab\colabã§è¨“ç·´ã—ã¦ä¿å­˜\kaggle-ready-improved-qlora"

    try:
        # æ”¹è‰¯ç‰ˆãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿
        improved_label_file = os.path.join(
            IMPROVED_MODEL_PATH, "improved_label_mapping.json"
        )
        if not os.path.exists(improved_label_file):
            print(f"âŒ æ”¹è‰¯ç‰ˆãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {improved_label_file}")
            return None

        with open(improved_label_file, "r", encoding="utf-8") as f:
            label_mapping = json.load(f)
        print(f"âœ… æ”¹è‰¯ç‰ˆãƒ©ãƒ™ãƒ«æ•°: {len(label_mapping)}")

        # æ”¹è‰¯ç‰ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        metadata_file = os.path.join(
            IMPROVED_MODEL_PATH, "improved_model_metadata.json"
        )
        if os.path.exists(metadata_file):
            with open(metadata_file, "r", encoding="utf-8") as f:
                metadata = json.load(f)
            print(f"ğŸ“‹ æ”¹è‰¯ç‰ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
            print(f"  ğŸ”¸ ãƒ¢ãƒ‡ãƒ«å: {metadata.get('improved_model_name', 'Unknown')}")
            print(
                f"  ğŸ”¸ QLoRAé©ç”¨: {metadata.get('improvements', {}).get('qlora_applied', False)}"
            )
            print(
                f"  ğŸ”¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–: {metadata.get('improvements', {}).get('prompt_optimization', 'Unknown')}"
            )
        else:
            metadata = {}
            print("âš ï¸ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼è¨­å®šèª­ã¿è¾¼ã¿
        adapter_config_path = os.path.join(IMPROVED_MODEL_PATH, "adapter_config.json")
        if not os.path.exists(adapter_config_path):
            print(f"âŒ ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {adapter_config_path}")
            return None

        with open(adapter_config_path, "r") as f:
            adapter_config = json.load(f)

        base_model_name = adapter_config.get(
            "base_model_name_or_path", "google/gemma-2-2b-it"
        )
        print(f"ğŸ“¦ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {base_model_name}")
        print(f"ğŸ”§ QLoRAè¨­å®š:")
        print(f"  ğŸ”¸ r (rank): {adapter_config.get('r', 'Unknown')}")
        print(f"  ğŸ”¸ lora_alpha: {adapter_config.get('lora_alpha', 'Unknown')}")
        print(f"  ğŸ”¸ target_modules: {adapter_config.get('target_modules', [])}")

        # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        print("\nğŸ§  ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        base_model = AutoModelForSequenceClassification.from_pretrained(
            base_model_name,
            num_labels=len(label_mapping),
            torch_dtype=torch.float32,  # QLoRAçµ±åˆã®ãŸã‚float32ä½¿ç”¨
            trust_remote_code=True,
        )
        print(
            f"âœ… ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº† (ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in base_model.parameters()):,})"
        )

        # QLoRA PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼é©ç”¨
        print("âš¡ QLoRA PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼èª­ã¿è¾¼ã¿ä¸­...")
        try:
            peft_model = PeftModel.from_pretrained(base_model, IMPROVED_MODEL_PATH)
            print("âœ… QLoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")
        except Exception as e:
            print(f"âŒ PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            return None

        # ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ¼ã‚¸
        print("ğŸ”€ QLoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ¼ã‚¸ä¸­...")
        try:
            merged_model = peft_model.merge_and_unload()
            print("âœ… ãƒãƒ¼ã‚¸å®Œäº† - æ”¹è‰¯ç‰ˆçµ±åˆãƒ¢ãƒ‡ãƒ«ä½œæˆæˆåŠŸ")
        except Exception as e:
            print(f"âŒ ãƒãƒ¼ã‚¸å¤±æ•—: {e}")
            return None

        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
        print("ğŸ“ æ”¹è‰¯ç‰ˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...")
        tokenizer = AutoTokenizer.from_pretrained(IMPROVED_MODEL_PATH)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        print(f"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(OUTPUT_PATH, exist_ok=True)
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {OUTPUT_PATH}")

        # çµ±åˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
        print(f"ğŸ’¾ æ”¹è‰¯ç‰ˆçµ±åˆãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")
        merged_model.save_pretrained(OUTPUT_PATH, safe_serialization=True)
        tokenizer.save_pretrained(OUTPUT_PATH)
        print("âœ… ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿å­˜å®Œäº†")

        # æ”¹è‰¯ç‰ˆãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚³ãƒ”ãƒ¼
        import shutil

        shutil.copy2(
            improved_label_file, os.path.join(OUTPUT_PATH, "label_mapping.json")
        )
        print("âœ… æ”¹è‰¯ç‰ˆãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚³ãƒ”ãƒ¼å®Œäº†")

        # Kaggleç”¨æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«æƒ…å ±ä¿å­˜
        kaggle_model_info = {
            "model_type": "merged_improved_qlora_model",
            "base_model": base_model_name,
            "peft_type": "QLoRA",
            "num_labels": len(label_mapping),
            "created_for": "kaggle_offline_environment",
            "usage": "Use AutoModelForSequenceClassification.from_pretrained() directly",
            "improvements": {
                "prompt_optimization": metadata.get("improvements", {}).get(
                    "prompt_optimization", "final_compact_prompt.py based"
                ),
                "qlora_applied": True,
                "quantization": "4-bit nf4 (merged to float32)",
                "label_coverage": f"{len(label_mapping)} labels including False_Correct:NA",
                "prompt_features": metadata.get("improvements", {}).get(
                    "prompt_features", []
                ),
            },
            "original_qlora_config": {
                "r": adapter_config.get("r"),
                "lora_alpha": adapter_config.get("lora_alpha"),
                "target_modules": adapter_config.get("target_modules"),
                "task_type": adapter_config.get("task_type"),
            },
            "training_info": metadata.get("training_info", {}),
            "created_from": "gemma-2-2b-improved-prompts-qlora",
        }

        kaggle_info_path = os.path.join(OUTPUT_PATH, "kaggle_model_info.json")
        with open(kaggle_info_path, "w", encoding="utf-8") as f:
            json.dump(kaggle_model_info, f, indent=2, ensure_ascii=False)
        print("âœ… Kaggleç”¨æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«æƒ…å ±ä¿å­˜å®Œäº†")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        print(f"\nğŸ“¦ ä½œæˆã•ã‚ŒãŸæ”¹è‰¯ç‰ˆçµ±åˆãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«:")
        total_size = 0
        for file in os.listdir(OUTPUT_PATH):
            file_path = os.path.join(OUTPUT_PATH, file)
            if os.path.isfile(file_path):
                size_mb = os.path.getsize(file_path) / (1024 * 1024)
                total_size += size_mb
                print(f"  ğŸ“„ {file}: {size_mb:.1f} MB")
        print(f"ğŸ“Š ç·ã‚µã‚¤ã‚º: {total_size:.1f} MB")

        # ä½¿ç”¨æ–¹æ³•èª¬æ˜
        print(f"\nğŸ¯ Kaggleã§ã®æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ä½¿ç”¨æ–¹æ³•:")
        print(f"1. {OUTPUT_PATH} ãƒ•ã‚©ãƒ«ãƒ€ã‚’Kaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
        print(f"2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå: 'gemma-2b-improved-prompts-kaggle' ãªã©ã«è¨­å®š")
        print(f"3. ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã®èª­ã¿è¾¼ã¿:")
        print(f"   ```python")
        print(
            f"   from transformers import AutoModelForSequenceClassification, AutoTokenizer"
        )
        print(f"   import json")
        print(f"   ")
        print(f"   # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿")
        print(
            f"   model = AutoModelForSequenceClassification.from_pretrained('/kaggle/input/your-dataset-name')"
        )
        print(
            f"   tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/your-dataset-name')"
        )
        print(f"   ")
        print(f"   # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿")
        print(
            f"   with open('/kaggle/input/your-dataset-name/label_mapping.json', 'r') as f:"
        )
        print(f"       label_mapping = json.load(f)")
        print(f"   ```")
        print(f"4. æ¨è«–æ™‚ã¯æ”¹è‰¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")

        print(f"\nâœ¨ æ”¹è‰¯ç‰ˆQLoRAâ†’Kaggleçµ±åˆãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†!")
        print(f"ğŸ‰ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ– + QLoRAåŠ¹æœãŒKaggleã§åˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ")

        return OUTPUT_PATH

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback

        traceback.print_exc()
        return None


if __name__ == "__main__":
    result = merge_improved_qlora_model_for_kaggle()
    if result:
        print(f"\nğŸš€ æˆåŠŸ: {result}")
    else:
        print(f"\nğŸ’” å¤±æ•—: ãƒ¢ãƒ‡ãƒ«çµ±åˆã«å¤±æ•—ã—ã¾ã—ãŸ")
