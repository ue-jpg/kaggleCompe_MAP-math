{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c8bebaa",
   "metadata": {},
   "source": [
    "# ğŸ¤– Gemma-2-2b-it åˆ†é¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»ä¿å­˜ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n",
    "\n",
    "## æ¦‚è¦\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€MAP - Charting Student Math Misunderstandingsã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ç”¨ã«ã€Gemma-2-2b-itãƒ¢ãƒ‡ãƒ«ã‚’6åˆ†é¡ã‚¿ã‚¹ã‚¯ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ä¿å­˜ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "### ãƒ¢ãƒ‡ãƒ«ä»•æ§˜\n",
    "- **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«**: google/gemma-2-2b-it (~2.6B parameters)\n",
    "- **ã‚¿ã‚¹ã‚¯**: 6ã‚¯ãƒ©ã‚¹åˆ†é¡ (True/False_Correct/Neither/Misconception)\n",
    "- **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: LoRA (Low-Rank Adaptation)\n",
    "- **GPU**: A100 æ¨å¥¨\n",
    "\n",
    "### ä¿å­˜å½¢å¼\n",
    "- Google Drive: `/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models/`\n",
    "- ãƒ­ãƒ¼ã‚«ãƒ«: zipãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "- å½¢å¼: Kaggleæå‡ºç”¨ã®AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3b90a7",
   "metadata": {},
   "source": [
    "## ğŸ“¦ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a792f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveãƒã‚¦ãƒ³ãƒˆ\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "import os\n",
    "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/map_data', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/logs', exist_ok=True)\n",
    "\n",
    "print(\"âœ… Google Drive ãƒã‚¦ãƒ³ãƒˆå®Œäº†\")\n",
    "print(\"âœ… å¿…è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆå®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd097a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Faceèªè¨¼\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02751a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install peft accelerate transformers datasets -q\n",
    "print(\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3726b2",
   "metadata": {},
   "source": [
    "## ğŸ“š ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ãƒ¢ãƒ‡ãƒ«è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c31ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import warnings\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
    "MODEL_NAME = \"google/gemma-2-2b-it\"\n",
    "NUM_LABELS = 6\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ–¥ï¸ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "print(\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd662bc0",
   "metadata": {},
   "source": [
    "## ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b0ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ï¼ˆ6åˆ†é¡å½¢å¼ï¼‰\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ï¼ˆ6åˆ†é¡å½¢å¼ï¼‰\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ï¼ˆGoogle Driveå†…ï¼‰\n",
    "    train_data_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/map_data/train.csv\"\n",
    "    \n",
    "    if not os.path.exists(train_data_path):\n",
    "        print(f\"âŒ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {train_data_path}\")\n",
    "        print(\"ğŸ’¡ Google Driveã®MyDrive/kaggleCompe_MAP-math/map_data/ã« train.csv ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "        train_df = pd.read_csv(train_data_path)\n",
    "        print(f\"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {train_df.shape}\")\n",
    "\n",
    "        # ã‚³ãƒ³ãƒšå½¢å¼ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç¢ºèªï¼ˆ6åˆ†é¡ï¼‰\n",
    "        print(\"\\nğŸ“‹ Categoryåˆ†å¸ƒ:\")\n",
    "        category_counts = train_df[\"Category\"].value_counts()\n",
    "        print(category_counts)\n",
    "\n",
    "        # NaNå€¤é™¤å»\n",
    "        before_len = len(train_df)\n",
    "        train_df = train_df.dropna(subset=[\"Category\", \"StudentExplanation\"])\n",
    "        after_len = len(train_df)\n",
    "        print(f\"\\nğŸ§¹ NaNé™¤å»: {before_len} -> {after_len} ({before_len - after_len}è¡Œå‰Šé™¤)\")\n",
    "\n",
    "        # å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆ\n",
    "        def create_enhanced_text(row):\n",
    "            \"\"\"Question + MC_Answer + Student Explanation ã‚’çµ±åˆ\"\"\"\n",
    "            question = str(row[\"QuestionText\"]) if pd.notna(row[\"QuestionText\"]) else \"\"\n",
    "            mc_answer = str(row[\"MC_Answer\"]) if pd.notna(row[\"MC_Answer\"]) else \"\"\n",
    "            explanation = str(row[\"StudentExplanation\"]) if pd.notna(row[\"StudentExplanation\"]) else \"\"\n",
    "            \n",
    "            enhanced_text = f\"Question: {question} Selected Answer: {mc_answer} Explanation: {explanation}\"\n",
    "            return enhanced_text\n",
    "\n",
    "        print(\"\\nğŸ”§ å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆä¸­...\")\n",
    "        train_df[\"enhanced_text\"] = train_df.apply(create_enhanced_text, axis=1)\n",
    "\n",
    "        # 6ã¤ã®ã‚«ãƒ†ã‚´ãƒªã®ç¢ºèª\n",
    "        unique_categories = sorted(train_df[\"Category\"].unique())\n",
    "        print(f\"\\nğŸ“Š ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚«ãƒ†ã‚´ãƒªæ•°: {len(unique_categories)}\")\n",
    "        print(\"ã‚«ãƒ†ã‚´ãƒªä¸€è¦§:\")\n",
    "        for i, cat in enumerate(unique_categories):\n",
    "            count = (train_df[\"Category\"] == cat).sum()\n",
    "            percentage = count / len(train_df) * 100\n",
    "            print(f\"  {i}: {cat} ({count:,}ä»¶, {percentage:.1f}%)\")\n",
    "\n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ\n",
    "        text_lengths = train_df[\"enhanced_text\"].str.len()\n",
    "        print(f\"\\nğŸ“ ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ:\")\n",
    "        print(f\"  å¹³å‡: {text_lengths.mean():.1f} æ–‡å­—\")\n",
    "        print(f\"  ä¸­å¤®å€¤: {text_lengths.median():.1f} æ–‡å­—\")\n",
    "        print(f\"  æœ€å¤§: {text_lengths.max()} æ–‡å­—\")\n",
    "        print(f\"  512æ–‡å­—ä»¥ä¸‹: {(text_lengths <= 512).sum()} ({(text_lengths <= 512).mean()*100:.1f}%)\")\n",
    "\n",
    "        return train_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¾ãŸã¯å‰å‡¦ç†ã«å¤±æ•—: {e}\")\n",
    "        return None\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Ÿè¡Œ\n",
    "train_df = load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f7e1ec",
   "metadata": {},
   "source": [
    "## ğŸ¤– Gemmaãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gemma_model(device):\n",
    "    \"\"\"Gemma-2-2b-itãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿ï¼ˆå…ƒã®å®šç¾©ã«æº–æ‹ ï¼‰\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ğŸ¤– Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {MODEL_NAME}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        print(\"ğŸ“ Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®šï¼ˆGemmaã®å ´åˆå¿…è¦ï¼‰\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            print(\"ğŸ”§ ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’EOSãƒˆãƒ¼ã‚¯ãƒ³ã«è¨­å®š\")\n",
    "\n",
    "        print(f\"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "        print(f\"ğŸ”– ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³: {tokenizer.pad_token}\")\n",
    "        print(f\"ğŸ“ èªå½™ã‚µã‚¤ã‚º: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "        print(\"\\nğŸ§  Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "\n",
    "        # åˆ†é¡ã‚¿ã‚¹ã‚¯ç”¨ã®è¨­å®š\n",
    "        config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "        config.num_labels = NUM_LABELS\n",
    "        config.problem_type = \"single_label_classification\"\n",
    "\n",
    "        # Gemma-2-2b-itãƒ¢ãƒ‡ãƒ«ã‚’åˆ†é¡ç”¨ã«èª­ã¿è¾¼ã¿\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            config=config,\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\" if device.type == \"cuda\" else None,\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†!\")\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "        print(f\"ğŸ“Š åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°: {NUM_LABELS}\")\n",
    "        print(f\"ğŸ“ˆ ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}\")\n",
    "        print(f\"ğŸ¯ è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}\")\n",
    "        print(f\"ğŸ’¡ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: ~{total_params / 1e9:.2f}B parameters\")\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Ÿè¡Œ\n",
    "model, tokenizer = load_gemma_model(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d541e87",
   "metadata": {},
   "source": [
    "## ğŸ”§ LoRAè¨­å®šã¨PEFTé©ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48284a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora_to_model(model):\n",
    "    \"\"\"LoRAè¨­å®šã¨PEFTé©ç”¨\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ”§ LoRA (PEFT) è¨­å®šã¨é©ç”¨\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # LoRAè¨­å®š\n",
    "        lora_config = LoraConfig(\n",
    "            r=16,                    # LoRA attention dimension\n",
    "            lora_alpha=32,          # Alpha parameter for LoRA scaling\n",
    "            target_modules=[        # Gemma-2ã®ä¸»è¦ãªç·šå½¢å±¤ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "            ],\n",
    "            lora_dropout=0.05,      # Dropout probability for LoRA layers\n",
    "            bias=\"none\",            # Bias type\n",
    "            task_type=\"SEQ_CLS\",    # Sequence Classification task\n",
    "            modules_to_save=[\"classifier\", \"score\"],  # åˆ†é¡ãƒ˜ãƒƒãƒ‰ã‚’ä¿å­˜\n",
    "        )\n",
    "\n",
    "        print(\"ğŸ“‹ LoRAè¨­å®š:\")\n",
    "        print(f\"  r (rank): {lora_config.r}\")\n",
    "        print(f\"  lora_alpha: {lora_config.lora_alpha}\")\n",
    "        print(f\"  target_modules: {lora_config.target_modules}\")\n",
    "        print(f\"  lora_dropout: {lora_config.lora_dropout}\")\n",
    "        print(f\"  task_type: {lora_config.task_type}\")\n",
    "\n",
    "        # PEFTãƒ¢ãƒ‡ãƒ«ä½œæˆ\n",
    "        print(\"\\nğŸ”„ PEFTãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...\")\n",
    "        peft_model = get_peft_model(model, lora_config)\n",
    "        \n",
    "        print(\"âœ… PEFTãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†\")\n",
    "        peft_model.print_trainable_parameters()\n",
    "\n",
    "        # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•\n",
    "        peft_model.to(device)\n",
    "        print(f\"ğŸ“ PEFTãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•: {next(peft_model.parameters()).device}\")\n",
    "\n",
    "        return peft_model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ LoRAé©ç”¨å¤±æ•—: {e}\")\n",
    "        return None\n",
    "\n",
    "# LoRAé©ç”¨\n",
    "if model is not None:\n",
    "    peft_model = apply_lora_to_model(model)\n",
    "else:\n",
    "    print(\"âŒ ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€LoRAã‚’é©ç”¨ã§ãã¾ã›ã‚“\")\n",
    "    peft_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7961c3",
   "metadata": {},
   "source": [
    "## ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹å®šç¾©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f3e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathMisconceptionDataset(Dataset):\n",
    "    \"\"\"Math Misconception Dataset for PyTorch\"\"\"\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "print(\"âœ… MathMisconceptionDataset ã‚¯ãƒ©ã‚¹å®šç¾©å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a8ee4",
   "metadata": {},
   "source": [
    "## ğŸ“Š è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d6b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_map3_metrics(eval_pred):\n",
    "    \"\"\"MAP@3ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = torch.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
    "\n",
    "    map_scores = []\n",
    "    for i, true_label in enumerate(labels):\n",
    "        top3_indices = np.argsort(predictions[i])[::-1][:3]\n",
    "        score = 0.0\n",
    "        for j, pred_idx in enumerate(top3_indices):\n",
    "            if pred_idx == true_label:\n",
    "                score = 1.0 / (j + 1)\n",
    "                break\n",
    "        map_scores.append(score)\n",
    "\n",
    "    map3_score = np.mean(map_scores)\n",
    "    accuracy = accuracy_score(labels, np.argmax(predictions, axis=1))\n",
    "\n",
    "    return {\"map3\": map3_score, \"accuracy\": accuracy}\n",
    "\n",
    "print(\"âœ… MAP@3è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6363d6b",
   "metadata": {},
   "source": [
    "## ğŸš€ ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_df, peft_model, tokenizer, device):\n",
    "    \"\"\"ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸš€ 6åˆ†é¡ãƒ¢ãƒ‡ãƒ« ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if train_df is None or peft_model is None:\n",
    "        print(\"âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "        return None, None\n",
    "\n",
    "    # ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_df[\"encoded_labels\"] = label_encoder.fit_transform(train_df[\"Category\"])\n",
    "\n",
    "    print(f\"ğŸ“‹ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«:\")\n",
    "    for i, label in enumerate(label_encoder.classes_):\n",
    "        count = (train_df[\"encoded_labels\"] == i).sum()\n",
    "        print(f\"  {i}: {label} ({count:,}ä»¶)\")\n",
    "\n",
    "    # è¨“ç·´/æ¤œè¨¼åˆ†å‰²\n",
    "    train_texts = train_df[\"enhanced_text\"].tolist()\n",
    "    train_labels = train_df[\"encoded_labels\"].tolist()\n",
    "\n",
    "    try:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            train_texts,\n",
    "            train_labels,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=train_labels,\n",
    "        )\n",
    "        print(\"âœ… Stratified splité©ç”¨\")\n",
    "    except ValueError:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            train_texts, train_labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "        print(\"âœ… Regular splité©ç”¨ï¼ˆå°‘æ•°ã‚¯ãƒ©ã‚¹ã®ãŸã‚ï¼‰\")\n",
    "\n",
    "    print(f\"ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train):,}ä»¶\")\n",
    "    print(f\"ğŸ“Š æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val):,}ä»¶\")\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
    "    train_dataset = MathMisconceptionDataset(\n",
    "        X_train, y_train, tokenizer, max_length=MAX_LENGTH\n",
    "    )\n",
    "    val_dataset = MathMisconceptionDataset(\n",
    "        X_val, y_val, tokenizer, max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "    # è¨“ç·´è¨­å®š\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models/gemma-2-2b-math-classification\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=8,  # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º32\n",
    "        warmup_steps=200,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"/content/drive/MyDrive/kaggleCompe_MAP-math/logs\",\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=300,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=300,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"map3\",\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\",\n",
    "        dataloader_pin_memory=False,\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        learning_rate=2e-5,\n",
    "        save_total_limit=2,\n",
    "        gradient_checkpointing=False,\n",
    "    )\n",
    "\n",
    "    print(\"\\nğŸ“‹ è¨“ç·´è¨­å®š:\")\n",
    "    print(f\"  ã‚¨ãƒãƒƒã‚¯æ•°: {training_args.num_train_epochs}\")\n",
    "    print(f\"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_args.per_device_train_batch_size}\")\n",
    "    print(f\"  å‹¾é…ç´¯ç©: {training_args.gradient_accumulation_steps}\")\n",
    "    print(f\"  å­¦ç¿’ç‡: {training_args.learning_rate}\")\n",
    "    print(f\"  ä¿å­˜å…ˆ: {training_args.output_dir}\")\n",
    "\n",
    "    # Trainerä½œæˆ\n",
    "    trainer = Trainer(\n",
    "        model=peft_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_map3_metrics,\n",
    "    )\n",
    "\n",
    "    # è¨“ç·´å®Ÿè¡Œ\n",
    "    print(\"\\nğŸš€ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†\")\n",
    "\n",
    "        # æœ€çµ‚è©•ä¾¡\n",
    "        print(\"\\nğŸ“Š æœ€çµ‚è©•ä¾¡:\")\n",
    "        eval_results = trainer.evaluate()\n",
    "        for key, value in eval_results.items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "        return trainer, label_encoder\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# è¨“ç·´å®Ÿè¡Œ\n",
    "if train_df is not None and peft_model is not None:\n",
    "    trainer, label_encoder = train_model(train_df, peft_model, tokenizer, device)\n",
    "else:\n",
    "    print(\"âŒ è¨“ç·´ã«å¿…è¦ãªè¦ç´ ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    trainer, label_encoder = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff391c",
   "metadata": {},
   "source": [
    "## ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b7c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(trainer, tokenizer, label_encoder):\n",
    "    \"\"\"è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ï¼ˆKaggleæå‡ºç”¨å½¢å¼ï¼‰\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ’¾ è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if trainer is None or tokenizer is None or label_encoder is None:\n",
    "        print(\"âŒ ä¿å­˜ã«å¿…è¦ãªè¦ç´ ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "        return False\n",
    "\n",
    "    save_base_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models\"\n",
    "    model_save_path = f\"{save_base_path}/gemma-2-2b-math-classification-final\"\n",
    "\n",
    "    try:\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜\n",
    "        print(f\"ğŸ“ ä¿å­˜å…ˆ: {model_save_path}\")\n",
    "        \n",
    "        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ï¼ˆPEFTãƒ¢ãƒ‡ãƒ«è¾¼ã¿ï¼‰\n",
    "        trainer.save_model(model_save_path)\n",
    "        tokenizer.save_pretrained(model_save_path)\n",
    "        \n",
    "        print(\"âœ… ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿å­˜å®Œäº†\")\n",
    "\n",
    "        # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã®ä¿å­˜\n",
    "        label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "        label_file = f\"{model_save_path}/label_mapping.json\"\n",
    "        \n",
    "        with open(label_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(label_mapping, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(\"âœ… ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ä¿å­˜å®Œäº†\")\n",
    "        print(f\"ğŸ“‹ ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°: {label_mapping}\")\n",
    "\n",
    "        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜\n",
    "        metadata = {\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"num_labels\": NUM_LABELS,\n",
    "            \"max_length\": MAX_LENGTH,\n",
    "            \"task_type\": \"sequence_classification\",\n",
    "            \"architecture\": \"AutoModelForSequenceClassification\",\n",
    "            \"peft_applied\": True,\n",
    "            \"lora_config\": {\n",
    "                \"r\": 16,\n",
    "                \"lora_alpha\": 32,\n",
    "                \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "                \"task_type\": \"SEQ_CLS\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        metadata_file = f\"{model_save_path}/model_metadata.json\"\n",
    "        with open(metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(\"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†\")\n",
    "\n",
    "        # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "        print(f\"\\nğŸ“‚ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
    "        for item in os.listdir(model_save_path):\n",
    "            item_path = os.path.join(model_save_path, item)\n",
    "            if os.path.isfile(item_path):\n",
    "                size_mb = os.path.getsize(item_path) / (1024 * 1024)\n",
    "                print(f\"  ğŸ“„ {item}: {size_mb:.2f} MB\")\n",
    "            else:\n",
    "                print(f\"  ğŸ“ {item}/\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return False\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Ÿè¡Œ\n",
    "if trainer is not None and tokenizer is not None and label_encoder is not None:\n",
    "    save_success = save_trained_model(trainer, tokenizer, label_encoder)\n",
    "    \n",
    "    if save_success:\n",
    "        print(\"\\nğŸ‰ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†!\")\n",
    "        print(\"ğŸ“ Google Driveä¿å­˜ãƒ‘ã‚¹: /content/drive/MyDrive/kaggleCompe_MAP-math/trained_models/\")\n",
    "        print(\"ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: Kaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æº–å‚™å®Œäº†\")\n",
    "    else:\n",
    "        print(\"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "else:\n",
    "    print(\"âŒ ä¿å­˜ã«å¿…è¦ãªè¦ç´ ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d91f89",
   "metadata": {},
   "source": [
    "## ğŸ“¦ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨zipä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_download_zip():\n",
    "    \"\"\"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã®zipãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“¦ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨zipä½œæˆ\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        import zipfile\n",
    "        \n",
    "        model_dir = \"/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models/gemma-2-2b-math-classification-final\"\n",
    "        zip_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/gemma-2-2b-math-model.zip\"\n",
    "        \n",
    "        if not os.path.exists(model_dir):\n",
    "            print(f\"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_dir}\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"ğŸ“ ã‚½ãƒ¼ã‚¹: {model_dir}\")\n",
    "        print(f\"ğŸ“¦ å‡ºåŠ›zip: {zip_path}\")\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for root, dirs, files in os.walk(model_dir):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, model_dir)\n",
    "                    zipf.write(file_path, arcname)\n",
    "                    print(f\"  â• {arcname}\")\n",
    "        \n",
    "        # zipãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª\n",
    "        zip_size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
    "        print(f\"\\nâœ… zipä½œæˆå®Œäº†!\")\n",
    "        print(f\"ğŸ“ zipã‚µã‚¤ã‚º: {zip_size_mb:.2f} MB\")\n",
    "        print(f\"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‘ã‚¹: {zip_path}\")\n",
    "        \n",
    "        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã‚³ãƒ¼ãƒ‰è¡¨ç¤º\n",
    "        print(f\"\\nğŸ’¡ Colabã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯:\")\n",
    "        print(f\"```python\")\n",
    "        print(f\"from google.colab import files\")\n",
    "        print(f\"files.download('{zip_path}')\")\n",
    "        print(f\"```\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ zipä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return False\n",
    "\n",
    "# zipä½œæˆå®Ÿè¡Œ\n",
    "zip_success = create_download_zip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34461dde",
   "metadata": {},
   "source": [
    "## ğŸ¯ å®Œäº†ã‚µãƒãƒªãƒ¼\n",
    "\n",
    "### âœ… å®Œäº†ã‚¿ã‚¹ã‚¯\n",
    "1. **ğŸ¤– ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿**: Gemma-2-2b-it (AutoModelForSequenceClassification)\n",
    "2. **ğŸ”§ LoRAé©ç”¨**: åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "3. **ğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™**: 6åˆ†é¡ã‚¿ã‚¹ã‚¯ç”¨å‰å‡¦ç†\n",
    "4. **ğŸš€ è¨“ç·´å®Ÿè¡Œ**: MAP@3æœ€é©åŒ–\n",
    "5. **ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜**: Google Drive + zipå½¢å¼\n",
    "\n",
    "### ğŸ“ ä¿å­˜å ´æ‰€\n",
    "- **Google Drive**: `/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models/`\n",
    "- **zipå½¢å¼**: `/content/drive/MyDrive/kaggleCompe_MAP-math/gemma-2-2b-math-model.zip`\n",
    "\n",
    "### ğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "1. **Kaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ**: ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’Kaggleã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "2. **æ¨è«–ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯**: test_saved2-2b.ipynbã§èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ\n",
    "3. **æå‡º**: submission.csvã®ç”Ÿæˆ\n",
    "\n",
    "### ğŸ¯ Kaggleæå‡ºç”¨èª­ã¿è¾¼ã¿å½¢å¼\n",
    "```python\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "```\n",
    "\n",
    "**ğŸ‰ Gemma-2-2b-itåˆ†é¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa58dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€çµ‚å®Ÿè¡Œç¢ºèªã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "if zip_success:\n",
    "    print(\"ğŸ‰ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ!\")\n",
    "    print(\"\\nğŸ“¥ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œ:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from google.colab import files\")\n",
    "    print(\"files.download('/content/drive/MyDrive/kaggleCompe_MAP-math/gemma-2-2b-math-model.zip')\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    # è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        response = input(\"ä»Šã™ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã‹ï¼Ÿ (y/n): \")\n",
    "        if response.lower() == 'y':\n",
    "            files.download('/content/drive/MyDrive/kaggleCompe_MAP-math/gemma-2-2b-math-model.zip')\n",
    "            print(\"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹!\")\n",
    "    except:\n",
    "        print(\"ğŸ’¡ æ‰‹å‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n",
    "else:\n",
    "    print(\"âŒ ä¸€éƒ¨ã®å‡¦ç†ãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“ã€‚ä¸Šè¨˜ã®ã‚»ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
