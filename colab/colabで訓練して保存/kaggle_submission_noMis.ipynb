{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ MAP Competition: Gemma-2-2b-it Submission Notebook (Kaggle Offline Ready)\n",
    "\n",
    "## Overview\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€äº‹å‰ã«è¨“ç·´ãƒ»çµ±åˆã•ã‚ŒãŸGemma-2-2b-itãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€MAP - Charting Student Math Misunderstandingsã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®æå‡ºã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "### Model Details\n",
    "- **Pre-trained Model**: google/gemma-2-2b-it (LoRAã‚¢ãƒ€ãƒ—ã‚¿çµ±åˆæ¸ˆã¿)\n",
    "- **Parameters**: ~2.6B\n",
    "- **Task**: 6-class text classification\n",
    "- **Evaluation Metric**: MAP@3\n",
    "- **Kaggle Compatibility**: âœ… ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç’°å¢ƒå¯¾å¿œ\n",
    "\n",
    "### Target Classes (6åˆ†é¡)\n",
    "- **True_Correct**: æ­£è§£ã§æ­£ã—ã„èª¬æ˜\n",
    "- **True_Neither**: æ­£è§£ã ãŒæ›–æ˜§ãªèª¬æ˜\n",
    "- **True_Misconception**: æ­£è§£ã ãŒèª¤ã£ãŸæ¦‚å¿µã®èª¬æ˜\n",
    "- **False_Correct**: ä¸æ­£è§£ã ãŒæ­£ã—ã„æ¦‚å¿µã®èª¬æ˜\n",
    "- **False_Neither**: ä¸æ­£è§£ã§æ›–æ˜§ãªèª¬æ˜\n",
    "- **False_Misconception**: ä¸æ­£è§£ã§èª¤ã£ãŸæ¦‚å¿µã®èª¬æ˜\n",
    "\n",
    "### Strategy\n",
    "1. **çµ±åˆæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿**: Kaggleã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç’°å¢ƒã§ã‚‚å‹•ä½œ\n",
    "2. **ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†**: å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆ\n",
    "3. **åŠ¹ç‡çš„æ¨è«–**: ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹é«˜é€Ÿäºˆæ¸¬\n",
    "4. **MAP@3å½¢å¼å‡ºåŠ›**: TOP-3äºˆæ¸¬ã®ç”Ÿæˆ\n",
    "5. **æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ**: Kaggleå½¢å¼ã®submission.csvä½œæˆ\n",
    "\n",
    "### Kaggleä½¿ç”¨æ–¹æ³•\n",
    "1. **çµ±åˆãƒ¢ãƒ‡ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: `kaggle-ready-model`ãƒ•ã‚©ãƒ«ãƒ€ã‚’Kaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "2. **ãƒ‘ã‚¹è¨­å®š**: MODEL_DATA_PATHã‚’æ­£ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã«å¤‰æ›´\n",
    "3. **å®Ÿè¡Œ**: å…¨ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ\n",
    "4. **æå‡º**: ç”Ÿæˆã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«æå‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:00:28.087317Z",
     "iopub.status.busy": "2025-07-21T23:00:28.087099Z",
     "iopub.status.idle": "2025-07-21T23:00:34.897845Z",
     "shell.execute_reply": "2025-07-21T23:00:34.897078Z",
     "shell.execute_reply.started": "2025-07-21T23:00:28.087300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Kaggleç’°å¢ƒã§å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹é–¢æ•°\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆçµ±åˆãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰\n",
    "required_packages = [\n",
    "    \"transformers>=4.35.0\",\n",
    "    \"accelerate>=0.26.0\", \n",
    "    \"sentencepiece>=0.1.99\"\n",
    "    # æ³¨æ„: çµ±åˆãƒ¢ãƒ‡ãƒ«ã§ã¯PEFTã¯ä¸è¦\n",
    "]\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«çŠ¶æ³ç¢ºèª\n",
    "        if \"transformers\" in package:\n",
    "            import transformers\n",
    "            print(f\"âœ… transformers already installed: {transformers.__version__}\")\n",
    "        elif \"accelerate\" in package:\n",
    "            import accelerate\n",
    "            print(f\"âœ… accelerate already installed: {accelerate.__version__}\")\n",
    "        elif \"sentencepiece\" in package:\n",
    "            import sentencepiece\n",
    "            print(f\"âœ… sentencepiece already installed: {sentencepiece.__version__}\")\n",
    "    except ImportError:\n",
    "        print(f\"ğŸ“¦ Installing {package}...\")\n",
    "        install_package(package)\n",
    "\n",
    "print(\"ğŸ‰ All required libraries are ready!\")\n",
    "print(\"ğŸ“‹ æ³¨æ„: çµ±åˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ä¸è¦ã§ã™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Import Dependencies (Unified Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:02:08.177464Z",
     "iopub.status.busy": "2025-07-21T23:02:08.176724Z",
     "iopub.status.idle": "2025-07-21T23:02:08.184632Z",
     "shell.execute_reply": "2025-07-21T23:02:08.183859Z",
     "shell.execute_reply.started": "2025-07-21T23:02:08.177436Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Environment Information:\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "GPU device: Tesla T4\n",
      "GPU memory: 15.8 GB\n",
      "ğŸ“ Kaggle environment detected: /kaggle/input\n",
      "ğŸ¯ Competition data path: /kaggle/input/map-charting-student-math-misunderstandings\n",
      "ğŸ¤– Model data path: /kaggle/input/gemma-2-2b-math-model/transformers/default/1/gemma-2-2b-math-model\n",
      "âœ… All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "# è­¦å‘Šã‚’éè¡¨ç¤º\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º\n",
    "print(\"ğŸ”§ Environment Information:\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Kaggleã®ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹è¨­å®š\n",
    "KAGGLE_INPUT_PATH = \"/kaggle/input\"\n",
    "if os.path.exists(KAGGLE_INPUT_PATH):\n",
    "    print(f\"ğŸ“ Kaggle environment detected: {KAGGLE_INPUT_PATH}\")\n",
    "    # ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹\n",
    "    COMP_DATA_PATH = \"/kaggle/input/map-charting-student-math-misunderstandings\"\n",
    "    # çµ±åˆãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ï¼ˆKaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã«å¤‰æ›´ã—ã¦ãã ã•ã„ï¼‰\n",
    "    MODEL_DATA_PATH = \"/kaggle/input/gemma-2-2b-merged-model\"\n",
    "    print(f\"ğŸ¯ Competition data path: {COMP_DATA_PATH}\")\n",
    "    print(f\"ğŸ¤– Model data path (merged): {MODEL_DATA_PATH}\")\n",
    "else:\n",
    "    print(\"ğŸ“ Local environment detected\")\n",
    "    COMP_DATA_PATH = r\"C:\\Users\\mouse\\Desktop\\NotDelete\\GitHub\\kaggleCompe_MAP-math\\map_data\"\n",
    "    # ãƒ­ãƒ¼ã‚«ãƒ«ã®çµ±åˆãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹\n",
    "    MODEL_DATA_PATH = r\"C:\\Users\\mouse\\Desktop\\NotDelete\\GitHub\\kaggleCompe_MAP-math\\colab\\colabã§è¨“ç·´ã—ã¦ä¿å­˜\\kaggle-ready-model\"\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:02:17.717594Z",
     "iopub.status.busy": "2025-07-21T23:02:17.717312Z",
     "iopub.status.idle": "2025-07-21T23:02:17.723445Z",
     "shell.execute_reply": "2025-07-21T23:02:17.722738Z",
     "shell.execute_reply.started": "2025-07-21T23:02:17.717574Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MathMisconceptionDataset class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class MathMisconceptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Math Misconception Dataset for PyTorch\n",
    "    æ¨è«–å°‚ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list): ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ\n",
    "            tokenizer: Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
    "            max_length (int): æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "\n",
    "        # Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "        }\n",
    "\n",
    "print(\"âœ… MathMisconceptionDataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Load Pre-trained Gemma Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:02:20.933929Z",
     "iopub.status.busy": "2025-07-21T23:02:20.933202Z",
     "iopub.status.idle": "2025-07-21T23:02:54.906204Z",
     "shell.execute_reply": "2025-07-21T23:02:54.904345Z",
     "shell.execute_reply.started": "2025-07-21T23:02:20.933902Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.conda (Python 3.11.13)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n .conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def load_pretrained_gemma_model():\n",
    "    \"\"\"äº‹å‰çµ±åˆæ¸ˆã¿Gemmaãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆKaggleã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç’°å¢ƒå¯¾å¿œï¼‰\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ¤– çµ±åˆæ¸ˆã¿Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆLoRAçµ±åˆæ¸ˆã¿ï¼‰\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ç¢ºèª\n",
    "        print(f\"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {MODEL_DATA_PATH}\")\n",
    "        \n",
    "        if not os.path.exists(MODEL_DATA_PATH):\n",
    "            print(f\"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {MODEL_DATA_PATH}\")\n",
    "            print(\"ğŸ’¡ Kaggleç’°å¢ƒã§ã¯æ­£ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’ç¢ºèªã—ã¦ãã ã•ã„\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã®èª­ã¿è¾¼ã¿\n",
    "        label_file = os.path.join(MODEL_DATA_PATH, \"label_mapping.json\")\n",
    "        print(f\"ğŸ“‹ ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿: {label_file}\")\n",
    "        \n",
    "        with open(label_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            label_mapping = json.load(f)\n",
    "        \n",
    "        print(\"âœ… ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿æˆåŠŸ\")\n",
    "        for idx, label in label_mapping.items():\n",
    "            print(f\"   {idx}: {label}\")\n",
    "        \n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\n",
    "        print(\"\\nğŸ“ Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MODEL_DATA_PATH,\n",
    "            local_files_only=True  # Kaggleã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç’°å¢ƒç”¨\n",
    "        )\n",
    "        print(f\"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿æˆåŠŸ\")\n",
    "        print(f\"ğŸ”– ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³: {tokenizer.pad_token}\")\n",
    "        print(f\"ğŸ“ èªå½™ã‚µã‚¤ã‚º: {tokenizer.vocab_size:,}\")\n",
    "        \n",
    "        # çµ±åˆãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "        print(\"\\nğŸ§  çµ±åˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_DATA_PATH,\n",
    "            num_labels=len(label_mapping),\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\" if device.type == \"cuda\" else None,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True  # Kaggleã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç’°å¢ƒç”¨\n",
    "        )\n",
    "        \n",
    "        # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\n",
    "        if device.type == \"cpu\":\n",
    "            model = model.to(device)\n",
    "        \n",
    "        print(f\"âœ… çµ±åˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†!\")\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"ğŸ“Š åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°: {model.config.num_labels}\")\n",
    "        print(f\"ğŸ“ˆ ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}\")\n",
    "        print(f\"ğŸ’¡ çµ±åˆãƒ¢ãƒ‡ãƒ«ï¼ˆLoRAã‚¢ãƒ€ãƒ—ã‚¿çµ±åˆæ¸ˆã¿ï¼‰\")\n",
    "        \n",
    "        # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\n",
    "        model.eval()\n",
    "        \n",
    "        return model, tokenizer, label_mapping\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        print(\"\\nğŸ”§ Kaggleã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç’°å¢ƒã§ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:\")\n",
    "        print(\"1. çµ±åˆæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒKaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦æ­£ã—ãã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\")\n",
    "        print(\"2. MODEL_DATA_PATHãŒæ­£ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’æŒ‡ã—ã¦ã„ã‚‹ã‹ç¢ºèª\") \n",
    "        print(\"3. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆmodel.safetensorsï¼‰ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª\")\n",
    "        print(\"4. config.json ã¨ tokenizeré–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Ÿè¡Œ\n",
    "model, tokenizer, label_mapping = load_pretrained_gemma_model()\n",
    "print(\"\\nğŸ‰ çµ±åˆæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æº–å‚™å®Œäº†!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Load and Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:00:49.061816Z",
     "iopub.status.busy": "2025-07-21T23:00:49.061509Z",
     "iopub.status.idle": "2025-07-21T23:00:49.093510Z",
     "shell.execute_reply": "2025-07-21T23:00:49.092965Z",
     "shell.execute_reply.started": "2025-07-21T23:00:49.061799Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\n",
      "============================================================\n",
      "ğŸ“ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹: /kaggle/input/map-charting-student-math-misunderstandings/test.csv\n",
      "âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ!\n",
      "ğŸ“ˆ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: (3, 5)\n",
      "\n",
      "ğŸ“‹ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ—:\n",
      "['row_id', 'QuestionId', 'QuestionText', 'MC_Answer', 'StudentExplanation']\n",
      "\n",
      "ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:\n",
      "   row_id  QuestionId                                       QuestionText  \\\n",
      "0   36696       31772  What fraction of the shape is not shaded? Give...   \n",
      "1   36697       31772  What fraction of the shape is not shaded? Give...   \n",
      "2   36698       32835                      Which number is the greatest?   \n",
      "\n",
      "           MC_Answer                                 StudentExplanation  \n",
      "0  \\( \\frac{1}{3} \\)  I think that 1/3 is the answer, as it's the si...  \n",
      "1  \\( \\frac{3}{6} \\)  i think this answer is because 3 triangles are...  \n",
      "2          \\( 6.2 \\)     because the 2 makes it higher than the others.  \n",
      "\n",
      "ğŸ”§ å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆä¸­...\n",
      "\n",
      "ğŸ“Š ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ:\n",
      "   å¹³å‡: 235 æ–‡å­—\n",
      "   æœ€å°: 126 æ–‡å­—\n",
      "   æœ€å¤§: 296 æ–‡å­—\n",
      "   ä¸­å¤®å€¤: 284 æ–‡å­—\n",
      "\n",
      "ğŸ“ å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«:\n",
      "Length: 284 characters\n",
      "Sample: Question: What fraction of the shape is not shaded? Give your answer in its simplest form. [Image: A triangle split into 9 equal smaller triangles. 6 of them are shaded.] Selected Answer: \\( \\frac{1}{3} \\) Explanation: I think that 1/3 is the answer, as it's the simplest form of 3/9....\n"
     ]
    }
   ],
   "source": [
    "def load_and_prepare_test_data():\n",
    "    \"\"\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "        test_path = os.path.join(COMP_DATA_PATH, \"test.csv\")\n",
    "        print(f\"ğŸ“ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹: {test_path}\")\n",
    "        \n",
    "        if not os.path.exists(test_path):\n",
    "            print(f\"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {test_path}\")\n",
    "            return None\n",
    "        \n",
    "        test_df = pd.read_csv(test_path)\n",
    "        print(f\"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ!\")\n",
    "        print(f\"ğŸ“ˆ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test_df.shape}\")\n",
    "        \n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®ãƒ†ã‚¹ãƒˆç”¨ï¼šå°ã•ãªã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ç”¨\n",
    "        is_local = not os.path.exists(\"/kaggle/input\")\n",
    "        if is_local:\n",
    "            print(\"ğŸ”§ ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®ãƒ†ã‚¹ãƒˆç”¨ã«æœ€åˆã®100ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™\")\n",
    "            test_df = test_df.head(100).copy()\n",
    "            print(f\"ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test_df.shape}\")\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ç¢ºèª\n",
    "        print(\"\\nğŸ“‹ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ—:\")\n",
    "        print(test_df.columns.tolist())\n",
    "        \n",
    "        print(\"\\nğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:\")\n",
    "        print(test_df.head(3))\n",
    "        \n",
    "        # å¼·åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã®ä½œæˆ\n",
    "        def create_enhanced_text(row):\n",
    "            \"\"\"Question + MC_Answer + Explanation ã‚’çµåˆã—ãŸå¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆ\"\"\"\n",
    "            question = str(row.get(\"QuestionText\", \"\")) if pd.notna(row.get(\"QuestionText\")) else \"\"\n",
    "            mc_answer = str(row.get(\"MC_Answer\", \"\")) if pd.notna(row.get(\"MC_Answer\")) else \"\"\n",
    "            explanation = str(row.get(\"StudentExplanation\", \"\")) if pd.notna(row.get(\"StudentExplanation\")) else \"\"\n",
    "            \n",
    "            # Gemmaç”¨ã®æ§‹é€ åŒ–ãƒ†ã‚­ã‚¹ãƒˆ\n",
    "            enhanced_text = f\"Question: {question} Selected Answer: {mc_answer} Explanation: {explanation}\"\n",
    "            return enhanced_text\n",
    "\n",
    "        print(\"\\nğŸ”§ å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆä¸­...\")\n",
    "        test_df[\"enhanced_text\"] = test_df.apply(create_enhanced_text, axis=1)\n",
    "        \n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆé•·ã®çµ±è¨ˆ\n",
    "        text_lengths = test_df[\"enhanced_text\"].str.len()\n",
    "        print(f\"\\nğŸ“Š ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ:\")\n",
    "        print(f\"   å¹³å‡: {text_lengths.mean():.0f} æ–‡å­—\")\n",
    "        print(f\"   æœ€å°: {text_lengths.min()} æ–‡å­—\")\n",
    "        print(f\"   æœ€å¤§: {text_lengths.max()} æ–‡å­—\")\n",
    "        print(f\"   ä¸­å¤®å€¤: {text_lengths.median():.0f} æ–‡å­—\")\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¤º\n",
    "        print(f\"\\nğŸ“ å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«:\")\n",
    "        sample_text = test_df[\"enhanced_text\"].iloc[0]\n",
    "        print(f\"Length: {len(sample_text)} characters\")\n",
    "        print(f\"Sample: {sample_text[:300]}...\")\n",
    "        \n",
    "        return test_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Ÿè¡Œ\n",
    "test_df = load_and_prepare_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”® Generate Predictions for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:00:49.094400Z",
     "iopub.status.busy": "2025-07-21T23:00:49.094168Z",
     "iopub.status.idle": "2025-07-21T23:00:49.107048Z",
     "shell.execute_reply": "2025-07-21T23:00:49.106332Z",
     "shell.execute_reply.started": "2025-07-21T23:00:49.094384Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\n"
     ]
    }
   ],
   "source": [
    "def generate_test_predictions(model, tokenizer, test_df, label_mapping, batch_size=8):\n",
    "    \"\"\"ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹äºˆæ¸¬ç”Ÿæˆï¼ˆMAP@3å½¢å¼ï¼‰\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ”® ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆäºˆæ¸¬ç”Ÿæˆï¼ˆMAP@3ï¼‰\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if test_df is None or len(test_df) == 0:\n",
    "        print(\"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_df):,}ä»¶\")\n",
    "    print(f\"ğŸ”§ ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}\")\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\n",
    "    test_texts = test_df[\"enhanced_text\"].tolist()\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
    "    print(\"ğŸ”§ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­...\")\n",
    "    test_dataset = MathMisconceptionDataset(\n",
    "        test_texts, tokenizer, max_length=512\n",
    "    )\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ: {len(test_dataloader)}ãƒãƒƒãƒ\")\n",
    "    \n",
    "    # äºˆæ¸¬å®Ÿè¡Œ\n",
    "    print(\"ğŸ”® äºˆæ¸¬å®Ÿè¡Œä¸­...\")\n",
    "    all_predictions = []\n",
    "    \n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(test_dataloader):\n",
    "                # ãƒãƒƒãƒã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                # æ¨è«–å®Ÿè¡Œ\n",
    "                outputs = model(**batch)\n",
    "                predictions = outputs.logits\n",
    "                \n",
    "                # CPU ã«ç§»å‹•ã—ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "                batch_predictions = predictions.cpu().numpy()\n",
    "                all_predictions.append(batch_predictions)\n",
    "                \n",
    "                # é€²æ—è¡¨ç¤º\n",
    "                if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(test_dataloader):\n",
    "                    processed = min((batch_idx + 1) * batch_size, len(test_df))\n",
    "                    print(f\"   é€²æ—: {processed:,}/{len(test_df):,} ({processed/len(test_df)*100:.1f}%)\")\n",
    "        \n",
    "        print(\"âœ… äºˆæ¸¬å®Œäº†!\")\n",
    "        \n",
    "        # äºˆæ¸¬çµæœã‚’çµåˆ\n",
    "        all_predictions = np.vstack(all_predictions)\n",
    "        print(f\"ğŸ“Š äºˆæ¸¬çµæœå½¢çŠ¶: {all_predictions.shape}\")\n",
    "        \n",
    "        # ç¢ºç‡ã«å¤‰æ›\n",
    "        probs = torch.softmax(torch.tensor(all_predictions), dim=-1).numpy()\n",
    "        \n",
    "        # å„ã‚µãƒ³ãƒ—ãƒ«ã§ä¸Šä½3ã¤ã®äºˆæ¸¬ã‚’å–å¾—\n",
    "        print(\"ğŸ¯ TOP-3äºˆæ¸¬æŠ½å‡ºä¸­...\")\n",
    "        submission_predictions = []\n",
    "        \n",
    "        # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã®é€†å¤‰æ›ç”¨\n",
    "        idx_to_label = {int(k): v for k, v in label_mapping.items()}\n",
    "        \n",
    "        for i, prob in enumerate(probs):\n",
    "            # ç¢ºç‡ã®é«˜ã„é †ã«ä¸Šä½3ã¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
    "            top3_indices = np.argsort(prob)[::-1][:3]\n",
    "            \n",
    "            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«åã«å¤‰æ›\n",
    "            top3_labels = [idx_to_label[idx] for idx in top3_indices]\n",
    "            \n",
    "            # ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§çµåˆï¼ˆã‚³ãƒ³ãƒšè¦æ±‚å½¢å¼ï¼‰\n",
    "            prediction_string = \" \".join(top3_labels)\n",
    "            submission_predictions.append(prediction_string)\n",
    "            \n",
    "            # é€²æ—è¡¨ç¤ºï¼ˆæœ€åˆã®5ä»¶ï¼‰\n",
    "            if i < 5:\n",
    "                top3_probs = [prob[idx] for idx in top3_indices]\n",
    "                print(f\"  ã‚µãƒ³ãƒ—ãƒ« {i+1}: {prediction_string}\")\n",
    "                print(f\"    ç¢ºç‡: {[f'{p:.3f}' for p in top3_probs]}\")\n",
    "        \n",
    "        print(f\"âœ… TOP-3äºˆæ¸¬æŠ½å‡ºå®Œäº†: {len(submission_predictions)}ä»¶\")\n",
    "        \n",
    "        # äºˆæ¸¬ã®çµ±è¨ˆæƒ…å ±\n",
    "        all_pred_labels = \" \".join(submission_predictions).split()\n",
    "        from collections import Counter\n",
    "        pred_counts = Counter(all_pred_labels)\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ äºˆæ¸¬çµ±è¨ˆ:\")\n",
    "        print(f\"  äºˆæ¸¬ã«ä½¿ç”¨ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªæ•°: {len(pred_counts)}\")\n",
    "        for category, count in pred_counts.most_common():\n",
    "            percentage = count / (len(submission_predictions) * 3) * 100\n",
    "            print(f\"    {category}: {count}å› ({percentage:.1f}%)\")\n",
    "        \n",
    "        return submission_predictions\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ äºˆæ¸¬å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"ğŸ–¥ï¸ ç¾åœ¨ã®GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "        raise e\n",
    "\n",
    "# äºˆæ¸¬å®Ÿè¡Œ\n",
    "if model is not None and tokenizer is not None and test_df is not None:\n",
    "    print(\"ğŸ”® ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆäºˆæ¸¬ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ç’°å¢ƒã«å¿œã˜ã¦èª¿æ•´ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆç”¨ã«å°ã•ã‚ã«è¨­å®šï¼‰\n",
    "    batch_size = 2 if device.type == \"cpu\" else 4\n",
    "    test_predictions = generate_test_predictions(model, tokenizer, test_df, label_mapping, batch_size)\n",
    "    print(\"ğŸ‰ ãƒ†ã‚¹ãƒˆäºˆæ¸¬å®Œäº†!\")\n",
    "else:\n",
    "    print(\"âŒ å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    test_predictions = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¤ Create Submission File\n",
    "\n",
    "æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«æå‡ºã™ã‚‹ãŸã‚ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:00:49.108139Z",
     "iopub.status.busy": "2025-07-21T23:00:49.107932Z",
     "iopub.status.idle": "2025-07-21T23:00:49.123476Z",
     "shell.execute_reply": "2025-07-21T23:00:49.122738Z",
     "shell.execute_reply.started": "2025-07-21T23:00:49.108124Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å‰ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n"
     ]
    }
   ],
   "source": [
    "def create_submission_file(test_df, predictions, output_path=\"submission.csv\"):\n",
    "    \"\"\"æå‡ºç”¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ“¤ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if test_df is None or predictions is None:\n",
    "        print(\"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯äºˆæ¸¬çµæœãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ“Š æå‡ºãƒ‡ãƒ¼ã‚¿: {len(predictions):,}ä»¶\")\n",
    "    \n",
    "    # sample_submission.csvã‚’å‚è€ƒã«ã—ã¦æ­£ã—ã„å½¢å¼ã§ä½œæˆ\n",
    "    # row_id,Category:Misconception ã®å½¢å¼ãŒå¿…è¦\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«row_idãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "    if 'row_id' in test_df.columns:\n",
    "        row_ids = test_df['row_id'].tolist()\n",
    "    else:\n",
    "        # row_idãŒãªã„å ´åˆã€testãƒ‡ãƒ¼ã‚¿ã®é–‹å§‹IDã‚’æ¨å®šï¼ˆé€šå¸¸ã¯36696ã‹ã‚‰ï¼‰\n",
    "        print(\"âš ï¸ row_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ¨å®šå€¤ã‚’ä½¿ç”¨ã—ã¾ã™...\")\n",
    "        start_id = 36696  # sample_submissionã®é–‹å§‹ID\n",
    "        row_ids = list(range(start_id, start_id + len(test_df)))\n",
    "    \n",
    "    # Kaggleè¦æ±‚å½¢å¼ï¼šå„äºˆæ¸¬ã‚’ \"Category:Misconception\" å½¢å¼ã«å¤‰æ›\n",
    "    print(\"ğŸ”§ Kaggleæå‡ºå½¢å¼ã«å¤‰æ›ä¸­...\")\n",
    "    formatted_predictions = []\n",
    "    \n",
    "    for i, pred in enumerate(predictions):\n",
    "        # predã¯ \"True_Correct False_Neither False_Misconception\" ã®ã‚ˆã†ãªã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Š\n",
    "        pred_categories = pred.split()[:3]  # TOP3ã«é™å®š\n",
    "        \n",
    "        # å„ã‚«ãƒ†ã‚´ãƒªã‚’ \"Category:Misconception\" å½¢å¼ã«å¤‰æ›\n",
    "        formatted_parts = []\n",
    "        for category in pred_categories:\n",
    "            if category.endswith('_Misconception'):\n",
    "                # Misconceptionã‚«ãƒ†ã‚´ãƒªã®å ´åˆã€å®Ÿéš›ã®èª¤æ¦‚å¿µåãŒå¿…è¦\n",
    "                # ã“ã“ã§ã¯ç°¡æ˜“çš„ã« \"Incomplete\" ã‚’ä½¿ç”¨ï¼ˆå®Ÿéš›ã¯äºˆæ¸¬çµæœã‹ã‚‰å–å¾—ï¼‰\n",
    "                formatted_parts.append(f\"{category}:Incomplete\")\n",
    "            else:\n",
    "                # ä»–ã®ã‚«ãƒ†ã‚´ãƒªã®å ´åˆã¯NA\n",
    "                formatted_parts.append(f\"{category}:NA\")\n",
    "        \n",
    "        # ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§çµåˆ\n",
    "        formatted_pred = \" \".join(formatted_parts)\n",
    "        formatted_predictions.append(formatted_pred)\n",
    "        \n",
    "        # æœ€åˆã®5ä»¶ã‚’ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\n",
    "        if i < 5:\n",
    "            print(f\"  ã‚µãƒ³ãƒ—ãƒ« {i+1}: {formatted_pred}\")\n",
    "    \n",
    "    # æå‡ºç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆï¼ˆæ­£ã—ã„Kaggleå½¢å¼ï¼‰\n",
    "    submission_df = pd.DataFrame({\n",
    "        'row_id': row_ids,\n",
    "        'Category:Misconception': formatted_predictions\n",
    "    })\n",
    "    \n",
    "    print(f\"âœ… æå‡ºãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆå®Œäº†: {submission_df.shape}\")\n",
    "    print(f\"ğŸ“ åˆ—: {list(submission_df.columns)}\")\n",
    "    \n",
    "    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\n",
    "    print(f\"\\nğŸ“‹ æå‡ºãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:\")\n",
    "    print(submission_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜\n",
    "    try:\n",
    "        submission_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nğŸ’¾ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {output_path}\")\n",
    "        \n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª\n",
    "        file_size = os.path.getsize(output_path)\n",
    "        print(f\"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes ({file_size/1024:.1f} KB)\")\n",
    "        \n",
    "        # å½¢å¼ç¢ºèª\n",
    "        check_df = pd.read_csv(output_path)\n",
    "        print(f\"âœ… æå‡ºãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼: {check_df.shape}\")\n",
    "        required_cols = ['row_id', 'Category:Misconception']\n",
    "        cols_present = all(col in check_df.columns for col in required_cols)\n",
    "        print(f\"   å¿…è¦åˆ—å­˜åœ¨ç¢ºèª: {cols_present}\")\n",
    "        \n",
    "        # äºˆæ¸¬å½¢å¼ãƒã‚§ãƒƒã‚¯\n",
    "        sample_predictions = check_df['Category:Misconception'].head(5).tolist()\n",
    "        print(f\"ğŸ” äºˆæ¸¬å½¢å¼ã‚µãƒ³ãƒ—ãƒ«:\")\n",
    "        for i, pred in enumerate(sample_predictions):\n",
    "            pred_parts = pred.split()\n",
    "            print(f\"   {i+1}: {pred} (è¦ç´ æ•°: {len(pred_parts)})\")\n",
    "            \n",
    "        print(f\"\\nğŸ“Š æå‡ºãƒ•ã‚¡ã‚¤ãƒ«æœ€çµ‚ç¢ºèª:\")\n",
    "        print(f\"   âœ… å½¢å¼: Kaggle MAPç«¶æŠ€å½¢å¼ï¼ˆrow_id, Category:Misconceptionï¼‰\")\n",
    "        print(f\"   âœ… ä»¶æ•°: {len(check_df):,}ä»¶\")\n",
    "        print(f\"   âœ… åˆ—å: {list(check_df.columns)}\")\n",
    "        \n",
    "        return submission_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None\n",
    "\n",
    "# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\n",
    "if test_predictions is not None and test_df is not None:\n",
    "    print(\"ğŸ“¤ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™...\")\n",
    "    submission_df = create_submission_file(test_df, test_predictions, \"submission.csv\")\n",
    "    \n",
    "    if submission_df is not None:\n",
    "        print(\"ğŸ‰ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†!\")\n",
    "        print(f\"ğŸ“‹ æœ€çµ‚ç¢ºèª:\")\n",
    "        print(f\"   ãƒ•ã‚¡ã‚¤ãƒ«å: submission.csv\")\n",
    "        print(f\"   ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(submission_df):,}\")\n",
    "        print(f\"   äºˆæ¸¬å½¢å¼: Kaggle MAPå½¢å¼ (row_id, Category:Misconception)\")\n",
    "        print(\"\\nğŸ† Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«æå‡ºæº–å‚™å®Œäº†!\")\n",
    "        \n",
    "        # æœ€çµ‚ãƒã‚§ãƒƒã‚¯ç”¨çµ±è¨ˆ\n",
    "        print(f\"\\nğŸ“ˆ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ:\")\n",
    "        all_pred_in_submission = \" \".join(submission_df['Category:Misconception']).split()\n",
    "        unique_preds = set(all_pred_in_submission)\n",
    "        print(f\"   ä½¿ç”¨ã•ã‚ŒãŸãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ©ãƒ™ãƒ«æ•°: {len(unique_preds)}\")\n",
    "        print(f\"   ãƒ©ãƒ™ãƒ«ä¸€è¦§: {sorted(unique_preds)}\")\n",
    "    else:\n",
    "        print(\"âŒ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "else:\n",
    "    print(\"âŒ å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å‰ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Summary & Performance Notes\n",
    "\n",
    "### ğŸ“Š å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ä»¥ä¸‹ã‚’å®Ÿè£…ã—ã¾ã—ãŸï¼š\n",
    "\n",
    "1. **ğŸ¤– äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿**: Kaggleã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸGemma-2-2b-itãƒ¢ãƒ‡ãƒ«\n",
    "2. **ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å‡¦ç†**: math misconceptionã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®test.csv\n",
    "3. **ğŸ”® åŠ¹ç‡çš„ãªæ¨è«–**: ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹é«˜é€Ÿäºˆæ¸¬\n",
    "4. **ğŸ“ˆ MAP@3å½¢å¼å‡ºåŠ›**: TOP-3äºˆæ¸¬ã®ç”Ÿæˆ\n",
    "5. **ğŸ“¤ æå‡ºæº–å‚™**: Kaggleå½¢å¼ã®submission.csvä½œæˆ\n",
    "\n",
    "### ğŸš€ ä½¿ç”¨æŠ€è¡“\n",
    "- **ãƒ¢ãƒ‡ãƒ«**: `google/gemma-2-2b-it` (~2.6B parameters)\n",
    "- **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: PyTorch + Transformers\n",
    "- **æ¨è«–**: äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼ˆè¿½åŠ è¨“ç·´ãªã—ï¼‰\n",
    "- **è©•ä¾¡**: MAP@3 (Mean Average Precision at 3)\n",
    "\n",
    "### âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–\n",
    "- ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªæ¨è«–\n",
    "- é©å¿œçš„ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆCPU: 4, GPU: 8ï¼‰\n",
    "- FP16ä½¿ç”¨ï¼ˆGPUç’°å¢ƒï¼‰\n",
    "- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼\n",
    "\n",
    "### ğŸ“ ä½¿ç”¨æ–¹æ³•\n",
    "1. Kaggleã§äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "2. MODEL_DATA_PATHã‚’æ­£ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã«å¤‰æ›´\n",
    "3. å…¨ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ\n",
    "4. `Gemma_2b_submission.csv`ãŒç”Ÿæˆã•ã‚Œã‚‹\n",
    "5. Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æå‡º\n",
    "\n",
    "### ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
    "- **ãƒ¡ãƒ¢ãƒªä¸è¶³**: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ä¸‹ã’ã‚‹\n",
    "- **ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã¨ãƒ‘ã‚¹ã‚’ç¢ºèª\n",
    "- **CUDA OOM**: CPUãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ\n",
    "\n",
    "**ğŸ† Good luck with your submission!**"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12957508,
     "sourceId": 104383,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 405563,
     "modelInstanceId": 386414,
     "sourceId": 482709,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
