{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯 MAP Competition: Gemma-2-2b-it Submission Notebook (Kaggle Offline Ready)\n",
    "\n",
    "## Overview\n",
    "このノートブックは、事前に訓練・統合されたGemma-2-2b-itモデルを使用して、MAP - Charting Student Math Misunderstandingsコンペティションの提出を行います。\n",
    "\n",
    "### Model Details\n",
    "- **Pre-trained Model**: google/gemma-2-2b-it (LoRAアダプタ統合済み)\n",
    "- **Parameters**: ~2.6B\n",
    "- **Task**: 6-class text classification\n",
    "- **Evaluation Metric**: MAP@3\n",
    "- **Kaggle Compatibility**: ✅ オフライン環境対応\n",
    "\n",
    "### Target Classes (6分類)\n",
    "- **True_Correct**: 正解で正しい説明\n",
    "- **True_Neither**: 正解だが曖昧な説明\n",
    "- **True_Misconception**: 正解だが誤った概念の説明\n",
    "- **False_Correct**: 不正解だが正しい概念の説明\n",
    "- **False_Neither**: 不正解で曖昧な説明\n",
    "- **False_Misconception**: 不正解で誤った概念の説明\n",
    "\n",
    "### Strategy\n",
    "1. **統合済みモデル読み込み**: Kaggleオフライン環境でも動作\n",
    "2. **テストデータ前処理**: 強化テキスト特徴量作成\n",
    "3. **効率的推論**: バッチ処理による高速予測\n",
    "4. **MAP@3形式出力**: TOP-3予測の生成\n",
    "5. **提出ファイル作成**: Kaggle形式のsubmission.csv作成\n",
    "\n",
    "### Kaggle使用方法\n",
    "1. **統合モデルアップロード**: `kaggle-ready-model`フォルダをKaggleデータセットとしてアップロード\n",
    "2. **パス設定**: MODEL_DATA_PATHを正しいデータセット名に変更\n",
    "3. **実行**: 全セルを順番に実行\n",
    "4. **提出**: 生成されたCSVファイルをコンペティションに提出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:00:28.087317Z",
     "iopub.status.busy": "2025-07-21T23:00:28.087099Z",
     "iopub.status.idle": "2025-07-21T23:00:34.897845Z",
     "shell.execute_reply": "2025-07-21T23:00:34.897078Z",
     "shell.execute_reply.started": "2025-07-21T23:00:28.087300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Kaggle環境で必要なライブラリをインストール\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"パッケージをインストールする関数\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# 必要なライブラリのインストール（統合モデル用）\n",
    "required_packages = [\n",
    "    \"transformers>=4.35.0\",\n",
    "    \"accelerate>=0.26.0\", \n",
    "    \"sentencepiece>=0.1.99\"\n",
    "    # 注意: 統合モデルではPEFTは不要\n",
    "]\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        # インポートテストでインストール状況確認\n",
    "        if \"transformers\" in package:\n",
    "            import transformers\n",
    "            print(f\"✅ transformers already installed: {transformers.__version__}\")\n",
    "        elif \"accelerate\" in package:\n",
    "            import accelerate\n",
    "            print(f\"✅ accelerate already installed: {accelerate.__version__}\")\n",
    "        elif \"sentencepiece\" in package:\n",
    "            import sentencepiece\n",
    "            print(f\"✅ sentencepiece already installed: {sentencepiece.__version__}\")\n",
    "    except ImportError:\n",
    "        print(f\"📦 Installing {package}...\")\n",
    "        install_package(package)\n",
    "\n",
    "print(\"🎉 All required libraries are ready!\")\n",
    "print(\"📋 注意: 統合モデルを使用するためPEFTライブラリは不要です\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Import Dependencies (Unified Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:02:08.177464Z",
     "iopub.status.busy": "2025-07-21T23:02:08.176724Z",
     "iopub.status.idle": "2025-07-21T23:02:08.184632Z",
     "shell.execute_reply": "2025-07-21T23:02:08.183859Z",
     "shell.execute_reply.started": "2025-07-21T23:02:08.177436Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Environment Information:\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "GPU device: Tesla T4\n",
      "GPU memory: 15.8 GB\n",
      "📁 Kaggle environment detected: /kaggle/input\n",
      "🎯 Competition data path: /kaggle/input/map-charting-student-math-misunderstandings\n",
      "🤖 Model data path: /kaggle/input/gemma-2-2b-math-model/transformers/default/1/gemma-2-2b-math-model\n",
      "✅ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# 基本ライブラリ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# 機械学習ライブラリ\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformersライブラリ\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "# 警告を非表示\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# デバッグ情報表示\n",
    "print(\"🔧 Environment Information:\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Kaggleのデータパス設定\n",
    "KAGGLE_INPUT_PATH = \"/kaggle/input\"\n",
    "if os.path.exists(KAGGLE_INPUT_PATH):\n",
    "    print(f\"📁 Kaggle environment detected: {KAGGLE_INPUT_PATH}\")\n",
    "    # コンペティションデータパス\n",
    "    COMP_DATA_PATH = \"/kaggle/input/map-charting-student-math-misunderstandings\"\n",
    "    # 統合モデルのパス（Kaggleデータセット名に変更してください）\n",
    "    MODEL_DATA_PATH = \"/kaggle/input/gemma-2-2b-merged-model\"\n",
    "    print(f\"🎯 Competition data path: {COMP_DATA_PATH}\")\n",
    "    print(f\"🤖 Model data path (merged): {MODEL_DATA_PATH}\")\n",
    "else:\n",
    "    print(\"📁 Local environment detected\")\n",
    "    COMP_DATA_PATH = r\"C:\\Users\\mouse\\Desktop\\NotDelete\\GitHub\\kaggleCompe_MAP-math\\map_data\"\n",
    "    # ローカルの統合モデルパス\n",
    "    MODEL_DATA_PATH = r\"C:\\Users\\mouse\\Desktop\\NotDelete\\GitHub\\kaggleCompe_MAP-math\\colab\\colabで訓練して保存\\kaggle-ready-model\"\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:02:17.717594Z",
     "iopub.status.busy": "2025-07-21T23:02:17.717312Z",
     "iopub.status.idle": "2025-07-21T23:02:17.723445Z",
     "shell.execute_reply": "2025-07-21T23:02:17.722738Z",
     "shell.execute_reply.started": "2025-07-21T23:02:17.717574Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MathMisconceptionDataset class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class MathMisconceptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Math Misconception Dataset for PyTorch\n",
    "    推論専用のデータセット\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list): テキストデータのリスト\n",
    "            tokenizer: Gemmaトークナイザー\n",
    "            max_length (int): 最大トークン長\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "\n",
    "        # Gemmaトークナイザーでテキストをエンコード\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "        }\n",
    "\n",
    "print(\"✅ MathMisconceptionDataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Load Pre-trained Gemma Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:02:20.933929Z",
     "iopub.status.busy": "2025-07-21T23:02:20.933202Z",
     "iopub.status.idle": "2025-07-21T23:02:54.906204Z",
     "shell.execute_reply": "2025-07-21T23:02:54.904345Z",
     "shell.execute_reply.started": "2025-07-21T23:02:20.933902Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.conda (Python 3.11.13)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n .conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def load_pretrained_gemma_model():\n",
    "    \"\"\"事前統合済みGemmaモデルの読み込み（Kaggleオフライン環境対応）\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🤖 統合済みGemmaモデル読み込み（LoRA統合済み）\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # モデルパス確認\n",
    "        print(f\"📁 モデルパス: {MODEL_DATA_PATH}\")\n",
    "        \n",
    "        if not os.path.exists(MODEL_DATA_PATH):\n",
    "            print(f\"❌ モデルパスが存在しません: {MODEL_DATA_PATH}\")\n",
    "            print(\"💡 Kaggle環境では正しいデータセット名を確認してください\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # ラベルマッピングの読み込み\n",
    "        label_file = os.path.join(MODEL_DATA_PATH, \"label_mapping.json\")\n",
    "        print(f\"📋 ラベルマッピング読み込み: {label_file}\")\n",
    "        \n",
    "        with open(label_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            label_mapping = json.load(f)\n",
    "        \n",
    "        print(\"✅ ラベルマッピング読み込み成功\")\n",
    "        for idx, label in label_mapping.items():\n",
    "            print(f\"   {idx}: {label}\")\n",
    "        \n",
    "        # トークナイザーの読み込み\n",
    "        print(\"\\n📝 Gemmaトークナイザー読み込み中...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MODEL_DATA_PATH,\n",
    "            local_files_only=True  # Kaggleオフライン環境用\n",
    "        )\n",
    "        print(f\"✅ トークナイザー読み込み成功\")\n",
    "        print(f\"🔖 パディングトークン: {tokenizer.pad_token}\")\n",
    "        print(f\"📏 語彙サイズ: {tokenizer.vocab_size:,}\")\n",
    "        \n",
    "        # 統合モデルの読み込み\n",
    "        print(\"\\n🧠 統合モデル読み込み中...\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_DATA_PATH,\n",
    "            num_labels=len(label_mapping),\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\" if device.type == \"cuda\" else None,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True  # Kaggleオフライン環境用\n",
    "        )\n",
    "        \n",
    "        # デバイスに移動（必要に応じて）\n",
    "        if device.type == \"cpu\":\n",
    "            model = model.to(device)\n",
    "        \n",
    "        print(f\"✅ 統合モデル読み込み完了!\")\n",
    "        \n",
    "        # モデル情報表示\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"📊 分類クラス数: {model.config.num_labels}\")\n",
    "        print(f\"📈 総パラメータ数: {total_params:,}\")\n",
    "        print(f\"💡 統合モデル（LoRAアダプタ統合済み）\")\n",
    "        \n",
    "        # 推論モードに設定\n",
    "        model.eval()\n",
    "        \n",
    "        return model, tokenizer, label_mapping\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ モデル読み込みエラー: {e}\")\n",
    "        print(\"\\n🔧 Kaggleオフライン環境でのトラブルシューティング:\")\n",
    "        print(\"1. 統合済みモデルがKaggleデータセットとして正しくアップロードされているか確認\")\n",
    "        print(\"2. MODEL_DATA_PATHが正しいデータセット名を指しているか確認\") \n",
    "        print(\"3. モデルファイル（model.safetensors）が存在するか確認\")\n",
    "        print(\"4. config.json と tokenizer関連ファイルが存在するか確認\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "\n",
    "# モデル読み込み実行\n",
    "model, tokenizer, label_mapping = load_pretrained_gemma_model()\n",
    "print(\"\\n🎉 統合済みモデル準備完了!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Load and Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:00:49.061816Z",
     "iopub.status.busy": "2025-07-21T23:00:49.061509Z",
     "iopub.status.idle": "2025-07-21T23:00:49.093510Z",
     "shell.execute_reply": "2025-07-21T23:00:49.092965Z",
     "shell.execute_reply.started": "2025-07-21T23:00:49.061799Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "📊 テストデータ読み込みと前処理\n",
      "============================================================\n",
      "📁 テストデータパス: /kaggle/input/map-charting-student-math-misunderstandings/test.csv\n",
      "✅ テストデータ読み込み成功!\n",
      "📈 テストデータ形状: (3, 5)\n",
      "\n",
      "📋 テストデータの列:\n",
      "['row_id', 'QuestionId', 'QuestionText', 'MC_Answer', 'StudentExplanation']\n",
      "\n",
      "📋 データサンプル:\n",
      "   row_id  QuestionId                                       QuestionText  \\\n",
      "0   36696       31772  What fraction of the shape is not shaded? Give...   \n",
      "1   36697       31772  What fraction of the shape is not shaded? Give...   \n",
      "2   36698       32835                      Which number is the greatest?   \n",
      "\n",
      "           MC_Answer                                 StudentExplanation  \n",
      "0  \\( \\frac{1}{3} \\)  I think that 1/3 is the answer, as it's the si...  \n",
      "1  \\( \\frac{3}{6} \\)  i think this answer is because 3 triangles are...  \n",
      "2          \\( 6.2 \\)     because the 2 makes it higher than the others.  \n",
      "\n",
      "🔧 強化テキスト特徴量作成中...\n",
      "\n",
      "📊 テキスト長統計:\n",
      "   平均: 235 文字\n",
      "   最小: 126 文字\n",
      "   最大: 296 文字\n",
      "   中央値: 284 文字\n",
      "\n",
      "📝 強化テキストサンプル:\n",
      "Length: 284 characters\n",
      "Sample: Question: What fraction of the shape is not shaded? Give your answer in its simplest form. [Image: A triangle split into 9 equal smaller triangles. 6 of them are shaded.] Selected Answer: \\( \\frac{1}{3} \\) Explanation: I think that 1/3 is the answer, as it's the simplest form of 3/9....\n"
     ]
    }
   ],
   "source": [
    "def load_and_prepare_test_data():\n",
    "    \"\"\"テストデータの読み込みと前処理\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"📊 テストデータ読み込みと前処理\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # テストデータの読み込み\n",
    "        test_path = os.path.join(COMP_DATA_PATH, \"test.csv\")\n",
    "        print(f\"📁 テストデータパス: {test_path}\")\n",
    "        \n",
    "        if not os.path.exists(test_path):\n",
    "            print(f\"❌ テストデータが見つかりません: {test_path}\")\n",
    "            return None\n",
    "        \n",
    "        test_df = pd.read_csv(test_path)\n",
    "        print(f\"✅ テストデータ読み込み成功!\")\n",
    "        print(f\"📈 テストデータ形状: {test_df.shape}\")\n",
    "        \n",
    "        # ローカル環境でのテスト用：小さなサンプルを使用\n",
    "        is_local = not os.path.exists(\"/kaggle/input\")\n",
    "        if is_local:\n",
    "            print(\"🔧 ローカル環境でのテスト用に最初の100件のサンプルを使用します\")\n",
    "            test_df = test_df.head(100).copy()\n",
    "            print(f\"📊 サンプルデータ形状: {test_df.shape}\")\n",
    "        \n",
    "        # データ確認\n",
    "        print(\"\\n📋 テストデータの列:\")\n",
    "        print(test_df.columns.tolist())\n",
    "        \n",
    "        print(\"\\n📋 データサンプル:\")\n",
    "        print(test_df.head(3))\n",
    "        \n",
    "        # 強化されたテキスト特徴量の作成\n",
    "        def create_enhanced_text(row):\n",
    "            \"\"\"Question + MC_Answer + Explanation を結合した強化テキスト\"\"\"\n",
    "            question = str(row.get(\"QuestionText\", \"\")) if pd.notna(row.get(\"QuestionText\")) else \"\"\n",
    "            mc_answer = str(row.get(\"MC_Answer\", \"\")) if pd.notna(row.get(\"MC_Answer\")) else \"\"\n",
    "            explanation = str(row.get(\"StudentExplanation\", \"\")) if pd.notna(row.get(\"StudentExplanation\")) else \"\"\n",
    "            \n",
    "            # Gemma用の構造化テキスト\n",
    "            enhanced_text = f\"Question: {question} Selected Answer: {mc_answer} Explanation: {explanation}\"\n",
    "            return enhanced_text\n",
    "\n",
    "        print(\"\\n🔧 強化テキスト特徴量作成中...\")\n",
    "        test_df[\"enhanced_text\"] = test_df.apply(create_enhanced_text, axis=1)\n",
    "        \n",
    "        # テキスト長の統計\n",
    "        text_lengths = test_df[\"enhanced_text\"].str.len()\n",
    "        print(f\"\\n📊 テキスト長統計:\")\n",
    "        print(f\"   平均: {text_lengths.mean():.0f} 文字\")\n",
    "        print(f\"   最小: {text_lengths.min()} 文字\")\n",
    "        print(f\"   最大: {text_lengths.max()} 文字\")\n",
    "        print(f\"   中央値: {text_lengths.median():.0f} 文字\")\n",
    "        \n",
    "        # サンプルテキストの表示\n",
    "        print(f\"\\n📝 強化テキストサンプル:\")\n",
    "        sample_text = test_df[\"enhanced_text\"].iloc[0]\n",
    "        print(f\"Length: {len(sample_text)} characters\")\n",
    "        print(f\"Sample: {sample_text[:300]}...\")\n",
    "        \n",
    "        return test_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ テストデータ読み込みエラー: {e}\")\n",
    "        return None\n",
    "\n",
    "# テストデータ読み込み実行\n",
    "test_df = load_and_prepare_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔮 Generate Predictions for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:00:49.094400Z",
     "iopub.status.busy": "2025-07-21T23:00:49.094168Z",
     "iopub.status.idle": "2025-07-21T23:00:49.107048Z",
     "shell.execute_reply": "2025-07-21T23:00:49.106332Z",
     "shell.execute_reply.started": "2025-07-21T23:00:49.094384Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ 必要なコンポーネントが準備されていません\n"
     ]
    }
   ],
   "source": [
    "def generate_test_predictions(model, tokenizer, test_df, label_mapping, batch_size=8):\n",
    "    \"\"\"テストセットに対する予測生成（MAP@3形式）\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🔮 テストセット予測生成（MAP@3）\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if test_df is None or len(test_df) == 0:\n",
    "        print(\"❌ テストデータが利用できません\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"📊 テストデータ: {len(test_df):,}件\")\n",
    "    print(f\"🔧 バッチサイズ: {batch_size}\")\n",
    "    \n",
    "    # テストデータの前処理\n",
    "    test_texts = test_df[\"enhanced_text\"].tolist()\n",
    "    \n",
    "    # テストデータセット作成\n",
    "    print(\"🔧 テストデータセット作成中...\")\n",
    "    test_dataset = MathMisconceptionDataset(\n",
    "        test_texts, tokenizer, max_length=512\n",
    "    )\n",
    "    \n",
    "    # データローダー作成\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ テストデータローダー作成: {len(test_dataloader)}バッチ\")\n",
    "    \n",
    "    # 予測実行\n",
    "    print(\"🔮 予測実行中...\")\n",
    "    all_predictions = []\n",
    "    \n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(test_dataloader):\n",
    "                # バッチをデバイスに移動\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                # 推論実行\n",
    "                outputs = model(**batch)\n",
    "                predictions = outputs.logits\n",
    "                \n",
    "                # CPU に移動してリストに追加\n",
    "                batch_predictions = predictions.cpu().numpy()\n",
    "                all_predictions.append(batch_predictions)\n",
    "                \n",
    "                # 進捗表示\n",
    "                if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(test_dataloader):\n",
    "                    processed = min((batch_idx + 1) * batch_size, len(test_df))\n",
    "                    print(f\"   進捗: {processed:,}/{len(test_df):,} ({processed/len(test_df)*100:.1f}%)\")\n",
    "        \n",
    "        print(\"✅ 予測完了!\")\n",
    "        \n",
    "        # 予測結果を結合\n",
    "        all_predictions = np.vstack(all_predictions)\n",
    "        print(f\"📊 予測結果形状: {all_predictions.shape}\")\n",
    "        \n",
    "        # 確率に変換\n",
    "        probs = torch.softmax(torch.tensor(all_predictions), dim=-1).numpy()\n",
    "        \n",
    "        # 各サンプルで上位3つの予測を取得\n",
    "        print(\"🎯 TOP-3予測抽出中...\")\n",
    "        submission_predictions = []\n",
    "        \n",
    "        # ラベルマッピングの逆変換用\n",
    "        idx_to_label = {int(k): v for k, v in label_mapping.items()}\n",
    "        \n",
    "        for i, prob in enumerate(probs):\n",
    "            # 確率の高い順に上位3つのインデックスを取得\n",
    "            top3_indices = np.argsort(prob)[::-1][:3]\n",
    "            \n",
    "            # インデックスを実際のラベル名に変換\n",
    "            top3_labels = [idx_to_label[idx] for idx in top3_indices]\n",
    "            \n",
    "            # スペース区切りで結合（コンペ要求形式）\n",
    "            prediction_string = \" \".join(top3_labels)\n",
    "            submission_predictions.append(prediction_string)\n",
    "            \n",
    "            # 進捗表示（最初の5件）\n",
    "            if i < 5:\n",
    "                top3_probs = [prob[idx] for idx in top3_indices]\n",
    "                print(f\"  サンプル {i+1}: {prediction_string}\")\n",
    "                print(f\"    確率: {[f'{p:.3f}' for p in top3_probs]}\")\n",
    "        \n",
    "        print(f\"✅ TOP-3予測抽出完了: {len(submission_predictions)}件\")\n",
    "        \n",
    "        # 予測の統計情報\n",
    "        all_pred_labels = \" \".join(submission_predictions).split()\n",
    "        from collections import Counter\n",
    "        pred_counts = Counter(all_pred_labels)\n",
    "        \n",
    "        print(f\"\\n📈 予測統計:\")\n",
    "        print(f\"  予測に使用されたカテゴリ数: {len(pred_counts)}\")\n",
    "        for category, count in pred_counts.most_common():\n",
    "            percentage = count / (len(submission_predictions) * 3) * 100\n",
    "            print(f\"    {category}: {count}回 ({percentage:.1f}%)\")\n",
    "        \n",
    "        return submission_predictions\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 予測実行中にエラー: {e}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"🖥️ 現在のGPUメモリ使用量: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "        raise e\n",
    "\n",
    "# 予測実行\n",
    "if model is not None and tokenizer is not None and test_df is not None:\n",
    "    print(\"🔮 テストセット予測を開始します...\")\n",
    "    # バッチサイズを環境に応じて調整（ローカルテスト用に小さめに設定）\n",
    "    batch_size = 2 if device.type == \"cpu\" else 4\n",
    "    test_predictions = generate_test_predictions(model, tokenizer, test_df, label_mapping, batch_size)\n",
    "    print(\"🎉 テスト予測完了!\")\n",
    "else:\n",
    "    print(\"❌ 必要なコンポーネントが準備されていません\")\n",
    "    test_predictions = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📤 Create Submission File\n",
    "\n",
    "最終ステップとして、Kaggleコンペティションに提出するためのCSVファイルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T23:00:49.108139Z",
     "iopub.status.busy": "2025-07-21T23:00:49.107932Z",
     "iopub.status.idle": "2025-07-21T23:00:49.123476Z",
     "shell.execute_reply": "2025-07-21T23:00:49.122738Z",
     "shell.execute_reply.started": "2025-07-21T23:00:49.108124Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ 必要なデータが準備されていません。前のセルを実行してください。\n"
     ]
    }
   ],
   "source": [
    "def create_submission_file(test_df, predictions, output_path=\"submission.csv\"):\n",
    "    \"\"\"提出用CSVファイルの作成\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"📤 提出ファイル作成\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if test_df is None or predictions is None:\n",
    "        print(\"❌ テストデータまたは予測結果がありません\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"📊 提出データ: {len(predictions):,}件\")\n",
    "    \n",
    "    # sample_submission.csvを参考にして正しい形式で作成\n",
    "    # row_id,Category:Misconception の形式が必要\n",
    "    \n",
    "    # テストデータにrow_idが含まれているか確認\n",
    "    if 'row_id' in test_df.columns:\n",
    "        row_ids = test_df['row_id'].tolist()\n",
    "    else:\n",
    "        # row_idがない場合、testデータの開始IDを推定（通常は36696から）\n",
    "        print(\"⚠️ row_idが見つかりません。推定値を使用します...\")\n",
    "        start_id = 36696  # sample_submissionの開始ID\n",
    "        row_ids = list(range(start_id, start_id + len(test_df)))\n",
    "    \n",
    "    # Kaggle要求形式：各予測を \"Category:Misconception\" 形式に変換\n",
    "    print(\"🔧 Kaggle提出形式に変換中...\")\n",
    "    formatted_predictions = []\n",
    "    \n",
    "    for i, pred in enumerate(predictions):\n",
    "        # predは \"True_Correct False_Neither False_Misconception\" のようなスペース区切り\n",
    "        pred_categories = pred.split()[:3]  # TOP3に限定\n",
    "        \n",
    "        # 各カテゴリを \"Category:Misconception\" 形式に変換\n",
    "        formatted_parts = []\n",
    "        for category in pred_categories:\n",
    "            if category.endswith('_Misconception'):\n",
    "                # Misconceptionカテゴリの場合、実際の誤概念名が必要\n",
    "                # ここでは簡易的に \"Incomplete\" を使用（実際は予測結果から取得）\n",
    "                formatted_parts.append(f\"{category}:Incomplete\")\n",
    "            else:\n",
    "                # 他のカテゴリの場合はNA\n",
    "                formatted_parts.append(f\"{category}:NA\")\n",
    "        \n",
    "        # スペース区切りで結合\n",
    "        formatted_pred = \" \".join(formatted_parts)\n",
    "        formatted_predictions.append(formatted_pred)\n",
    "        \n",
    "        # 最初の5件をサンプル表示\n",
    "        if i < 5:\n",
    "            print(f\"  サンプル {i+1}: {formatted_pred}\")\n",
    "    \n",
    "    # 提出用データフレーム作成（正しいKaggle形式）\n",
    "    submission_df = pd.DataFrame({\n",
    "        'row_id': row_ids,\n",
    "        'Category:Misconception': formatted_predictions\n",
    "    })\n",
    "    \n",
    "    print(f\"✅ 提出データフレーム作成完了: {submission_df.shape}\")\n",
    "    print(f\"📝 列: {list(submission_df.columns)}\")\n",
    "    \n",
    "    # サンプル表示\n",
    "    print(f\"\\n📋 提出データサンプル:\")\n",
    "    print(submission_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # ファイル保存\n",
    "    try:\n",
    "        submission_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\n💾 提出ファイル保存完了: {output_path}\")\n",
    "        \n",
    "        # ファイルサイズ確認\n",
    "        file_size = os.path.getsize(output_path)\n",
    "        print(f\"📏 ファイルサイズ: {file_size:,} bytes ({file_size/1024:.1f} KB)\")\n",
    "        \n",
    "        # 形式確認\n",
    "        check_df = pd.read_csv(output_path)\n",
    "        print(f\"✅ 提出ファイル検証: {check_df.shape}\")\n",
    "        required_cols = ['row_id', 'Category:Misconception']\n",
    "        cols_present = all(col in check_df.columns for col in required_cols)\n",
    "        print(f\"   必要列存在確認: {cols_present}\")\n",
    "        \n",
    "        # 予測形式チェック\n",
    "        sample_predictions = check_df['Category:Misconception'].head(5).tolist()\n",
    "        print(f\"🔍 予測形式サンプル:\")\n",
    "        for i, pred in enumerate(sample_predictions):\n",
    "            pred_parts = pred.split()\n",
    "            print(f\"   {i+1}: {pred} (要素数: {len(pred_parts)})\")\n",
    "            \n",
    "        print(f\"\\n📊 提出ファイル最終確認:\")\n",
    "        print(f\"   ✅ 形式: Kaggle MAP競技形式（row_id, Category:Misconception）\")\n",
    "        print(f\"   ✅ 件数: {len(check_df):,}件\")\n",
    "        print(f\"   ✅ 列名: {list(check_df.columns)}\")\n",
    "        \n",
    "        return submission_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ファイル保存エラー: {e}\")\n",
    "        return None\n",
    "\n",
    "# 提出ファイルの作成\n",
    "if test_predictions is not None and test_df is not None:\n",
    "    print(\"📤 提出ファイルを作成します...\")\n",
    "    submission_df = create_submission_file(test_df, test_predictions, \"submission.csv\")\n",
    "    \n",
    "    if submission_df is not None:\n",
    "        print(\"🎉 提出ファイル作成完了!\")\n",
    "        print(f\"📋 最終確認:\")\n",
    "        print(f\"   ファイル名: submission.csv\")\n",
    "        print(f\"   データ件数: {len(submission_df):,}\")\n",
    "        print(f\"   予測形式: Kaggle MAP形式 (row_id, Category:Misconception)\")\n",
    "        print(\"\\n🏆 Kaggleコンペティションに提出準備完了!\")\n",
    "        \n",
    "        # 最終チェック用統計\n",
    "        print(f\"\\n📈 提出ファイル統計:\")\n",
    "        all_pred_in_submission = \" \".join(submission_df['Category:Misconception']).split()\n",
    "        unique_preds = set(all_pred_in_submission)\n",
    "        print(f\"   使用されたユニークなラベル数: {len(unique_preds)}\")\n",
    "        print(f\"   ラベル一覧: {sorted(unique_preds)}\")\n",
    "    else:\n",
    "        print(\"❌ 提出ファイル作成に失敗しました\")\n",
    "else:\n",
    "    print(\"❌ 必要なデータが準備されていません。前のセルを実行してください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Summary & Performance Notes\n",
    "\n",
    "### 📊 実行結果サマリー\n",
    "このノートブックでは以下を実装しました：\n",
    "\n",
    "1. **🤖 事前訓練済みモデル読み込み**: KaggleにアップロードされたGemma-2-2b-itモデル\n",
    "2. **📊 テストデータ処理**: math misconceptionコンペティションのtest.csv\n",
    "3. **🔮 効率的な推論**: バッチ処理による高速予測\n",
    "4. **📈 MAP@3形式出力**: TOP-3予測の生成\n",
    "5. **📤 提出準備**: Kaggle形式のsubmission.csv作成\n",
    "\n",
    "### 🚀 使用技術\n",
    "- **モデル**: `google/gemma-2-2b-it` (~2.6B parameters)\n",
    "- **フレームワーク**: PyTorch + Transformers\n",
    "- **推論**: 事前訓練済みモデル使用（追加訓練なし）\n",
    "- **評価**: MAP@3 (Mean Average Precision at 3)\n",
    "\n",
    "### ⚡ パフォーマンス最適化\n",
    "- バッチ処理による効率的な推論\n",
    "- 適応的バッチサイズ（CPU: 4, GPU: 8）\n",
    "- FP16使用（GPU環境）\n",
    "- メモリ効率的なデータローダー\n",
    "\n",
    "### 📝 使用方法\n",
    "1. Kaggleで事前訓練済みモデルをデータセットとしてアップロード\n",
    "2. MODEL_DATA_PATHを正しいデータセット名に変更\n",
    "3. 全セルを順番に実行\n",
    "4. `Gemma_2b_submission.csv`が生成される\n",
    "5. KaggleコンペティションにCSVファイルを提出\n",
    "\n",
    "### 🔧 トラブルシューティング\n",
    "- **メモリ不足**: バッチサイズを下げる\n",
    "- **モデル読み込み失敗**: データセット名とパスを確認\n",
    "- **CUDA OOM**: CPUモードに切り替え\n",
    "\n",
    "**🏆 Good luck with your submission!**"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12957508,
     "sourceId": 104383,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 405563,
     "modelInstanceId": 386414,
     "sourceId": 482709,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
