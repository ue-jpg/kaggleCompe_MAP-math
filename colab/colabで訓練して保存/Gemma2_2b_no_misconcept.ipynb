{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3c8bebaa",
      "metadata": {
        "id": "3c8bebaa"
      },
      "source": [
        "# ğŸ¤– Gemma-2-2b-it åˆ†é¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»ä¿å­˜ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n",
        "\n",
        "## æ¦‚è¦\n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€MAP - Charting Student Math Misunderstandingsã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ç”¨ã«ã€Gemma-2-2b-itãƒ¢ãƒ‡ãƒ«ã‚’6åˆ†é¡ã‚¿ã‚¹ã‚¯ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ä¿å­˜ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "### ãƒ¢ãƒ‡ãƒ«ä»•æ§˜\n",
        "- **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«**: google/gemma-2-2b-it (~2.6B parameters)\n",
        "- **ã‚¿ã‚¹ã‚¯**: 6ã‚¯ãƒ©ã‚¹åˆ†é¡ (True/False_Correct/Neither/Misconception)\n",
        "- **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: LoRA (Low-Rank Adaptation)\n",
        "- **GPU**: A100 æ¨å¥¨\n",
        "\n",
        "### ä¿å­˜å½¢å¼\n",
        "- Google Drive: `/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models/`\n",
        "- ãƒ­ãƒ¼ã‚«ãƒ«: zipãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "- å½¢å¼: Kaggleæå‡ºç”¨ã®AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa3b90a7",
      "metadata": {
        "id": "aa3b90a7"
      },
      "source": [
        "## ğŸ“¦ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1a792f8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a792f8e",
        "outputId": "9d8e0114-e907-4492-9605-3b7bc20be7e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… Google Drive ãƒã‚¦ãƒ³ãƒˆå®Œäº†\n",
            "âœ… å¿…è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆå®Œäº†\n"
          ]
        }
      ],
      "source": [
        "# Google Driveãƒã‚¦ãƒ³ãƒˆ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
        "import os\n",
        "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/map_data', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/kaggleCompe_MAP-math/logs', exist_ok=True)\n",
        "\n",
        "print(\"âœ… Google Drive ãƒã‚¦ãƒ³ãƒˆå®Œäº†\")\n",
        "print(\"âœ… å¿…è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆå®Œäº†\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9cd097a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cd097a2",
        "outputId": "ac43410a-ee34-457e-9606-1671e9cbc189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: read).\n",
            "The token `GemmaPractice` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `GemmaPractice`\n"
          ]
        }
      ],
      "source": [
        "# Hugging Faceèªè¨¼\n",
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "02751a84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02751a84",
        "outputId": "41983399-caf5-4676-f1f8-0788ebdb1b7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«/ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆå®Œäº†\n"
          ]
        }
      ],
      "source": [
        "# å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«/ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ\n",
        "!pip install peft accelerate transformers datasets -U -q\n",
        "print(\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«/ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆå®Œäº†\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb3726b2",
      "metadata": {
        "id": "eb3726b2"
      },
      "source": [
        "## ğŸ“š ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ãƒ¢ãƒ‡ãƒ«è¨­å®š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e44c31ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e44c31ad",
        "outputId": "cc39426f-4e7a-418b-f1fc-4523fe1fc3af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ–¥ï¸ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: cuda\n",
            "ğŸ® GPU: NVIDIA A100-SXM4-40GB\n",
            "ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒª: 42.5 GB\n",
            "âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿å®Œäº†\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import warnings\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
        "MODEL_NAME = \"google/gemma-2-2b-it\"\n",
        "NUM_LABELS = 6\n",
        "MAX_LENGTH = 512\n",
        "\n",
        "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ğŸ–¥ï¸ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "print(\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿å®Œäº†\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd662bc0",
      "metadata": {
        "id": "cd662bc0"
      },
      "source": [
        "## ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "779b0ce2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "779b0ce2",
        "outputId": "3f13d8e9-fc49-428b-f050-d855432be872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ï¼ˆ6åˆ†é¡å½¢å¼ï¼‰\n",
            "============================================================\n",
            "âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: (36696, 7)\n",
            "\n",
            "ğŸ“‹ Categoryåˆ†å¸ƒ:\n",
            "Category\n",
            "True_Correct           14802\n",
            "False_Misconception     9457\n",
            "False_Neither           6542\n",
            "True_Neither            5265\n",
            "True_Misconception       403\n",
            "False_Correct            227\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ğŸ§¹ NaNé™¤å»: 36696 -> 36696 (0è¡Œå‰Šé™¤)\n",
            "\n",
            "ğŸ”§ å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆä¸­...\n",
            "\n",
            "ğŸ“Š ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚«ãƒ†ã‚´ãƒªæ•°: 6\n",
            "ã‚«ãƒ†ã‚´ãƒªä¸€è¦§:\n",
            "  0: False_Correct (227ä»¶, 0.6%)\n",
            "  1: False_Misconception (9,457ä»¶, 25.8%)\n",
            "  2: False_Neither (6,542ä»¶, 17.8%)\n",
            "  3: True_Correct (14,802ä»¶, 40.3%)\n",
            "  4: True_Misconception (403ä»¶, 1.1%)\n",
            "  5: True_Neither (5,265ä»¶, 14.3%)\n",
            "\n",
            "ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ:\n",
            "  å¹³å‡: 222.1 æ–‡å­—\n",
            "  ä¸­å¤®å€¤: 213.0 æ–‡å­—\n",
            "  æœ€å¤§: 683 æ–‡å­—\n",
            "  512æ–‡å­—ä»¥ä¸‹: 36674 (99.9%)\n"
          ]
        }
      ],
      "source": [
        "def load_and_prepare_data():\n",
        "    \"\"\"ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ï¼ˆ6åˆ†é¡å½¢å¼ï¼‰\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ï¼ˆ6åˆ†é¡å½¢å¼ï¼‰\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ï¼ˆGoogle Driveå†…ï¼‰\n",
        "    train_data_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/map_data/train.csv\"\n",
        "\n",
        "    if not os.path.exists(train_data_path):\n",
        "        print(f\"âŒ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {train_data_path}\")\n",
        "        print(\"ğŸ’¡ Google Driveã®MyDrive/kaggleCompe_MAP-math/map_data/ã« train.csv ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
        "        train_df = pd.read_csv(train_data_path)\n",
        "        print(f\"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {train_df.shape}\")\n",
        "\n",
        "        # ã‚³ãƒ³ãƒšå½¢å¼ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç¢ºèªï¼ˆ6åˆ†é¡ï¼‰\n",
        "        print(\"\\nğŸ“‹ Categoryåˆ†å¸ƒ:\")\n",
        "        category_counts = train_df[\"Category\"].value_counts()\n",
        "        print(category_counts)\n",
        "\n",
        "        # NaNå€¤é™¤å»\n",
        "        before_len = len(train_df)\n",
        "        train_df = train_df.dropna(subset=[\"Category\", \"StudentExplanation\"])\n",
        "        after_len = len(train_df)\n",
        "        print(f\"\\nğŸ§¹ NaNé™¤å»: {before_len} -> {after_len} ({before_len - after_len}è¡Œå‰Šé™¤)\")\n",
        "\n",
        "        # å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆ\n",
        "        def create_enhanced_text(row):\n",
        "            \"\"\"Question + MC_Answer + Student Explanation ã‚’çµ±åˆ\"\"\"\n",
        "            question = str(row[\"QuestionText\"]) if pd.notna(row[\"QuestionText\"]) else \"\"\n",
        "            mc_answer = str(row[\"MC_Answer\"]) if pd.notna(row[\"MC_Answer\"]) else \"\"\n",
        "            explanation = str(row[\"StudentExplanation\"]) if pd.notna(row[\"StudentExplanation\"]) else \"\"\n",
        "\n",
        "            enhanced_text = f\"Question: {question} Selected Answer: {mc_answer} Explanation: {explanation}\"\n",
        "            return enhanced_text\n",
        "\n",
        "        print(\"\\nğŸ”§ å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆä¸­...\")\n",
        "        train_df[\"enhanced_text\"] = train_df.apply(create_enhanced_text, axis=1)\n",
        "\n",
        "        # 6ã¤ã®ã‚«ãƒ†ã‚´ãƒªã®ç¢ºèª\n",
        "        unique_categories = sorted(train_df[\"Category\"].unique())\n",
        "        print(f\"\\nğŸ“Š ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚«ãƒ†ã‚´ãƒªæ•°: {len(unique_categories)}\")\n",
        "        print(\"ã‚«ãƒ†ã‚´ãƒªä¸€è¦§:\")\n",
        "        for i, cat in enumerate(unique_categories):\n",
        "            count = (train_df[\"Category\"] == cat).sum()\n",
        "            percentage = count / len(train_df) * 100\n",
        "            print(f\"  {i}: {cat} ({count:,}ä»¶, {percentage:.1f}%)\")\n",
        "\n",
        "        # ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ\n",
        "        text_lengths = train_df[\"enhanced_text\"].str.len()\n",
        "        print(f\"\\nğŸ“ ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ:\")\n",
        "        print(f\"  å¹³å‡: {text_lengths.mean():.1f} æ–‡å­—\")\n",
        "        print(f\"  ä¸­å¤®å€¤: {text_lengths.median():.1f} æ–‡å­—\")\n",
        "        print(f\"  æœ€å¤§: {text_lengths.max()} æ–‡å­—\")\n",
        "        print(f\"  512æ–‡å­—ä»¥ä¸‹: {(text_lengths <= 512).sum()} ({(text_lengths <= 512).mean()*100:.1f}%)\")\n",
        "\n",
        "        return train_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¾ãŸã¯å‰å‡¦ç†ã«å¤±æ•—: {e}\")\n",
        "        return None\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Ÿè¡Œ\n",
        "train_df = load_and_prepare_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13f7e1ec",
      "metadata": {
        "id": "13f7e1ec"
      },
      "source": [
        "## ğŸ¤– Gemmaãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5a06b479",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364,
          "referenced_widgets": [
            "439cc99c22174d5a8bab68c7fac421ee",
            "eb00f8d1949b4c59a1f94ba53662e7a8",
            "7f15ca0ba10e45d1837db7a80cc37d3f",
            "87c2b27bb7b54da4a16c6176462b1099",
            "0ddb37b847924c27bc3005b6905a2b99",
            "a1128c2f9caf4793a6d3cbc17dd65df8",
            "3888b119eaa74038964fe282ebc685ce",
            "60f83dd266c04fb3a39121e924f8ff1e",
            "fa9f906209a842e28d297766efbc9611",
            "929a939f474e43589601889264c72773",
            "047a1251a1544e9f95757dcfe0e5d4f0"
          ]
        },
        "id": "5a06b479",
        "outputId": "793c6de0-072a-418d-84d6-0f11374caf0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ¤– Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: google/gemma-2-2b-it\n",
            "============================================================\n",
            "ğŸ“ Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...\n",
            "âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†\n",
            "ğŸ”– ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³: <pad>\n",
            "ğŸ“ èªå½™ã‚µã‚¤ã‚º: 256,000\n",
            "\n",
            "ğŸ§  Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "439cc99c22174d5a8bab68c7fac421ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at google/gemma-2-2b-it and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†!\n",
            "ğŸ“Š åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°: 6\n",
            "ğŸ“ˆ ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 2,614,355,712\n",
            "ğŸ¯ è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 2,614,355,712\n",
            "ğŸ’¡ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: ~2.61B parameters\n"
          ]
        }
      ],
      "source": [
        "def load_gemma_model(device):\n",
        "    \"\"\"Gemma-2-2b-itãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿ï¼ˆå…ƒã®å®šç¾©ã«æº–æ‹ ï¼‰\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"ğŸ¤– Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {MODEL_NAME}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        print(\"ğŸ“ Gemmaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®šï¼ˆGemmaã®å ´åˆå¿…è¦ï¼‰\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "            print(\"ğŸ”§ ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’EOSãƒˆãƒ¼ã‚¯ãƒ³ã«è¨­å®š\")\n",
        "\n",
        "        print(f\"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†\")\n",
        "        print(f\"ğŸ”– ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³: {tokenizer.pad_token}\")\n",
        "        print(f\"ğŸ“ èªå½™ã‚µã‚¤ã‚º: {tokenizer.vocab_size:,}\")\n",
        "\n",
        "        print(\"\\nğŸ§  Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "\n",
        "        # åˆ†é¡ã‚¿ã‚¹ã‚¯ç”¨ã®è¨­å®š\n",
        "        config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "        config.num_labels = NUM_LABELS\n",
        "        config.problem_type = \"single_label_classification\"\n",
        "\n",
        "        # Gemma-2-2b-itãƒ¢ãƒ‡ãƒ«ã‚’åˆ†é¡ç”¨ã«èª­ã¿è¾¼ã¿\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            config=config,\n",
        "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if device.type == \"cuda\" else None,\n",
        "        )\n",
        "\n",
        "        print(f\"âœ… Gemmaãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†!\")\n",
        "\n",
        "        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "        print(f\"ğŸ“Š åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°: {NUM_LABELS}\")\n",
        "        print(f\"ğŸ“ˆ ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}\")\n",
        "        print(f\"ğŸ¯ è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}\")\n",
        "        print(f\"ğŸ’¡ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: ~{total_params / 1e9:.2f}B parameters\")\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Ÿè¡Œ\n",
        "model, tokenizer = load_gemma_model(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d541e87",
      "metadata": {
        "id": "1d541e87"
      },
      "source": [
        "## ğŸ”§ LoRAè¨­å®šã¨PEFTé©ç”¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a48284a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a48284a3",
        "outputId": "e1b03041-7741-4be8-8220-db5c207de37e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ”§ LoRA (PEFT) è¨­å®šã¨é©ç”¨\n",
            "============================================================\n",
            "ğŸ“¦ PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒãƒ¼ã‚¸ãƒ§ãƒ³: 0.16.0\n",
            "ğŸ”„ PEFTé©ç”¨å‰ã«ãƒ¢ãƒ‡ãƒ«ã‚’float32ã«ã‚­ãƒ£ã‚¹ãƒˆ...\n",
            "âœ… ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿å‹: torch.float32\n",
            "ğŸ“‹ LoRAè¨­å®š:\n",
            "  r (rank): 16\n",
            "  lora_alpha: 32\n",
            "  target_modules: {'down_proj', 'gate_proj', 'o_proj', 'k_proj', 'up_proj', 'v_proj', 'q_proj'}\n",
            "  lora_dropout: 0.05\n",
            "  task_type: SEQ_CLS\n",
            "\n",
            "ğŸ”„ PEFTãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...\n",
            "âœ… PEFTãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†\n",
            "trainable params: 20,780,544 || all params: 2,635,136,256 || trainable%: 0.7886\n",
            "ğŸ“ PEFTãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•: cuda\n",
            "âœ… ãƒ¢ãƒ‡ãƒ«ãŒãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•ã—ã¾ã—ãŸ: cuda:0\n"
          ]
        }
      ],
      "source": [
        "def apply_lora_to_model(model):\n",
        "    \"\"\"LoRAè¨­å®šã¨PEFTé©ç”¨\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ğŸ”§ LoRA (PEFT) è¨­å®šã¨é©ç”¨\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if model is None:\n",
        "        print(\"âŒ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª\n",
        "        import peft\n",
        "        print(f\"ğŸ“¦ PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒãƒ¼ã‚¸ãƒ§ãƒ³: {peft.__version__}\")\n",
        "\n",
        "        # PEFTé©ç”¨å‰ã«ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’æ˜ç¤ºçš„ã«float32ã«ã™ã‚‹\n",
        "        print(\"ğŸ”„ PEFTé©ç”¨å‰ã«ãƒ¢ãƒ‡ãƒ«ã‚’float32ã«ã‚­ãƒ£ã‚¹ãƒˆ...\")\n",
        "        model = model.to(torch.float32)\n",
        "        print(f\"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿å‹: {next(model.parameters()).dtype}\")\n",
        "\n",
        "\n",
        "        # LoRAè¨­å®š\n",
        "        lora_config = LoraConfig(\n",
        "            r=16,                    # LoRA attention dimension\n",
        "            lora_alpha=32,          # Alpha parameter for LoRA scaling\n",
        "            target_modules=[        # Gemma-2ã®ä¸»è¦ãªç·šå½¢å±¤ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ\n",
        "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "            ],\n",
        "            lora_dropout=0.05,      # Dropout probability for LoRA layers\n",
        "            bias=\"none\",            # Bias type\n",
        "            task_type=\"SEQ_CLS\",    # Sequence Classification task\n",
        "            modules_to_save=[\"classifier\", \"score\"],  # åˆ†é¡ãƒ˜ãƒƒãƒ‰ã‚’ä¿å­˜\n",
        "        )\n",
        "\n",
        "        print(\"ğŸ“‹ LoRAè¨­å®š:\")\n",
        "        print(f\"  r (rank): {lora_config.r}\")\n",
        "        print(f\"  lora_alpha: {lora_config.lora_alpha}\")\n",
        "        print(f\"  target_modules: {lora_config.target_modules}\")\n",
        "        print(f\"  lora_dropout: {lora_config.lora_dropout}\")\n",
        "        print(f\"  task_type: {lora_config.task_type}\")\n",
        "\n",
        "        # PEFTãƒ¢ãƒ‡ãƒ«ä½œæˆ\n",
        "        print(\"\\nğŸ”„ PEFTãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...\")\n",
        "        peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "        print(\"âœ… PEFTãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†\")\n",
        "        peft_model.print_trainable_parameters()\n",
        "\n",
        "        # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•\n",
        "        # PEFTé©ç”¨å¾Œã€ãƒ¢ãƒ‡ãƒ«ã‚’å†åº¦ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹• (get_peft_modelãŒCPUã«æˆ»ã™å ´åˆãŒã‚ã‚‹ãŸã‚)\n",
        "        print(f\"ğŸ“ PEFTãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•: {device}\")\n",
        "        peft_model.to(device)\n",
        "        print(f\"âœ… ãƒ¢ãƒ‡ãƒ«ãŒãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•ã—ã¾ã—ãŸ: {next(peft_model.parameters()).device}\")\n",
        "\n",
        "\n",
        "        return peft_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ LoRAé©ç”¨å¤±æ•—: {e}\")\n",
        "        return None\n",
        "\n",
        "# LoRAé©ç”¨\n",
        "if model is not None:\n",
        "    peft_model = apply_lora_to_model(model)\n",
        "else:\n",
        "    print(\"âŒ ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€LoRAã‚’é©ç”¨ã§ãã¾ã›ã‚“\")\n",
        "    peft_model = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a7961c3",
      "metadata": {
        "id": "1a7961c3"
      },
      "source": [
        "## ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹å®šç¾©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "925f3e45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "925f3e45",
        "outputId": "99a4e362-123c-4f0e-be61-4fa97a80cded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… MathMisconceptionDataset ã‚¯ãƒ©ã‚¹å®šç¾©å®Œäº†\n"
          ]
        }
      ],
      "source": [
        "class MathMisconceptionDataset(Dataset):\n",
        "    \"\"\"Math Misconception Dataset for PyTorch\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "print(\"âœ… MathMisconceptionDataset ã‚¯ãƒ©ã‚¹å®šç¾©å®Œäº†\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e23a8ee4",
      "metadata": {
        "id": "e23a8ee4"
      },
      "source": [
        "## ğŸ“Š è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "236d6b03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "236d6b03",
        "outputId": "7f33fa64-71fa-4fc6-a293-565be8a04fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… MAP@3è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©å®Œäº†\n"
          ]
        }
      ],
      "source": [
        "def compute_map3_metrics(eval_pred):\n",
        "    \"\"\"MAP@3ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = torch.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
        "\n",
        "    map_scores = []\n",
        "    for i, true_label in enumerate(labels):\n",
        "        top3_indices = np.argsort(predictions[i])[::-1][:3]\n",
        "        score = 0.0\n",
        "        for j, pred_idx in enumerate(top3_indices):\n",
        "            if pred_idx == true_label:\n",
        "                score = 1.0 / (j + 1)\n",
        "                break\n",
        "        map_scores.append(score)\n",
        "\n",
        "    map3_score = np.mean(map_scores)\n",
        "    accuracy = accuracy_score(labels, np.argmax(predictions, axis=1))\n",
        "\n",
        "    return {\"map3\": map3_score, \"accuracy\": accuracy}\n",
        "\n",
        "print(\"âœ… MAP@3è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©å®Œäº†\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6363d6b",
      "metadata": {
        "id": "d6363d6b"
      },
      "source": [
        "## ğŸš€ ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Ÿè¡Œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b027d18f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b027d18f",
        "outputId": "03802afa-37ee-4bf5-c61c-be1f683c6e09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸš€ 6åˆ†é¡ãƒ¢ãƒ‡ãƒ« ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹\n",
            "=============================================================\n",
            "ğŸ“‹ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«:\n",
            "  0: False_Correct (227ä»¶)\n",
            "  1: False_Misconception (9,457ä»¶)\n",
            "  2: False_Neither (6,542ä»¶)\n",
            "  3: True_Correct (14,802ä»¶)\n",
            "  4: True_Misconception (403ä»¶)\n",
            "  5: True_Neither (5,265ä»¶)\n",
            "âœ… Stratified splité©ç”¨\n",
            "ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 29,356ä»¶\n",
            "ğŸ“Š æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 7,340ä»¶\n",
            "\n",
            "ğŸ“‹ è¨“ç·´è¨­å®š:\n",
            "  ã‚¨ãƒãƒƒã‚¯æ•°: 3\n",
            "  per_device_train_batch_size: 4\n",
            "  gradient_accumulation_steps: 8\n",
            "  å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: 32\n",
            "  å­¦ç¿’ç‡: 2e-05\n",
            "  ä¿å­˜å…ˆ: /content/drive/MyDrive/kaggleCompe_MAP-math/trained_models/gemma-2-2b-math-classification\n",
            "  bf16: True\n",
            "\n",
            "ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2754' max='2754' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2754/2754 2:34:26, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Map3</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.752000</td>\n",
              "      <td>0.559650</td>\n",
              "      <td>0.878156</td>\n",
              "      <td>0.767030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.453000</td>\n",
              "      <td>0.461666</td>\n",
              "      <td>0.901317</td>\n",
              "      <td>0.809264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.383600</td>\n",
              "      <td>0.391792</td>\n",
              "      <td>0.919482</td>\n",
              "      <td>0.845368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.322600</td>\n",
              "      <td>0.354655</td>\n",
              "      <td>0.928588</td>\n",
              "      <td>0.861172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.296100</td>\n",
              "      <td>0.345375</td>\n",
              "      <td>0.930313</td>\n",
              "      <td>0.864986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.277000</td>\n",
              "      <td>0.330186</td>\n",
              "      <td>0.933356</td>\n",
              "      <td>0.870845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.197100</td>\n",
              "      <td>0.356158</td>\n",
              "      <td>0.934718</td>\n",
              "      <td>0.873569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.175100</td>\n",
              "      <td>0.376239</td>\n",
              "      <td>0.935059</td>\n",
              "      <td>0.874251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.357493</td>\n",
              "      <td>0.935831</td>\n",
              "      <td>0.875749</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†\n",
            "\n",
            "ğŸ“Š æœ€çµ‚è©•ä¾¡:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1835' max='1835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1835/1835 03:43]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  eval_loss: 0.3575\n",
            "  eval_map3: 0.9358\n",
            "  eval_accuracy: 0.8757\n",
            "  eval_runtime: 223.7793\n",
            "  eval_samples_per_second: 32.8000\n",
            "  eval_steps_per_second: 8.2000\n",
            "  epoch: 3.0000\n"
          ]
        }
      ],
      "source": [
        "def train_model(train_df, peft_model, tokenizer, device):\n",
        "    \"\"\"ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ğŸš€ 6åˆ†é¡ãƒ¢ãƒ‡ãƒ« ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹\")\n",
        "    print(\"=\" + \"=\" * 60)\n",
        "\n",
        "    if train_df is None or peft_model is None:\n",
        "        print(\"âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
        "        return None, None\n",
        "\n",
        "    # ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
        "    label_encoder = LabelEncoder()\n",
        "    train_df[\"encoded_labels\"] = label_encoder.fit_transform(train_df[\"Category\"])\n",
        "\n",
        "    print(f\"ğŸ“‹ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«:\")\n",
        "    for i, label in enumerate(label_encoder.classes_):\n",
        "        count = (train_df[\"encoded_labels\"] == i).sum()\n",
        "        print(f\"  {i}: {label} ({count:,}ä»¶)\")\n",
        "\n",
        "    # è¨“ç·´/æ¤œè¨¼åˆ†å‰²\n",
        "    train_texts = train_df[\"enhanced_text\"].tolist()\n",
        "    train_labels = train_df[\"encoded_labels\"].tolist()\n",
        "\n",
        "    try:\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            train_texts,\n",
        "            train_labels,\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=train_labels,\n",
        "        )\n",
        "        print(\"âœ… Stratified splité©ç”¨\")\n",
        "    except ValueError:\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            train_texts, train_labels, test_size=0.2, random_state=42\n",
        "        )\n",
        "        print(\"âœ… Regular splité©ç”¨ï¼ˆå°‘æ•°ã‚¯ãƒ©ã‚¹ã®ãŸã‚ï¼‰\")\n",
        "\n",
        "    print(f\"ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train):,}ä»¶\")\n",
        "    print(f\"ğŸ“Š æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val):,}ä»¶\")\n",
        "\n",
        "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
        "    train_dataset = MathMisconceptionDataset(\n",
        "        X_train, y_train, tokenizer, max_length=MAX_LENGTH\n",
        "    )\n",
        "    val_dataset = MathMisconceptionDataset(\n",
        "        X_val, y_val, tokenizer, max_length=MAX_LENGTH\n",
        "    )\n",
        "\n",
        "    # è¨“ç·´è¨­å®š - ãŠã™ã™ã‚è¨­å®šã‚’é©ç”¨\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models/gemma-2-2b-math-classification\",\n",
        "        num_train_epochs=3, # ãŠã™ã™ã‚è¨­å®š: 3ã‚¨ãƒãƒƒã‚¯\n",
        "        per_device_train_batch_size=4,  # ãŠã™ã™ã‚è¨­å®š: ãƒãƒƒãƒã‚µã‚¤ã‚º 4\n",
        "        per_device_eval_batch_size=4,\n",
        "        gradient_accumulation_steps=8,  # ãŠã™ã™ã‚è¨­å®š: å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º 32\n",
        "        warmup_steps=200,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"/content/drive/MyDrive/kaggleCompe_MAP-math/logs\",\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=300,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=300,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"map3\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\",\n",
        "        dataloader_pin_memory=False,\n",
        "        fp16=False,\n",
        "        bf16=True, # ãŠã™ã™ã‚è¨­å®š: bf16ã‚’æœ‰åŠ¹åŒ–\n",
        "        optim=\"adamw_torch\",\n",
        "        learning_rate=2e-5,\n",
        "        save_total_limit=2,\n",
        "        gradient_checkpointing=False,\n",
        "    )\n",
        "\n",
        "    print(\"\\nğŸ“‹ è¨“ç·´è¨­å®š:\")\n",
        "    print(f\"  ã‚¨ãƒãƒƒã‚¯æ•°: {training_args.num_train_epochs}\")\n",
        "    print(f\"  per_device_train_batch_size: {training_args.per_device_train_batch_size}\")\n",
        "    print(f\"  gradient_accumulation_steps: {training_args.gradient_accumulation_steps}\")\n",
        "    print(f\"  å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "    print(f\"  å­¦ç¿’ç‡: {training_args.learning_rate}\")\n",
        "    print(f\"  ä¿å­˜å…ˆ: {training_args.output_dir}\")\n",
        "    print(f\"  bf16: {training_args.bf16}\")\n",
        "\n",
        "\n",
        "    # Trainerä½œæˆ\n",
        "    trainer = Trainer(\n",
        "        model=peft_model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "        compute_metrics=compute_map3_metrics,\n",
        "    )\n",
        "\n",
        "    # è¨“ç·´å®Ÿè¡Œ\n",
        "    print(\"\\nğŸš€ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...\")\n",
        "    try:\n",
        "        trainer.train()\n",
        "        print(\"âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†\")\n",
        "\n",
        "        # æœ€çµ‚è©•ä¾¡\n",
        "        print(\"\\nğŸ“Š æœ€çµ‚è©•ä¾¡:\")\n",
        "        eval_results = trainer.evaluate()\n",
        "        for key, value in eval_results.items():\n",
        "            print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "        return trainer, label_encoder\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# è¨“ç·´å®Ÿè¡Œ\n",
        "if train_df is not None and peft_model is not None:\n",
        "    trainer, label_encoder = train_model(train_df, peft_model, tokenizer, device)\n",
        "else:\n",
        "    print(\"âŒ è¨“ç·´ã«å¿…è¦ãªè¦ç´ ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
        "    trainer, label_encoder = None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42ff391c",
      "metadata": {
        "id": "42ff391c"
      },
      "source": [
        "## ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "35b7c8b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35b7c8b7",
        "outputId": "aace1513-f6df-4cbc-a62d-ffc266e288b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ’¾ è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
            "============================================================\n",
            "ğŸ“ ä¿å­˜å…ˆ: /content/drive/MyDrive/kaggleCompe_MAP-math/trained_models/gemma-2-2b-math-classification-final\n",
            "âœ… ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿å­˜å®Œäº†\n",
            "âœ… ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ä¿å­˜å®Œäº†\n",
            "ğŸ“‹ ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°: {0: 'False_Correct', 1: 'False_Misconception', 2: 'False_Neither', 3: 'True_Correct', 4: 'True_Misconception', 5: 'True_Neither'}\n",
            "âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†\n",
            "\n",
            "ğŸ“‚ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\n",
            "  ğŸ“„ README.md: 0.00 MB\n",
            "  ğŸ“„ adapter_model.safetensors: 79.32 MB\n",
            "  ğŸ“„ adapter_config.json: 0.00 MB\n",
            "  ğŸ“„ chat_template.jinja: 0.00 MB\n",
            "  ğŸ“„ tokenizer_config.json: 0.04 MB\n",
            "  ğŸ“„ special_tokens_map.json: 0.00 MB\n",
            "  ğŸ“„ tokenizer.model: 4.04 MB\n",
            "  ğŸ“„ tokenizer.json: 32.77 MB\n",
            "  ğŸ“„ training_args.bin: 0.01 MB\n",
            "  ğŸ“„ label_mapping.json: 0.00 MB\n",
            "  ğŸ“„ model_metadata.json: 0.00 MB\n",
            "\n",
            "ğŸ‰ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†!\n",
            "ğŸ“ Google Driveä¿å­˜ãƒ‘ã‚¹: /content/drive/MyDrive/kaggleCompe_MAP-math/trained_models/\n",
            "ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: Kaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æº–å‚™å®Œäº†\n"
          ]
        }
      ],
      "source": [
        "def save_trained_model(trainer, tokenizer, label_encoder):\n",
        "    \"\"\"è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ï¼ˆKaggleæå‡ºç”¨å½¢å¼ï¼‰\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ğŸ’¾ è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if trainer is None or tokenizer is None or label_encoder is None:\n",
        "        print(\"âŒ ä¿å­˜ã«å¿…è¦ãªè¦ç´ ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
        "        return False\n",
        "\n",
        "    save_base_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models\"\n",
        "    model_save_path = f\"{save_base_path}/gemma-2-2b-math-classification-final\"\n",
        "\n",
        "    try:\n",
        "        # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜\n",
        "        print(f\"ğŸ“ ä¿å­˜å…ˆ: {model_save_path}\")\n",
        "\n",
        "        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ï¼ˆPEFTãƒ¢ãƒ‡ãƒ«è¾¼ã¿ï¼‰\n",
        "        trainer.save_model(model_save_path)\n",
        "        tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "        print(\"âœ… ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿å­˜å®Œäº†\")\n",
        "\n",
        "        # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã®ä¿å­˜\n",
        "        label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
        "        label_file = f\"{model_save_path}/label_mapping.json\"\n",
        "\n",
        "        with open(label_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(label_mapping, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(\"âœ… ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ä¿å­˜å®Œäº†\")\n",
        "        print(f\"ğŸ“‹ ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°: {label_mapping}\")\n",
        "\n",
        "        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜\n",
        "        metadata = {\n",
        "            \"model_name\": MODEL_NAME,\n",
        "            \"num_labels\": NUM_LABELS,\n",
        "            \"max_length\": MAX_LENGTH,\n",
        "            \"task_type\": \"sequence_classification\",\n",
        "            \"architecture\": \"AutoModelForSequenceClassification\",\n",
        "            \"peft_applied\": True,\n",
        "            \"lora_config\": {\n",
        "                \"r\": 16,\n",
        "                \"lora_alpha\": 32,\n",
        "                \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                \"task_type\": \"SEQ_CLS\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        metadata_file = f\"{model_save_path}/model_metadata.json\"\n",
        "        with open(metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(\"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†\")\n",
        "\n",
        "        # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
        "        print(f\"\\nğŸ“‚ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
        "        for item in os.listdir(model_save_path):\n",
        "            item_path = os.path.join(model_save_path, item)\n",
        "            if os.path.isfile(item_path):\n",
        "                size_mb = os.path.getsize(item_path) / (1024 * 1024)\n",
        "                print(f\"  ğŸ“„ {item}: {size_mb:.2f} MB\")\n",
        "            else:\n",
        "                print(f\"  ğŸ“ {item}/\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "        return False\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Ÿè¡Œ\n",
        "if trainer is not None and tokenizer is not None and label_encoder is not None:\n",
        "    save_success = save_trained_model(trainer, tokenizer, label_encoder)\n",
        "\n",
        "    if save_success:\n",
        "        print(\"\\nğŸ‰ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†!\")\n",
        "        print(\"ğŸ“ Google Driveä¿å­˜ãƒ‘ã‚¹: /content/drive/MyDrive/kaggleCompe_MAP-math/trained_models/\")\n",
        "        print(\"ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: Kaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æº–å‚™å®Œäº†\")\n",
        "    else:\n",
        "        print(\"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
        "else:\n",
        "    print(\"âŒ ä¿å­˜ã«å¿…è¦ãªè¦ç´ ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94d91f89",
      "metadata": {
        "id": "94d91f89"
      },
      "source": [
        "## ğŸ“¦ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨zipä½œæˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "33da6312",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33da6312",
        "outputId": "227c652e-94f9-4edf-fadc-4d2bff60fb9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ“¦ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨zipä½œæˆ\n",
            "============================================================\n",
            "ğŸ“ ã‚½ãƒ¼ã‚¹: /content/drive/MyDrive/kaggleCompe_MAP-math/trained_models/gemma-2-2b-math-classification-final\n",
            "ğŸ“¦ å‡ºåŠ›zip: /content/drive/MyDrive/kaggleCompe_MAP-math/gemma-2-2b-math-model.zip\n",
            "  â• README.md\n",
            "  â• adapter_model.safetensors\n",
            "  â• adapter_config.json\n",
            "  â• chat_template.jinja\n",
            "  â• tokenizer_config.json\n",
            "  â• special_tokens_map.json\n",
            "  â• tokenizer.model\n",
            "  â• tokenizer.json\n",
            "  â• training_args.bin\n",
            "  â• label_mapping.json\n",
            "  â• model_metadata.json\n",
            "\n",
            "âœ… zipä½œæˆå®Œäº†!\n",
            "ğŸ“ zipã‚µã‚¤ã‚º: 80.58 MB\n",
            "ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‘ã‚¹: /content/drive/MyDrive/kaggleCompe_MAP-math/gemma-2-2b-math-model.zip\n",
            "\n",
            "ğŸ’¡ Colabã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯:\n",
            "```python\n",
            "from google.colab import files\n",
            "files.download('/content/drive/MyDrive/kaggleCompe_MAP-math/gemma-2-2b-math-model.zip')\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "def create_download_zip():\n",
        "    \"\"\"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã®zipãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ğŸ“¦ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨zipä½œæˆ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        import zipfile\n",
        "\n",
        "        model_dir = \"/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models/gemma-2-2b-math-classification-final\"\n",
        "        zip_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/gemma-2-2b-math-model.zip\"\n",
        "\n",
        "        if not os.path.exists(model_dir):\n",
        "            print(f\"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_dir}\")\n",
        "            return False\n",
        "\n",
        "        print(f\"ğŸ“ ã‚½ãƒ¼ã‚¹: {model_dir}\")\n",
        "        print(f\"ğŸ“¦ å‡ºåŠ›zip: {zip_path}\")\n",
        "\n",
        "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for root, dirs, files in os.walk(model_dir):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    arcname = os.path.relpath(file_path, model_dir)\n",
        "                    zipf.write(file_path, arcname)\n",
        "                    print(f\"  â• {arcname}\")\n",
        "\n",
        "        # zipãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª\n",
        "        zip_size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
        "        print(f\"\\nâœ… zipä½œæˆå®Œäº†!\")\n",
        "        print(f\"ğŸ“ zipã‚µã‚¤ã‚º: {zip_size_mb:.2f} MB\")\n",
        "        print(f\"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‘ã‚¹: {zip_path}\")\n",
        "\n",
        "        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã‚³ãƒ¼ãƒ‰è¡¨ç¤º\n",
        "        print(f\"\\nğŸ’¡ Colabã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯:\")\n",
        "        print(f\"```python\")\n",
        "        print(f\"from google.colab import files\")\n",
        "        print(f\"files.download('{zip_path}')\")\n",
        "        print(f\"```\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ zipä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "        return False\n",
        "\n",
        "# zipä½œæˆå®Ÿè¡Œ\n",
        "zip_success = create_download_zip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34461dde",
      "metadata": {
        "id": "34461dde"
      },
      "source": [
        "## ğŸ¯ å®Œäº†ã‚µãƒãƒªãƒ¼\n",
        "\n",
        "### âœ… å®Œäº†ã‚¿ã‚¹ã‚¯\n",
        "1. **ğŸ¤– ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿**: Gemma-2-2b-it (AutoModelForSequenceClassification)\n",
        "2. **ğŸ”§ LoRAé©ç”¨**: åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
        "3. **ğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™**: 6åˆ†é¡ã‚¿ã‚¹ã‚¯ç”¨å‰å‡¦ç†\n",
        "4. **ğŸš€ è¨“ç·´å®Ÿè¡Œ**: MAP@3æœ€é©åŒ–\n",
        "5. **ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜**: Google Drive + zipå½¢å¼\n",
        "\n",
        "### ğŸ“ ä¿å­˜å ´æ‰€\n",
        "- **Google Drive**: `/content/drive/MyDrive/kaggleCompe_MAP-math/trained_models/`\n",
        "- **zipå½¢å¼**: `/content/drive/MyDrive/kaggleCompe_MAP-math/gemma-2-2b-math-model.zip`\n",
        "\n",
        "### ğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
        "1. **Kaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ**: ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’Kaggleã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
        "2. **æ¨è«–ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯**: test_saved2-2b.ipynbã§èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ\n",
        "3. **æå‡º**: submission.csvã®ç”Ÿæˆ\n",
        "\n",
        "### ğŸ¯ Kaggleæå‡ºç”¨èª­ã¿è¾¼ã¿å½¢å¼\n",
        "```python\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "```\n",
        "\n",
        "**ğŸ‰ Gemma-2-2b-itåˆ†é¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "dfa58dba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "dfa58dba",
        "outputId": "25187bb4-0d4f-42c9-d289-90564818e203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ‰ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ!\n",
            "\n",
            "ğŸ“¥ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œ:\n",
            "```python\n",
            "from google.colab import files\n",
            "files.download('/content/drive/MyDrive/kaggleCompe_MAP-math/gemma-2-2b-math-model.zip')\n",
            "```\n",
            "ä»Šã™ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã‹ï¼Ÿ (y/n): y\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fd58903e-e3a4-4cc5-8d3f-ec350537f48f\", \"gemma-2-2b-math-model.zip\", 84494819)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹!\n"
          ]
        }
      ],
      "source": [
        "# æœ€çµ‚å®Ÿè¡Œç¢ºèªã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "if zip_success:\n",
        "    print(\"ğŸ‰ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ!\")\n",
        "    print(\"\\nğŸ“¥ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œ:\")\n",
        "    print(\"```python\")\n",
        "    print(\"from google.colab import files\")\n",
        "    print(\"files.download('/content/drive/MyDrive/kaggleCompe_MAP-math/gemma-2-2b-math-model.zip')\")\n",
        "    print(\"```\")\n",
        "\n",
        "    # è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        response = input(\"ä»Šã™ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã‹ï¼Ÿ (y/n): \")\n",
        "        if response.lower() == 'y':\n",
        "            files.download('/content/drive/MyDrive/kaggleCompe_MAP-math/gemma-2-2b-math-model.zip')\n",
        "            print(\"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹!\")\n",
        "    except:\n",
        "        print(\"ğŸ’¡ æ‰‹å‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n",
        "else:\n",
        "    print(\"âŒ ä¸€éƒ¨ã®å‡¦ç†ãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“ã€‚ä¸Šè¨˜ã®ã‚»ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "è¨“ç·´ãŒã§ããªã„ã‚¨ãƒ©ãƒ¼ã®èª¿æŸ»ç”¨ã ã‹ã‚‰çµ‚ã‚ã£ãŸã‚‰ã„ã‚‰ãªã„ã‚³ãƒ¼ãƒ‰"
      ],
      "metadata": {
        "id": "sFoWtFm8Ucqc"
      },
      "id": "sFoWtFm8Ucqc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e508c927",
        "outputId": "f17927f6-dd3c-4945-a727-c99c6edb949e"
      },
      "source": [
        "# PEFTé©ç”¨å¾Œã®ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã¨trainableãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¢ºèª\n",
        "if peft_model is not None:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ğŸ§  PEFTé©ç”¨å¾Œã®ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã¨Trainableãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # åˆ†é¡ãƒ˜ãƒƒãƒ‰ã¨æ€ã‚ã‚Œã‚‹å±¤ã®åå‰ã‚’ç‰¹å®š\n",
        "    # Gemma2ForSequenceClassificationã®æ§‹é€ ã‚’ç¢ºèªã—ã€åˆ†é¡ãƒ˜ãƒƒãƒ‰ã‚’æ¢ã™\n",
        "    classifier_names = [\"score\", \"classifier\", \"lm_head\"] # ã‚ˆãä½¿ã‚ã‚Œã‚‹åå‰å€™è£œ\n",
        "\n",
        "    found_classifier = False\n",
        "    for name, module in peft_model.named_modules():\n",
        "        is_target_classifier = False\n",
        "        for cls_name in classifier_names:\n",
        "            if cls_name in name:\n",
        "                is_target_classifier = True\n",
        "                break\n",
        "\n",
        "        if \"lora\" in name:\n",
        "             # LoRAå±¤ã¯trainableã«ãªã£ã¦ã„ã‚‹ã¯ãš\n",
        "             print(f\"âœ… LoRA Layer: {name} (requires_grad: {module.weight.requires_grad if hasattr(module, 'weight') else False})\")\n",
        "        elif is_target_classifier and isinstance(module, (torch.nn.Linear, torch.nn.ModuleDict, torch.nn.Sequential)):\n",
        "             # åˆ†é¡ãƒ˜ãƒƒãƒ‰ã¨æ€ã‚ã‚Œã‚‹å±¤ã®æƒ…å ±\n",
        "             print(f\"\\nğŸ¯ Classifier/Score Layer: {name}\")\n",
        "             print(f\"  Type: {type(module)}\")\n",
        "             # å†å¸°çš„ã«ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®requires_gradã‚’ç¢ºèª\n",
        "             all_classifier_params_trainable = True\n",
        "             classifier_params_count = 0\n",
        "             for param_name, param in module.named_parameters():\n",
        "                 print(f\"    - {name}.{param_name} (requires_grad: {param.requires_grad})\")\n",
        "                 if not param.requires_grad:\n",
        "                     all_classifier_params_trainable = False\n",
        "                 classifier_params_count += param.numel()\n",
        "             print(f\"  Total parameters in this module: {classifier_params_count:,}\")\n",
        "             print(f\"  All parameters trainable in this module: {all_classifier_params_trainable}\")\n",
        "             found_classifier = True\n",
        "        elif any(target_m in name for target_m in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]):\n",
        "             # LoRAãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ç·šå½¢å±¤ï¼ˆè‡ªèº«ã¯å‡çµã•ã‚Œã¦ã„ã‚‹ã¯ãšï¼‰\n",
        "             pass # è©³ç´°ãªå‡ºåŠ›ã¯çœç•¥\n",
        "        else:\n",
        "             # ãã®ä»–ã®å±¤ï¼ˆé€šå¸¸ã¯å‡çµï¼‰\n",
        "             pass # è©³ç´°ãªå‡ºåŠ›ã¯çœç•¥\n",
        "\n",
        "    if not found_classifier:\n",
        "        print(\"\\nâš ï¸ è­¦å‘Š: åˆ†é¡ãƒ˜ãƒƒãƒ‰ ('score' or 'classifier') ãŒãƒ¢ãƒ‡ãƒ«æ§‹é€ å†…ã§è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "        print(\"PEFTã®è¨­å®š 'modules_to_save' ã®åå‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "    # PEFTãƒ¢ãƒ‡ãƒ«å…¨ä½“ã® trainable ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å†ç¢ºèª (å¿µã®ãŸã‚)\n",
        "    total_trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
        "    print(f\"\\nğŸ“ˆ PEFTãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_trainable_params:,}\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ PEFTãƒ¢ãƒ‡ãƒ«ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\")"
      ],
      "id": "e508c927",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ§  PEFTé©ç”¨å¾Œã®ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã¨Trainableãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:\n",
            "============================================================\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.q_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.q_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.q_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.q_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.q_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.q_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.k_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.k_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.k_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.k_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.k_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.k_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.v_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.v_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.v_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.v_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.v_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.v_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.o_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.o_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.o_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.o_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.o_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.o_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.gate_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.gate_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.gate_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.gate_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.gate_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.gate_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.gate_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.gate_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.gate_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.up_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.up_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.up_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.up_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.up_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.up_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.up_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.up_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.up_proj.lora_magnitude_vector (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.down_proj.lora_dropout (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.down_proj.lora_dropout.default (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.down_proj.lora_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.down_proj.lora_A.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.down_proj.lora_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.down_proj.lora_B.default (requires_grad: True)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.down_proj.lora_embedding_A (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.down_proj.lora_embedding_B (requires_grad: False)\n",
            "âœ… LoRA Layer: base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector (requires_grad: False)\n",
            "\n",
            "ğŸ¯ Classifier/Score Layer: base_model.model.score.original_module\n",
            "  Type: <class 'torch.nn.modules.linear.Linear'>\n",
            "    - base_model.model.score.original_module.weight (requires_grad: False)\n",
            "  Total parameters in this module: 13,824\n",
            "  All parameters trainable in this module: False\n",
            "\n",
            "ğŸ¯ Classifier/Score Layer: base_model.model.score.modules_to_save\n",
            "  Type: <class 'torch.nn.modules.container.ModuleDict'>\n",
            "    - base_model.model.score.modules_to_save.default.weight (requires_grad: True)\n",
            "  Total parameters in this module: 13,824\n",
            "  All parameters trainable in this module: True\n",
            "\n",
            "ğŸ¯ Classifier/Score Layer: base_model.model.score.modules_to_save.default\n",
            "  Type: <class 'torch.nn.modules.linear.Linear'>\n",
            "    - base_model.model.score.modules_to_save.default.weight (requires_grad: True)\n",
            "  Total parameters in this module: 13,824\n",
            "  All parameters trainable in this module: True\n",
            "\n",
            "ğŸ“ˆ PEFTãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 20,780,544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1186e3ba",
        "outputId": "b9f23a84-ed0d-495d-978b-399ee662c1f3"
      },
      "source": [
        "# PEFTé©ç”¨å¾Œã®ãƒ¢ãƒ‡ãƒ«é‡ã¿ã®ç¢ºèª\n",
        "if peft_model is not None:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ğŸ§  PEFTé©ç”¨å¾Œã®ãƒ¢ãƒ‡ãƒ«é‡ã¿ç¢ºèª:\")\n",
        "    print(\"=\" + \"=\" * 60)\n",
        "\n",
        "    # åˆ†é¡ãƒ˜ãƒƒãƒ‰ (score) ã®é‡ã¿ã‚’ç¢ºèª\n",
        "    score_layer = None\n",
        "    for name, module in peft_model.named_modules():\n",
        "        if name == \"score\" and isinstance(module, torch.nn.Linear):\n",
        "            score_layer = module\n",
        "            break\n",
        "\n",
        "    if score_layer:\n",
        "        print(f\"\\nğŸ¯ åˆ†é¡ãƒ˜ãƒƒãƒ‰ ('score') ã®é‡ã¿ ({score_layer.weight.shape}):\")\n",
        "        weight = score_layer.weight.data.cpu().numpy()\n",
        "        print(f\"  å¹³å‡: {np.mean(weight):.6f}\")\n",
        "        print(f\"  æ¨™æº–åå·®: {np.std(weight):.6f}\")\n",
        "        print(f\"  æœ€å°å€¤: {np.min(weight):.6f}\")\n",
        "        print(f\"  æœ€å¤§å€¤: {np.max(weight):.6f}\")\n",
        "        # ã‚¼ãƒ­ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹è»½ããƒã‚§ãƒƒã‚¯\n",
        "        zero_count = np.sum(weight == 0)\n",
        "        total_count = weight.size\n",
        "        print(f\"  ã‚¼ãƒ­å€¤ã®æ•°: {zero_count} ({zero_count / total_count * 100:.4f}%)\")\n",
        "    else:\n",
        "        print(\"\\nâš ï¸ è­¦å‘Š: åˆ†é¡ãƒ˜ãƒƒãƒ‰ ('score') ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "\n",
        "    # ä¸€éƒ¨ã®LoRAå±¤ã®é‡ã¿ã‚’ç¢ºèª (ä¾‹: æœ€åˆã®Attention Q_projã®Aã¨B)\n",
        "    lora_A_weight = None\n",
        "    lora_B_weight = None\n",
        "    lora_layer_found = False\n",
        "    for name, module in peft_model.named_modules():\n",
        "        if \"layers.0.self_attn.q_proj.lora_A.default\" in name and hasattr(module, 'weight'):\n",
        "             lora_A_weight = module.weight.data.cpu().numpy()\n",
        "             lora_layer_found = True\n",
        "             print(f\"\\nâœ… LoRA-A ({name}) ã®é‡ã¿ ({module.weight.shape}):\")\n",
        "             print(f\"  å¹³å‡: {np.mean(lora_A_weight):.6f}\")\n",
        "             print(f\"  æ¨™æº–åå·®: {np.std(lora_A_weight):.6f}\")\n",
        "             print(f\"  æœ€å°å€¤: {np.min(lora_A_weight):.6f}\")\n",
        "             print(f\"  æœ€å¤§å€¤: {np.max(lora_A_weight):.6f}\")\n",
        "             zero_count = np.sum(lora_A_weight == 0)\n",
        "             total_count = lora_A_weight.size\n",
        "             print(f\"  ã‚¼ãƒ­å€¤ã®æ•°: {zero_count} ({zero_count / total_count * 100:.4f}%)\")\n",
        "\n",
        "        if \"layers.0.self_attn.q_proj.lora_B.default\" in name and hasattr(module, 'weight'):\n",
        "             lora_B_weight = module.weight.data.cpu().numpy()\n",
        "             print(f\"\\nâœ… LoRA-B ({name}) ã®é‡ã¿ ({module.weight.shape}):\")\n",
        "             print(f\"  å¹³å‡: {np.mean(lora_B_weight):.6f}\")\n",
        "             print(f\"  æ¨™æº–åå·®: {np.std(lora_B_weight):.6f}\")\n",
        "             print(f\"  æœ€å°å€¤: {np.min(lora_B_weight):.6f}\")\n",
        "             print(f\"  æœ€å¤§å€¤: {np.max(lora_B_weight):.6f}\")\n",
        "             zero_count = np.sum(lora_B_weight == 0)\n",
        "             total_count = lora_B_weight.size\n",
        "             print(f\"  ã‚¼ãƒ­å€¤ã®æ•°: {zero_count} ({zero_count / total_count * 100:.4f}%)\")\n",
        "\n",
        "        # ä»–ã®LoRAå±¤ã‚‚ç¢ºèªã—ãŸã„å ´åˆã¯ã“ã“ã«è¿½åŠ \n",
        "\n",
        "    if not lora_layer_found:\n",
        "         print(\"\\nâš ï¸ è­¦å‘Š: LoRAå±¤ã®é‡ã¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚PEFTãŒæ­£ã—ãé©ç”¨ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ PEFTãƒ¢ãƒ‡ãƒ«ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\")"
      ],
      "id": "1186e3ba",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ§  PEFTé©ç”¨å¾Œã®ãƒ¢ãƒ‡ãƒ«é‡ã¿ç¢ºèª:\n",
            "=============================================================\n",
            "\n",
            "âš ï¸ è­¦å‘Š: åˆ†é¡ãƒ˜ãƒƒãƒ‰ ('score') ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n",
            "\n",
            "âœ… LoRA-A (base_model.model.model.layers.0.self_attn.q_proj.lora_A.default) ã®é‡ã¿ (torch.Size([16, 2304])):\n",
            "  å¹³å‡: 0.000020\n",
            "  æ¨™æº–åå·®: 0.012074\n",
            "  æœ€å°å€¤: -0.022465\n",
            "  æœ€å¤§å€¤: 0.022262\n",
            "  ã‚¼ãƒ­å€¤ã®æ•°: 0 (0.0000%)\n",
            "\n",
            "âœ… LoRA-B (base_model.model.model.layers.0.self_attn.q_proj.lora_B.default) ã®é‡ã¿ (torch.Size([2048, 16])):\n",
            "  å¹³å‡: -0.000007\n",
            "  æ¨™æº–åå·®: 0.000564\n",
            "  æœ€å°å€¤: -0.002245\n",
            "  æœ€å¤§å€¤: 0.002472\n",
            "  ã‚¼ãƒ­å€¤ã®æ•°: 0 (0.0000%)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "439cc99c22174d5a8bab68c7fac421ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb00f8d1949b4c59a1f94ba53662e7a8",
              "IPY_MODEL_7f15ca0ba10e45d1837db7a80cc37d3f",
              "IPY_MODEL_87c2b27bb7b54da4a16c6176462b1099"
            ],
            "layout": "IPY_MODEL_0ddb37b847924c27bc3005b6905a2b99"
          }
        },
        "eb00f8d1949b4c59a1f94ba53662e7a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1128c2f9caf4793a6d3cbc17dd65df8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3888b119eaa74038964fe282ebc685ce",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "7f15ca0ba10e45d1837db7a80cc37d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60f83dd266c04fb3a39121e924f8ff1e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa9f906209a842e28d297766efbc9611",
            "value": 2
          }
        },
        "87c2b27bb7b54da4a16c6176462b1099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_929a939f474e43589601889264c72773",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_047a1251a1544e9f95757dcfe0e5d4f0",
            "value": "â€‡2/2â€‡[00:01&lt;00:00,â€‡â€‡1.56s/it]"
          }
        },
        "0ddb37b847924c27bc3005b6905a2b99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1128c2f9caf4793a6d3cbc17dd65df8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3888b119eaa74038964fe282ebc685ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60f83dd266c04fb3a39121e924f8ff1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa9f906209a842e28d297766efbc9611": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "929a939f474e43589601889264c72773": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "047a1251a1544e9f95757dcfe0e5d4f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}