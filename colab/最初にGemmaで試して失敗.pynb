{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6dd23d5468f44ff99be22f7a4c037c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0897814d23c476798fbe44fd13f9b40",
              "IPY_MODEL_c814326798a844b0a465ceba73202e65",
              "IPY_MODEL_31a6ef69c4fc42c1ba04430ec159554a"
            ],
            "layout": "IPY_MODEL_acd921f303d24e189a30ef742adce4c3"
          }
        },
        "c0897814d23c476798fbe44fd13f9b40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dff38ca47d74b0aa43c5ecb2b46299f",
            "placeholder": "​",
            "style": "IPY_MODEL_7ba747bc0d2c4a2a8be82d8023387a0f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c814326798a844b0a465ceba73202e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d931b83a17914efea56777529efe0afc",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11fc9e18498e411b9ebe457f7a19ec9b",
            "value": 2
          }
        },
        "31a6ef69c4fc42c1ba04430ec159554a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85f440a789d3477b9aed8e7fe206dea8",
            "placeholder": "​",
            "style": "IPY_MODEL_547ef253e5ab4da8a391dcf50d322580",
            "value": " 2/2 [00:02&lt;00:00,  2.20s/it]"
          }
        },
        "acd921f303d24e189a30ef742adce4c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dff38ca47d74b0aa43c5ecb2b46299f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ba747bc0d2c4a2a8be82d8023387a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d931b83a17914efea56777529efe0afc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11fc9e18498e411b9ebe457f7a19ec9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85f440a789d3477b9aed8e7fe206dea8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "547ef253e5ab4da8a391dcf50d322580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "メインコード\n",
        "google/gemma-2-2b-it\n",
        "LoRA\n",
        "分類タスク用カスタムヘッド\n",
        "A100使用"
      ],
      "metadata": {
        "id": "YKoEcrroXhV5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6dd23d5468f44ff99be22f7a4c037c20",
            "c0897814d23c476798fbe44fd13f9b40",
            "c814326798a844b0a465ceba73202e65",
            "31a6ef69c4fc42c1ba04430ec159554a",
            "acd921f303d24e189a30ef742adce4c3",
            "1dff38ca47d74b0aa43c5ecb2b46299f",
            "7ba747bc0d2c4a2a8be82d8023387a0f",
            "d931b83a17914efea56777529efe0afc",
            "11fc9e18498e411b9ebe457f7a19ec9b",
            "85f440a789d3477b9aed8e7fe206dea8",
            "547ef253e5ab4da8a391dcf50d322580"
          ]
        },
        "id": "1oP9oFn73d9k",
        "outputId": "d0d69312-70ee-4d75-d07f-12e0a60e134e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Start of cell 1oP9oFn73d9k ---\n",
            "MathMisconceptionDataset class defined.\n",
            "custom_data_collator function defined.\n",
            "🚀 MAP競技 - Gemma 分類モデル開始\n",
            "============================================================\n",
            "システム要件チェック:\n",
            "PyTorch: 2.6.0+cu124\n",
            "CUDA利用可能: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "GPU メモリ: 42.5 GB\n",
            "🎯 MAP競技 - Gemma 分類NLPモデル（コンペ形式準拠）\n",
            "============================================================\n",
            "============================================================\n",
            "データ読み込みと前処理（6分類形式）\n",
            "============================================================\n",
            "訓練データ: (36696, 7)\n",
            "テストデータ: (3, 5)\n",
            "Category分布:\n",
            "Category\n",
            "True_Correct           14802\n",
            "False_Misconception     9457\n",
            "False_Neither           6542\n",
            "True_Neither            5265\n",
            "True_Misconception       403\n",
            "False_Correct            227\n",
            "Name: count, dtype: int64\n",
            "NaN除去: 36696 -> 36696 (0行削除)\n",
            "\n",
            "ユニークなカテゴリ数: 6\n",
            "カテゴリ一覧:\n",
            "  0: False_Correct (227件)\n",
            "  1: False_Misconception (9457件)\n",
            "  2: False_Neither (6542件)\n",
            "  3: True_Correct (14802件)\n",
            "  4: True_Misconception (403件)\n",
            "  5: True_Neither (5265件)\n",
            "\n",
            "============================================================\n",
            "モデル準備: google/gemma-2-2b-it (SequenceClassification ロード、PEFT適用、デバイス配置)\n",
            "============================================================\n",
            "使用デバイス: cuda\n",
            "google/gemma-2-2b-it トークナイザー読み込み中...\n",
            "google/gemma-2-2b-it ベースモデル (SequenceClassificationとして) 読み込み中...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6dd23d5468f44ff99be22f7a4c037c20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at google/gemma-2-2b-it and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoModelForSequenceClassification モデル (google/gemma-2-2b-it) ロード完了\n",
            "モデルパラメータ数: 2,614,355,712\n",
            "\n",
            "PEFT (LoRA) 設定中...\n",
            "LoRA Config:\n",
            "LoraConfig(task_type='SEQ_CLS', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=16, target_modules={'gate_proj', 'v_proj', 'o_proj', 'q_proj', 'down_proj', 'up_proj', 'k_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['classifier', 'score'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PEFTモデル作成完了。\n",
            "trainable params: 20,780,544 || all params: 2,635,136,256 || trainable%: 0.7886\n",
            "PEFTモデルをデバイスに移動: cuda:0\n",
            "\n",
            "============================================================\n",
            "6分類モデル ファインチューニング開始\n",
            "============================================================\n",
            "エンコードされたラベル:\n",
            "  0: False_Correct\n",
            "  1: False_Misconception\n",
            "  2: False_Neither\n",
            "  3: True_Correct\n",
            "  4: True_Misconception\n",
            "  5: True_Neither\n",
            "Stratified split適用\n",
            "訓練データ: 29356\n",
            "検証データ: 7340\n",
            "\n",
            "TrainingArguments:\n",
            "TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=False,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=300,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=4,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_revision=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-05,\n",
            "length_column_name=length,\n",
            "liger_kernel_config=None,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/drive/MyDrive/kaggleCompe_MAP-math/logs_gemma_classification,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=map3,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=/content/drive/MyDrive/kaggleCompe_MAP-math/gemma_classification_model,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=[],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/drive/MyDrive/kaggleCompe_MAP-math/gemma_classification_model,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=300,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=200,\n",
            "weight_decay=0.01,\n",
            ")\n",
            "ファインチューニング開始...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1544' max='4590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1544/4590 1:16:42 < 2:31:32, 0.34 it/s, Epoch 1.68/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Map3</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.283447</td>\n",
              "      <td>0.006131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.283447</td>\n",
              "      <td>0.006131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.283447</td>\n",
              "      <td>0.006131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.283447</td>\n",
              "      <td>0.006131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.283447</td>\n",
              "      <td>0.006131</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Entry point print for the cell\n",
        "print(\"--- Start of cell 1oP9oFn73d9k ---\")\n",
        "\n",
        "import sys # Moved import sys to the very top\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM, # Keep import for potential future use, but not used in this flow\n",
        "    AutoModelForSequenceClassification, # Import AutoModelForSequenceClassification\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding, # Keep import for reference, but use custom collator\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from peft import LoraConfig, get_peft_model # Ensure PEFT is imported and used\n",
        "import warnings\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Re-define the Dataset class to ensure it's available\n",
        "class MathMisconceptionDataset(Dataset):\n",
        "    \"\"\"Math Misconception Dataset for PyTorch\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # トークン化\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Return tensors with an explicit batch dimension of 1\n",
        "        # This is important for custom collation, and generally good practice\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"], # Shape [1, max_length]\n",
        "            \"attention_mask\": encoding[\"attention_mask\"], # Shape [1, max_length]\n",
        "            \"labels\": torch.tensor([label], dtype=torch.long), # Shape [1]\n",
        "        }\n",
        "print(\"MathMisconceptionDataset class defined.\")\n",
        "\n",
        "# Custom data collator function (Modified slightly for clarity, but logic is similar)\n",
        "# This collator will stack the tensors, resulting in shapes [batch_size, ...]\n",
        "def custom_data_collator(features):\n",
        "    \"\"\"Custom data collator to handle batching.\"\"\"\n",
        "    if not isinstance(features, list):\n",
        "        features = [features]\n",
        "\n",
        "    # Stack tensors from the list along the batch dimension (dim=0)\n",
        "    input_ids = torch.cat([f[\"input_ids\"] for f in features], dim=0)\n",
        "    attention_mask = torch.cat([f[\"attention_mask\"] for f in features], dim=0)\n",
        "    labels = torch.cat([f[\"labels\"] for f in features], dim=0)\n",
        "\n",
        "    # Ensure labels are 1D for CrossEntropyLoss (shape [batch_size])\n",
        "    if labels.ndim > 1:\n",
        "         labels = labels.squeeze(1) # Remove dimension if it's [batch_size, 1]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "print(\"custom_data_collator function defined.\")\n",
        "\n",
        "\n",
        "# Re-define load_and_prepare_data\n",
        "def load_and_prepare_data():\n",
        "    \"\"\"データの読み込みと前処理（6分類形式）\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"データ読み込みと前処理（6分類形式）\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Define the data paths on Google Drive\n",
        "    train_data_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/map_data/train.csv\"\n",
        "    test_data_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/map_data/test.csv\"\n",
        "\n",
        "    try:\n",
        "        # データ読み込み\n",
        "        train_df = pd.read_csv(train_data_path)\n",
        "        test_df = pd.read_csv(test_data_path)\n",
        "\n",
        "        print(f\"訓練データ: {train_df.shape}\")\n",
        "        print(f\"テストデータ: {test_df.shape}\")\n",
        "\n",
        "        # コンペ形式のターゲット作成（6分類）\n",
        "        print(\"Category分布:\")\n",
        "        print(train_df[\"Category\"].value_counts())\n",
        "\n",
        "        # NaN値除去\n",
        "        before_len = len(train_df)\n",
        "        train_df = train_df.dropna(subset=[\"Category\", \"StudentExplanation\"])\n",
        "        after_len = len(train_df)\n",
        "        print(f\"NaN除去: {before_len} -> {after_len} ({before_len - after_len}行削除)\")\n",
        "\n",
        "        # テキスト特徴量作成\n",
        "        def create_enhanced_text(row):\n",
        "            \"\"\"強化されたテキスト特徴量を作成\"\"\"\n",
        "            question = str(row[\"QuestionText\"]) if pd.notna(row[\"QuestionText\"]) else \"\"\n",
        "            mc_answer = str(row[\"MC_Answer\"]) if pd.notna(row[\"MC_Answer\"]) else \"\"\n",
        "            explanation = (\n",
        "                str(row[\"StudentExplanation\"])\n",
        "                if pd.notna(row[\"StudentExplanation\"])\n",
        "                else \"\"\n",
        "            )\n",
        "\n",
        "            # 質問、選択された答え、説明を結合\n",
        "            enhanced_text = f\"Question: {question} Selected Answer: {mc_answer} Explanation: {explanation}\"\n",
        "            return enhanced_text\n",
        "\n",
        "        train_df[\"enhanced_text\"] = train_df.apply(create_enhanced_text, axis=1)\n",
        "        test_df[\"enhanced_text\"] = test_df.apply(create_enhanced_text, axis=1)\n",
        "\n",
        "        # 6つのカテゴリの確認\n",
        "        unique_categories = sorted(train_df[\"Category\"].unique())\n",
        "        print(f\"\\nユニークなカテゴリ数: {len(unique_categories)}\")\n",
        "        print(\"カテゴリ一覧:\")\n",
        "        for i, cat in enumerate(unique_categories):\n",
        "            count = (train_df[\"Category\"] == cat).sum()\n",
        "            print(f\"  {i}: {cat} ({count}件)\")\n",
        "\n",
        "        return train_df, test_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ データ読み込みまたは前処理に失敗しました: {e}\", file=sys.stderr)\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# Remove the custom GemmaForSequenceClassification class definition\n",
        "# class GemmaForSequenceClassification(nn.Module):\n",
        "#     def __init__(self, base_model, num_labels):\n",
        "#         ...\n",
        "#     def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "#         ...\n",
        "# Removed the entire class definition\n",
        "\n",
        "\n",
        "# New prepare_model function using AutoModelForSequenceClassification and applying PEFT\n",
        "def prepare_model_for_classification(num_labels=6, model_name=\"google/gemma-2-2b-it\"): # Changed model name to google/gemma-2-2b-it\n",
        "    \"\"\"AutoModelForSequenceClassification モデルとトークナイザーのロード、PEFT適用、デバイス配置\"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 60)\n",
        "    print(f\"モデル準備: {model_name} (SequenceClassification ロード、PEFT適用、デバイス配置)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"使用デバイス: {device}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"{model_name} トークナイザー読み込み中...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "            print(f\"パディングトークンを '{tokenizer.pad_token}' に設定しました。\")\n",
        "        # Add a classification token if the model doesn't have one (common for CausalLMs fine-tuned for classification)\n",
        "        # Note: Gemma typically uses EOS as a classification token, check tokenizer details if needed.\n",
        "\n",
        "\n",
        "        print(f\"{model_name} ベースモデル (SequenceClassificationとして) 読み込み中...\")\n",
        "        # Use AutoModelForSequenceClassification directly\n",
        "        # Pass num_labels here, the model will automatically add a classification head\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=num_labels,\n",
        "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if device.type == \"cuda\" else None,\n",
        "            low_cpu_mem_usage=True,\n",
        "            # Add pad_token_id if necessary, usually matches eos_token_id\n",
        "            pad_token_id=tokenizer.eos_token_id if tokenizer.pad_token_id is None else tokenizer.pad_token_id,\n",
        "        )\n",
        "\n",
        "        print(f\"AutoModelForSequenceClassification モデル ({model_name}) ロード完了\")\n",
        "        print(f\"モデルパラメータ数: {model.num_parameters():,}\")\n",
        "\n",
        "\n",
        "        # Configure PEFT (LoRA)\n",
        "        print(\"\\nPEFT (LoRA) 設定中...\")\n",
        "        # Adjust LoRA config parameters as needed\n",
        "        lora_config = LoraConfig(\n",
        "            r=16, # LoRA attention dimension\n",
        "            lora_alpha=32, # Alpha parameter for LoRA scaling\n",
        "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # Added common linear layers for LoRA\n",
        "            lora_dropout=0.05, # Dropout probability for LoRA layers\n",
        "            bias=\"none\", # Bias type\n",
        "            task_type=\"SEQ_CLS\", # Specify task type for Sequence Classification\n",
        "            modules_to_save=[\"classifier\", \"score\"], # Save the classification head and potentially the scoring layer if it exists\n",
        "        )\n",
        "        print(\"LoRA Config:\")\n",
        "        print(lora_config)\n",
        "\n",
        "        # Apply PEFT to the base model\n",
        "        model = get_peft_model(model, lora_config)\n",
        "        print(\"PEFTモデル作成完了。\")\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "        # Move the PEFT model to the correct device\n",
        "        # device_map=\"auto\" in from_pretrained might handle this, but explicit to() is safer\n",
        "        model.to(device)\n",
        "        print(f\"PEFTモデルをデバイスに移動: {next(model.parameters()).device}\")\n",
        "\n",
        "\n",
        "        return model, tokenizer, device\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ モデルまたはトークナイザーの読み込み、PEFT適用に失敗しました: {e}\", file=sys.stderr)\n",
        "        print(\"\\n考えられる原因:\", file=sys.stderr)\n",
        "        print(\"1. Hugging Face Hub への認証が必要\", file=sys.stderr)\n",
        "        print(\"2. Internet接続の問題\", file=sys.stderr) # Corrected typo\n",
        "        print(\"3. GPU メモリ不足\", file=sys.stderr)\n",
        "        print(\"4. ライブラリのバージョン問題 (transformers, peft, torch)\", file=sys.stderr)\n",
        "        print(\"5. モデル名の誤り\", file=sys.stderr)\n",
        "        print(\"\\n解決方法:\", file=sys.stderr)\n",
        "        print(\"- huggingface-cli login でログイン\", file=sys.stderr)\n",
        "        print(\"- Internet接続確認\", file=sys.stderr) # Corrected typo\n",
        "        print(\"- GPU メモリ空き容量確認\")\n",
        "        print(\"- ライブラリをアップデート (pip install -U transformers peft accelerate)\")\n",
        "        print(\"- モデル名を確認 (例: google/gemma-2-2b-it)\", file=sys.stderr)\n",
        "        print(\"\\nプログラムを終了します。\", file=sys.stderr)\n",
        "        return None, None, None\n",
        "\n",
        "\n",
        "# Re-define compute_map3_metrics and other functions\n",
        "def compute_map3_metrics(eval_pred):\n",
        "    \"\"\"MAP@3メトリクスの計算\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = torch.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
        "\n",
        "    map_scores = []\n",
        "    for i, true_label in enumerate(labels):\n",
        "        top3_indices = np.argsort(predictions[i])[::-1][:3]\n",
        "        score = 0.0\n",
        "        for j, pred_idx in enumerate(top3_indices):\n",
        "            if pred_idx == true_label:\n",
        "                score = 1.0 / (j + 1)\n",
        "                break\n",
        "        map_scores.append(score)\n",
        "\n",
        "    map3_score = np.mean(map_scores)\n",
        "    accuracy = accuracy_score(labels, np.argmax(predictions, axis=1))\n",
        "\n",
        "    return {\"map3\": map3_score, \"accuracy\": accuracy}\n",
        "\n",
        "def fine_tune_model(train_df, model, tokenizer, device):\n",
        "    \"\"\"モデルのファインチューニング\"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 60)\n",
        "    print(\"6分類モデル ファインチューニング開始\")\n",
        "    print(\"=\" * 60)\n",
        "    # print(\"--- Inside fine_tune_model ---\") # Removed debug print\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    train_df[\"encoded_labels\"] = label_encoder.fit_transform(train_df[\"Category\"])\n",
        "\n",
        "    print(f\"エンコードされたラベル:\")\n",
        "    for i, label in enumerate(label_encoder.classes_):\n",
        "        print(f\"  {i}: {label}\")\n",
        "\n",
        "    train_texts = train_df[\"enhanced_text\"].tolist()\n",
        "    train_labels = train_df[\"encoded_labels\"].tolist()\n",
        "\n",
        "    try:\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            train_texts,\n",
        "            train_labels,\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=train_labels,\n",
        "        )\n",
        "        print(\"Stratified split適用\")\n",
        "    except ValueError:\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            train_texts, train_labels, test_size=0.2, random_state=42\n",
        "        )\n",
        "        print(\"Regular split適用（少数クラスのため）\")\n",
        "\n",
        "    print(f\"訓練データ: {len(X_train)}\")\n",
        "    print(f\"検証データ: {len(X_val)}\")\n",
        "\n",
        "    train_dataset = MathMisconceptionDataset(\n",
        "        X_train, y_train, tokenizer, max_length=512\n",
        "    )\n",
        "    val_dataset = MathMisconceptionDataset(X_val, y_val, tokenizer, max_length=512)\n",
        "    # print(\"train_dataset and val_dataset created.\") # Removed debug print\n",
        "\n",
        "    # Use the custom data collator\n",
        "    data_collator = custom_data_collator\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"/content/drive/MyDrive/kaggleCompe_MAP-math/gemma_classification_model\", # Updated output dir name\n",
        "        num_train_epochs=5,\n",
        "        per_device_train_batch_size=8, # Changed batch size back to 8 as requested\n",
        "        per_device_eval_batch_size=8, # Changed batch size back to 8\n",
        "        gradient_accumulation_steps=4, # Adjusted accumulation to 4 to keep effective batch size 32\n",
        "        warmup_steps=200,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"/content/drive/MyDrive/kaggleCompe_MAP-math/logs_gemma_classification\", # Updated log dir name\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=300,\n",
        "        save_strategy=\"steps\", # Match eval_strategy for load_best_model_at_end\n",
        "        save_steps=300, # Save checkpoints when evaluating\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"map3\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\",\n",
        "        dataloader_pin_memory=False,\n",
        "        fp16=False, # Keep FP16 disabled for potential stability\n",
        "        bf16=True, # Keep BF16 enabled (if supported)\n",
        "        optim=\"adamw_torch\",\n",
        "        learning_rate=1e-5,\n",
        "        save_total_limit=2,\n",
        "        gradient_checkpointing=False, # Keep gradient checkpointing disabled\n",
        "        # gradient_checkpointing_kwargs={'use_reentrant': False}, # Remove kwargs as gradient checkpointing is False\n",
        "    )\n",
        "\n",
        "    print(\"\\nTrainingArguments:\")\n",
        "    print(training_args)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator, # Use custom collator\n",
        "        compute_metrics=compute_map3_metrics,\n",
        "    )\n",
        "\n",
        "    print(\"ファインチューニング開始...\")\n",
        "    try:\n",
        "        trainer.train()\n",
        "        print(\"ファインチューニング完了。\")\n",
        "        # Explicitly save the model after training\n",
        "        save_path = training_args.output_dir\n",
        "        print(f\"ファインチューニング済みモデルを保存中: {save_path}\")\n",
        "        trainer.save_model(save_path)\n",
        "        print(\"ファインチューニング済みモデル保存完了。\")\n",
        "\n",
        "        if trainer is None:\n",
        "             print(\"Warning: trainer is None after trainer.train()\", file=sys.stderr)\n",
        "        else:\n",
        "             print(\"trainer object is valid after trainer.train().\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ファインチューニング中にエラーが発生しました: {e}\", file=sys.stderr)\n",
        "        return None, None # Return None on failure\n",
        "\n",
        "    print(\"\\n最終評価:\")\n",
        "    try:\n",
        "        eval_results = trainer.evaluate()\n",
        "        for key, value in eval_results.items():\n",
        "            print(f\"{key}: {value:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 最終評価中にエラーが発生しました: {e}\", file=sys.stderr)\n",
        "        return None, None\n",
        "\n",
        "\n",
        "    return trainer, label_encoder\n",
        "\n",
        "def generate_submission(trainer, tokenizer, test_df, label_encoder):\n",
        "    \"\"\"コンペ形式の提出ファイル生成\"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 60)\n",
        "    print(\"コンペ形式提出ファイル生成\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if trainer is None:\n",
        "         print(\"Error: Trainer object is None, cannot generate submission.\", file=sys.stderr)\n",
        "         return None\n",
        "\n",
        "    test_texts = test_df[\"enhanced_text\"].tolist()\n",
        "    test_dataset = MathMisconceptionDataset(\n",
        "        test_texts, [0] * len(test_texts), tokenizer, max_length=512\n",
        "    )\n",
        "\n",
        "    print(\"予測実行中...\")\n",
        "    try:\n",
        "        # Use the same data collator for prediction\n",
        "        data_collator = custom_data_collator\n",
        "        # Trainer's predict method automatically uses the data collator from TrainingArguments\n",
        "        # or can be passed explicitly, but it usually works fine with the one set during init.\n",
        "        # However, explicitly setting it here for clarity if needed, though Trainer should use self.data_collator\n",
        "        # predictions = trainer.predict(test_dataset, data_collator=data_collator)\n",
        "\n",
        "        predictions = trainer.predict(test_dataset) # Trainer should use the data_collator set during init\n",
        "\n",
        "\n",
        "        print(\"予測完了。\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 予測中にエラーが発生しました: {e}\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "\n",
        "    print(\"ソフトマックス適用中...\")\n",
        "    try:\n",
        "        # predictions.predictions contains the raw logits\n",
        "        probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1).numpy()\n",
        "        print(\"ソフトマックス適用完了。\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ソフトマックス適用中にエラーが発生しました: {e}\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "\n",
        "    # 各サンプルで上位3つの予測を取得\n",
        "    submission_predictions = []\n",
        "    print(\"上位3つの予測を取得中...\")\n",
        "    try:\n",
        "        for prob in probs:\n",
        "            top3_indices = np.argsort(prob)[::-1][:3]\n",
        "            top3_labels = [label_encoder.classes_[idx] for idx in top3_indices]\n",
        "            submission_predictions.append(\" \".join(top3_labels))\n",
        "        print(\"上位3つの予測取得完了。\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 上位3つの予測取得中にエラーが発生しました: {e}\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "\n",
        "    # 提出形式のDataFrame作成\n",
        "    print(\"提出形式DataFrame作成中...\")\n",
        "    try:\n",
        "        submission_df = pd.DataFrame(\n",
        "            {\n",
        "                \"row_id\": range(len(test_df)),\n",
        "                \"Category:Misconception\": submission_predictions,\n",
        "            }\n",
        "        )\n",
        "        print(\"提出形式DataFrame作成完了。\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 提出形式DataFrame作成中にエラーが発生しました: {e}\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "\n",
        "    # サンプル表示\n",
        "    print(\"提出ファイルサンプル:\")\n",
        "    print(submission_df.head(10))\n",
        "\n",
        "    print(f\"予測完了: {len(submission_df)}行\")\n",
        "    return submission_df\n",
        "\n",
        "\n",
        "# Update the main function to run the full pipeline and check return values\n",
        "def main():\n",
        "    \"\"\"メイン実行関数\"\"\"\n",
        "    print(\"🎯 MAP競技 - Gemma 分類NLPモデル（コンペ形式準拠）\") # Updated title\n",
        "    print(\"=\" * 60)\n",
        "    # print(\"--- Inside main ---\") # Removed debug print\n",
        "\n",
        "    train_df, test_df = load_and_prepare_data()\n",
        "    if train_df is None or test_df is None:\n",
        "         print(\"データ準備に失敗しました。プログラムを終了します。\", file=sys.stderr)\n",
        "         return None, None, None\n",
        "\n",
        "    # Use the new prepare_model_for_classification function\n",
        "    # We don't need the global peft_model, tokenizer, device from the old cell 1713c1f2 anymore\n",
        "    # The new function loads everything needed internally\n",
        "    # global peft_model, tokenizer, device # Remove access to old global variables\n",
        "\n",
        "\n",
        "    # Prepare the model using the new function\n",
        "    # We pass the number of labels and the desired model name\n",
        "    # model, tokenizer, device = prepare_model(num_labels=6, model_name=\"google/gemma-3-1b-it\") # Removed old call\n",
        "    model, tokenizer, device = prepare_model_for_classification(num_labels=6, model_name=\"google/gemma-2-2b-it\") # Call the new function\n",
        "\n",
        "\n",
        "    if model is None or tokenizer is None or device is None:\n",
        "        print(\"モデル準備に失敗しました。プログラムを終了します。\", file=sys.stderr) # Simplified error message\n",
        "        return None, None, None\n",
        "\n",
        "\n",
        "    trainer, label_encoder = fine_tune_model(train_df, model, tokenizer, device)\n",
        "    # Check if fine_tune_model returned a valid trainer\n",
        "    if trainer is None and label_encoder is None: # Check for both None\n",
        "         print(\"ファインチューニング中にエラーが発生しました。プログラムを終了します。\", file=sys.stderr)\n",
        "         return None, None, None\n",
        "\n",
        "\n",
        "    submission_df = generate_submission(trainer, tokenizer, test_df, label_encoder)\n",
        "    # Check if generate_submission returned a valid DataFrame\n",
        "    if submission_df is None:\n",
        "         print(\"提出ファイル生成中にエラーが発生しました。プログラムを終了します。\", file=sys.stderr)\n",
        "         return None, None, None\n",
        "\n",
        "\n",
        "    submission_file_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/gemma_classification_submission.csv\" # Updated submission file name\n",
        "    print(f\"提出ファイル保存: {submission_file_path}\")\n",
        "    try:\n",
        "        submission_df.to_csv(submission_file_path, index=False)\n",
        "        print(f\"提出ファイル保存完了: {submission_file_path}\")\n",
        "    except Exception as e:\n",
        "         print(f\"❌ 提出ファイルの保存に失敗しました: {e}\", file=sys.stderr)\n",
        "         return None, None, None\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"6分類モデル開発完了！\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    return trainer, label_encoder, submission_df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🚀 MAP競技 - Gemma 分類モデル開始\") # Updated title\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(\"システム要件チェック:\")\n",
        "    print(f\"PyTorch: {torch.__version__}\")\n",
        "    print(f\"CUDA利用可能: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(\n",
        "            f\"GPU メモリ: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        # Execute the full main pipeline\n",
        "        trainer, label_encoder, submission_df = main()\n",
        "        if trainer is not None and label_encoder is not None and submission_df is not None:\n",
        "             print(\"\\n🎉 Gemmaモデル開発完了！\")\n",
        "        else:\n",
        "             print(\"\\n❌ モデル開発プロセス中にエラーが発生しました。\")\n",
        "\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n⏹️ ユーザーによって中断されました\")\n",
        "        pass\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ 予期しないエラーが発生しました: {e}\", file=sys.stderr)\n",
        "        print(\"\\nGemmaモデルの実行には以下が必要です:\", file=sys.stderr)\n",
        "        print(\"1. 十分なGPUメモリ (推奨: 8GB以上)\", file=sys.stderr)\n",
        "        print(\"2. Hugging Face Hub認証 (huggingface-cli login)\", file=sys.stderr)\n",
        "        print(\"3. 安定したInternet接続\", file=sys.stderr)\n",
        "        # Removed specific Gemma 3N E4B compatibility note as we are switching models\n",
        "        print(\"\\nプログラムを終了します。\", file=sys.stderr)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "最初にやるやつ"
      ],
      "metadata": {
        "id": "XNC2ARUSW5pa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "58cebf22"
      },
      "source": [
        "!huggingface-cli login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "09024185"
      },
      "source": [
        "!pip install peft accelerate -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03a216d9"
      },
      "source": [
        "!pip install timm -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fbb5455d"
      },
      "source": [
        "try:\n",
        "    import timm\n",
        "    print(\"timm library imported successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Error: timm library could not be imported.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemma3nでやろうとしてメモリが足りなくて失敗したもの"
      ],
      "metadata": {
        "id": "vcksfKyw5Q1J"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37164cfb"
      },
      "source": [
        "# --- Test Cell for GemmaForSequenceClassification ---\n",
        "print(\"--- Start of GemmaForSequenceClassification Test Cell ---\")\n",
        "\n",
        "import sys # Moved import sys to the very top\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM # Need CausalLM to load the base model\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset/DataLoader just in case needed, though dummy data is used\n",
        "\n",
        "# Define the GemmaForSequenceClassification class (copied from 1oP9oFn73d9k)\n",
        "# Add print statements inside the forward method for debugging shapes\n",
        "class GemmaForSequenceClassification(nn.Module):\n",
        "    def __init__(self, base_model, num_labels):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        # Access the hidden size from the text_config attribute\n",
        "        hidden_size = None\n",
        "        config = base_model.config # Start with the model's config\n",
        "\n",
        "        # If it's a PEFT model, try accessing the base model's config\n",
        "        if hasattr(base_model, 'base_model') and hasattr(base_model.base_model, 'config'):\n",
        "             config = base_model.base_model.config\n",
        "             print(f\"TEST: Accessing config from base_model.base_model.config ({type(config)})\")\n",
        "        else:\n",
        "             print(f\"TEST: Accessing config from base_model.config ({type(config)})\")\n",
        "\n",
        "        try:\n",
        "            # Try common attribute names for hidden size sequentially\n",
        "            print(\"TEST: Attempting to find hidden size using common attribute names...\")\n",
        "            if hasattr(config, 'hidden_size') and config.hidden_size is not None:\n",
        "                 hidden_size = config.hidden_size\n",
        "                 print(f\"TEST: Attempted config.hidden_size, Found: {hidden_size}\")\n",
        "            elif hasattr(config, 'd_model') and config.d_model is not None:\n",
        "                 hidden_size = config.d_model\n",
        "                 print(f\"TEST: Attempted config.d_model, Found: {hidden_size}\")\n",
        "            elif hasattr(config, 'n_embd') and config.n_embd is not None:\n",
        "                 hidden_size = config.n_embd\n",
        "                 print(f\"TEST: Attempted config.n_embd, Found: {hidden_size}\")\n",
        "            elif hasattr(config, 'text_config') and hasattr(config.text_config, 'hidden_size') and config.text_config.hidden_size is not None:\n",
        "                 hidden_size = config.text_config.hidden_size\n",
        "                 print(f\"TEST: Attempted config.text_config.hidden_size, Found: {hidden_size}\")\n",
        "            else:\n",
        "                 checked_attributes = ['hidden_size', 'd_model', 'n_embd', 'text_config.hidden_size']\n",
        "                 print(f\"TEST: Checked attributes: {', '.join(checked_attributes)} - None found.\")\n",
        "                 raise AttributeError(f\"Could not find hidden size attribute using common names in {type(config)}. Checked: {', '.join(checked_attributes)}\")\n",
        "\n",
        "            if hidden_size is None:\n",
        "                 raise AttributeError(\"Hidden size attribute found but its value is None.\")\n",
        "\n",
        "        except AttributeError as ae:\n",
        "            print(f\"TEST: ❌ Error accessing hidden size in GemmaForSequenceClassification init: {ae}\")\n",
        "            raise ae\n",
        "\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "        print(f\"TEST: Classification head created with input size: {hidden_size}\")\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        print(\"-\" * 20 + \" TEST: Forward Pass Debug (Start)\" + \"-\" * 20)\n",
        "        print(f\"TEST: input_ids shape (start): {input_ids.shape if input_ids is not None else 'None'}\")\n",
        "        print(f\"TEST: attention_mask shape (start): {attention_mask.shape if attention_mask is not None else 'None'}\")\n",
        "        print(f\"TEST: labels shape (start): {labels.shape if labels is not None else 'None'}\")\n",
        "\n",
        "        # Pass input through the PEFT model\n",
        "        # Use **kwargs to pass other arguments like labels if the base_model accepts them\n",
        "        # although for classification, we primarily care about hidden states/logits\n",
        "        model_inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"output_hidden_states\": True}\n",
        "        # Add labels if they exist and the base model's forward accepts them (GemmaForCausalLM might not directly use labels in its forward, but PEFT wrapper might pass it)\n",
        "        if labels is not None:\n",
        "             model_inputs['labels'] = labels # Pass labels to the base model forward call if it's configured to handle them\n",
        "\n",
        "        # Ensure inputs are on the correct device\n",
        "        device = next(self.parameters()).device\n",
        "        model_inputs = {k: v.to(device) for k, v in model_inputs.items() if isinstance(v, torch.Tensor)}\n",
        "        print(f\"TEST: Model inputs moved to device: {device}\")\n",
        "\n",
        "\n",
        "        outputs = self.base_model(**model_inputs)\n",
        "\n",
        "        print(\"-\" * 20 + \" TEST: Forward Pass Debug (After Base Model)\" + \"-\" * 20)\n",
        "        print(f\"TEST: Base model outputs type: {type(outputs)}\")\n",
        "        # Print keys if outputs is a dictionary or object with keys\n",
        "        if hasattr(outputs, 'keys'):\n",
        "             print(f\"TEST: Base model outputs keys: {outputs.keys()}\")\n",
        "\n",
        "\n",
        "        # Access the last hidden state from the outputs\n",
        "        last_hidden_state = None\n",
        "        if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
        "             # outputs.hidden_states is a tuple, the last element is the hidden states from the last layer\n",
        "             last_hidden_state = outputs.hidden_states[-1] # Shape: (batch_size, sequence_length, hidden_size)\n",
        "             print(f\"TEST: last_hidden_state shape (from hidden_states): {last_hidden_state.shape}\")\n",
        "        elif hasattr(outputs, 'last_hidden_state') and outputs.last_hidden_state is not None: # Some models use last_hidden_state directly\n",
        "             last_hidden_state = outputs.last_hidden_state\n",
        "             print(f\"TEST: last_hidden_state shape (from last_hidden_state): {last_hidden_state.shape}\")\n",
        "        else:\n",
        "             # If hidden_states is not available, try accessing the last layer output directly if possible\n",
        "             # This is less standard for CausalLM outputs, but as a fallback\n",
        "             print(\"TEST: Warning: hidden_states or last_hidden_state not found in base model output. Attempting direct access.\")\n",
        "             try:\n",
        "                 # This might vary depending on the output structure\n",
        "                 if isinstance(outputs, torch.Tensor) and outputs.ndim == 3:\n",
        "                      last_hidden_state = outputs # Assuming the output itself is the last hidden state\n",
        "                      print(f\"TEST: Accessed last hidden state directly from outputs (Tensor): {last_hidden_state.shape}\")\n",
        "                 elif isinstance(outputs, (list, tuple)) and len(outputs) > 0 and isinstance(outputs[0], torch.Tensor) and outputs[0].ndim == 3:\n",
        "                     last_hidden_state = outputs[0] # Assuming the first element is the last hidden state\n",
        "                     print(f\"TEST: Accessed last hidden state from outputs[0] (Tensor): {last_hidden_state.shape}\")\n",
        "                 else:\n",
        "                      print(f\"TEST: Could not find a 3D tensor in outputs: {outputs}\")\n",
        "                      raise AttributeError(\"Could not find a suitable tensor for last hidden state.\")\n",
        "             except Exception as e:\n",
        "                 print(f\"TEST: ❌ Error accessing last hidden state from base model output: {e}\")\n",
        "                 # Still raise the error if we can't get the hidden state\n",
        "                 raise AttributeError(\"Could not get last hidden state from base model output.\")\n",
        "\n",
        "\n",
        "        if last_hidden_state is None:\n",
        "             raise RuntimeError(\"Failed to obtain last hidden state from the base model output.\")\n",
        "\n",
        "\n",
        "        # Take the hidden state corresponding to the last token in the sequence\n",
        "        # Note: For classification, it's common to use the [CLS] token output or the last token output.\n",
        "        # For CausalLM fine-tuning for classification, the last token is used.\n",
        "        print(f\"TEST: last_hidden_state device before slicing: {last_hidden_state.device}\")\n",
        "        sequence_output = last_hidden_state[:, -1, :] # Shape: (batch_size, hidden_size)\n",
        "        print(f\"TEST: sequence_output shape (last token): {sequence_output.shape}\")\n",
        "\n",
        "\n",
        "        # Pass the hidden states through the classifier\n",
        "        print(f\"TEST: sequence_output device before classifier: {sequence_output.device}\")\n",
        "        print(f\"TEST: Classifier layer device: {self.classifier.weight.device}\")\n",
        "        logits = self.classifier(sequence_output) # Shape: (batch_size, num_labels)\n",
        "        print(f\"TEST: logits shape (after classifier): {logits.shape}\")\n",
        "\n",
        "\n",
        "        # Calculate loss internally (as before)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            # Ensure logits and labels are on the same device before loss calculation\n",
        "            # This is a potential source of subtle bugs\n",
        "            if logits.device != labels.device:\n",
        "                 print(f\"TEST: Warning: logits ({logits.device}) and labels ({labels.device}) are on different devices. Moving logits to labels device.\")\n",
        "                 logits = logits.to(labels.device)\n",
        "\n",
        "            print(\"-\" * 20 + \" TEST: Forward Pass Debug (Before Loss)\" + \"-\" * 20)\n",
        "            print(f\"TEST: logits shape (before loss call): {logits.shape}\")\n",
        "            print(f\"TEST: labels shape (before loss call): {labels.shape if labels is not None else 'None'}\")\n",
        "            if labels is not None:\n",
        "                 print(f\"TEST: logits device (before loss call): {logits.device}, labels device (before loss call): {labels.device}\")\n",
        "            print(\"-\" * 20 + \" TEST: Forward Pass Debug (End)\" + \"-\" * 20)\n",
        "\n",
        "\n",
        "            loss = loss_fct(logits, labels) # This is where the error might happen\n",
        "            print(f\"TEST: Loss calculated: {loss.item()}\")\n",
        "\n",
        "\n",
        "        # Return logits and loss for the Trainer\n",
        "        return {\"logits\": logits, \"loss\": loss}\n",
        "\n",
        "\n",
        "# --- Model Loading and PEFT Application ---\n",
        "print(\"\\n--- Loading Model and Applying PEFT ---\")\n",
        "model_name = \"google/gemma-3n-E4B-it\" # Use the model that was causing issues\n",
        "num_labels = 6 # Assuming 6 classes\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"TEST: Using device: {device}\")\n",
        "\n",
        "try:\n",
        "    print(f\"TEST: Loading tokenizer for {model_name}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(f\"TEST: Setting padding token to '{tokenizer.pad_token}'.\")\n",
        "\n",
        "    print(f\"TEST: Loading base model (CausalLM) {model_name}...\")\n",
        "    # Use torch_dtype=torch.float16 and device_map=\"auto\" to handle potentially large model\n",
        "    # The meta tensor error suggests the model is still too big even with auto device map\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16, # Keep float16 for memory efficiency\n",
        "        device_map=\"auto\", # Attempt auto device mapping\n",
        "        low_cpu_mem_usage=True,\n",
        "    )\n",
        "    # Ensure the base model is on the correct device if device_map was not 'auto' or if running on CPU\n",
        "    # This explicit .to(device) might be the source of the meta tensor error if device_map failed\n",
        "    # Let's REMOVE this explicit .to(device) after from_pretrained with device_map=\"auto\"\n",
        "    # if device.type != 'cuda' or base_model.device.type != device.type:\n",
        "    #      base_model.to(device)\n",
        "    #      print(f\"TEST: Base model moved to device: {base_model.device}\")\n",
        "\n",
        "\n",
        "    # PEFT (LoRA) configuration\n",
        "    print(\"\\nTEST: Configuring PEFT (LoRA)...\")\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\", # Keep CAUSAL_LM task type for base model PEFT\n",
        "        # modules_to_save should NOT include classifier here, as it's in the wrapper\n",
        "    )\n",
        "    print(\"TEST: LoRA Config:\")\n",
        "    print(lora_config)\n",
        "\n",
        "    # Apply PEFT to the base model\n",
        "    peft_model = get_peft_model(base_model, lora_config)\n",
        "    print(\"TEST: PEFT model created.\")\n",
        "    peft_model.print_trainable_parameters()\n",
        "\n",
        "    # Ensure PEFT model is on the correct device\n",
        "    # This explicit .to(device) might also cause issues if the PEFT model is still too big\n",
        "    # Let's REMOVE this explicit .to(device) as device_map=\"auto\" should handle device placement\n",
        "    # if device.type != 'cuda' or peft_model.device.type != device.type:\n",
        "    #      peft_model.to(device)\n",
        "    #      print(f\"TEST: PEFT model moved to device: {peft_model.device}\")\n",
        "\n",
        "    # Check the device of the PEFT model after get_peft_model\n",
        "    print(f\"TEST: PEFT model device after get_peft_model: {next(peft_model.parameters()).device}\")\n",
        "\n",
        "\n",
        "    # Wrap the PEFT model with the custom classification head\n",
        "    print(\"\\nTEST: Wrapping PEFT model with GemmaForSequenceClassification...\")\n",
        "    classification_model = GemmaForSequenceClassification(peft_model, num_labels=num_labels)\n",
        "    print(\"TEST: GemmaForSequenceClassification instance created.\")\n",
        "\n",
        "    # Ensure the final classification model is on the correct device\n",
        "    # This is where the meta tensor error is likely happening during the .to(device) call\n",
        "    print(f\"TEST: Classification model device BEFORE .to(device): {next(classification_model.parameters()).device}\")\n",
        "    classification_model.to(device) # This line is the likely source of the meta tensor error\n",
        "    print(f\"TEST: Classification model moved to device AFTER .to(device): {next(classification_model.parameters()).device}\")\n",
        "\n",
        "\n",
        "    # --- Create a Dummy Input Batch (Batch Size 1) ---\n",
        "    print(\"\\n--- Creating Dummy Input Batch ---\")\n",
        "    dummy_text = \"This is a test sentence.\"\n",
        "    # Tokenize the dummy text\n",
        "    dummy_encoding = tokenizer(\n",
        "        dummy_text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512, # Use the same max_length as the Dataset\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    # Mimic DataLoader output format with batch size 1\n",
        "    # Ensure dummy data is created on CPU first if model loading fails to avoid cuda error\n",
        "    dummy_input_ids = dummy_encoding[\"input_ids\"] # Start on CPU\n",
        "    dummy_attention_mask = dummy_encoding[\"attention_mask\"] # Start on CPU\n",
        "    dummy_labels = torch.tensor([0], dtype=torch.long) # Start on CPU\n",
        "\n",
        "    # The .to(device) call for dummy data should happen just before passing to the model's forward pass\n",
        "    # Inside the forward method of GemmaForSequenceClassification, we added device handling for inputs.\n",
        "\n",
        "    print(f\"TEST: Dummy input_ids shape (CPU): {dummy_input_ids.shape}\")\n",
        "    print(f\"TEST: Dummy attention_mask shape (CPU): {dummy_attention_mask.shape}\")\n",
        "    print(f\"TEST: Dummy labels shape (CPU): {dummy_labels.shape}\")\n",
        "\n",
        "\n",
        "    # --- Pass Dummy Batch Through the Model's Forward Method ---\n",
        "    print(\"\\n--- Running Forward Pass with Dummy Data ---\")\n",
        "    # The forward method now handles moving inputs to the correct device\n",
        "    try:\n",
        "        # Call the forward method with the dummy batch\n",
        "        # Pass all relevant inputs\n",
        "        output_dict = classification_model(\n",
        "            input_ids=dummy_input_ids,\n",
        "            attention_mask=dummy_attention_mask,\n",
        "            labels=dummy_labels # Include labels to test loss calculation path\n",
        "        )\n",
        "\n",
        "        print(\"\\n--- Forward Pass Output ---\")\n",
        "        print(f\"TEST: Output dictionary keys: {output_dict.keys()}\")\n",
        "        if 'logits' in output_dict:\n",
        "             print(f\"TEST: Logits shape: {output_dict['logits'].shape}, device: {output_dict['logits'].device}\")\n",
        "        if 'loss' in output_dict:\n",
        "             print(f\"TEST: Loss value: {output_dict['loss'].item()}, device: {output_dict['loss'].device}\")\n",
        "\n",
        "        print(\"\\n--- GemmaForSequenceClassification Test Complete ---\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"TEST: ❌ Error during forward pass test: {e}\", file=sys.stderr)\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"TEST: ❌ Error during model/PEFT loading or setup: {e}\", file=sys.stderr)\n",
        "\n",
        "print(\"--- End of GemmaForSequenceClassification Test Cell ---\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
