{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6dd23d5468f44ff99be22f7a4c037c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0897814d23c476798fbe44fd13f9b40",
              "IPY_MODEL_c814326798a844b0a465ceba73202e65",
              "IPY_MODEL_31a6ef69c4fc42c1ba04430ec159554a"
            ],
            "layout": "IPY_MODEL_acd921f303d24e189a30ef742adce4c3"
          }
        },
        "c0897814d23c476798fbe44fd13f9b40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dff38ca47d74b0aa43c5ecb2b46299f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7ba747bc0d2c4a2a8be82d8023387a0f",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "c814326798a844b0a465ceba73202e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d931b83a17914efea56777529efe0afc",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11fc9e18498e411b9ebe457f7a19ec9b",
            "value": 2
          }
        },
        "31a6ef69c4fc42c1ba04430ec159554a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85f440a789d3477b9aed8e7fe206dea8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_547ef253e5ab4da8a391dcf50d322580",
            "value": "â€‡2/2â€‡[00:02&lt;00:00,â€‡â€‡2.20s/it]"
          }
        },
        "acd921f303d24e189a30ef742adce4c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dff38ca47d74b0aa43c5ecb2b46299f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ba747bc0d2c4a2a8be82d8023387a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d931b83a17914efea56777529efe0afc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11fc9e18498e411b9ebe457f7a19ec9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85f440a789d3477b9aed8e7fe206dea8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "547ef253e5ab4da8a391dcf50d322580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰\n",
        "google/gemma-2-2b-it\n",
        "LoRA\n",
        "åˆ†é¡ã‚¿ã‚¹ã‚¯ç”¨ã‚«ã‚¹ã‚¿ãƒ ãƒ˜ãƒƒãƒ‰\n",
        "A100ä½¿ç”¨"
      ],
      "metadata": {
        "id": "YKoEcrroXhV5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6dd23d5468f44ff99be22f7a4c037c20",
            "c0897814d23c476798fbe44fd13f9b40",
            "c814326798a844b0a465ceba73202e65",
            "31a6ef69c4fc42c1ba04430ec159554a",
            "acd921f303d24e189a30ef742adce4c3",
            "1dff38ca47d74b0aa43c5ecb2b46299f",
            "7ba747bc0d2c4a2a8be82d8023387a0f",
            "d931b83a17914efea56777529efe0afc",
            "11fc9e18498e411b9ebe457f7a19ec9b",
            "85f440a789d3477b9aed8e7fe206dea8",
            "547ef253e5ab4da8a391dcf50d322580"
          ]
        },
        "id": "1oP9oFn73d9k",
        "outputId": "d0d69312-70ee-4d75-d07f-12e0a60e134e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Start of cell 1oP9oFn73d9k ---\n",
            "MathMisconceptionDataset class defined.\n",
            "custom_data_collator function defined.\n",
            "ğŸš€ MAPç«¶æŠ€ - Gemma åˆ†é¡ãƒ¢ãƒ‡ãƒ«é–‹å§‹\n",
            "============================================================\n",
            "ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ãƒã‚§ãƒƒã‚¯:\n",
            "PyTorch: 2.6.0+cu124\n",
            "CUDAåˆ©ç”¨å¯èƒ½: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "GPU ãƒ¡ãƒ¢ãƒª: 42.5 GB\n",
            "ğŸ¯ MAPç«¶æŠ€ - Gemma åˆ†é¡NLPãƒ¢ãƒ‡ãƒ«ï¼ˆã‚³ãƒ³ãƒšå½¢å¼æº–æ‹ ï¼‰\n",
            "============================================================\n",
            "============================================================\n",
            "ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ï¼ˆ6åˆ†é¡å½¢å¼ï¼‰\n",
            "============================================================\n",
            "è¨“ç·´ãƒ‡ãƒ¼ã‚¿: (36696, 7)\n",
            "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: (3, 5)\n",
            "Categoryåˆ†å¸ƒ:\n",
            "Category\n",
            "True_Correct           14802\n",
            "False_Misconception     9457\n",
            "False_Neither           6542\n",
            "True_Neither            5265\n",
            "True_Misconception       403\n",
            "False_Correct            227\n",
            "Name: count, dtype: int64\n",
            "NaNé™¤å»: 36696 -> 36696 (0è¡Œå‰Šé™¤)\n",
            "\n",
            "ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚«ãƒ†ã‚´ãƒªæ•°: 6\n",
            "ã‚«ãƒ†ã‚´ãƒªä¸€è¦§:\n",
            "  0: False_Correct (227ä»¶)\n",
            "  1: False_Misconception (9457ä»¶)\n",
            "  2: False_Neither (6542ä»¶)\n",
            "  3: True_Correct (14802ä»¶)\n",
            "  4: True_Misconception (403ä»¶)\n",
            "  5: True_Neither (5265ä»¶)\n",
            "\n",
            "============================================================\n",
            "ãƒ¢ãƒ‡ãƒ«æº–å‚™: google/gemma-2-2b-it (SequenceClassification ãƒ­ãƒ¼ãƒ‰ã€PEFTé©ç”¨ã€ãƒ‡ãƒã‚¤ã‚¹é…ç½®)\n",
            "============================================================\n",
            "ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: cuda\n",
            "google/gemma-2-2b-it ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...\n",
            "google/gemma-2-2b-it ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« (SequenceClassificationã¨ã—ã¦) èª­ã¿è¾¼ã¿ä¸­...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6dd23d5468f44ff99be22f7a4c037c20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at google/gemma-2-2b-it and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoModelForSequenceClassification ãƒ¢ãƒ‡ãƒ« (google/gemma-2-2b-it) ãƒ­ãƒ¼ãƒ‰å®Œäº†\n",
            "ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 2,614,355,712\n",
            "\n",
            "PEFT (LoRA) è¨­å®šä¸­...\n",
            "LoRA Config:\n",
            "LoraConfig(task_type='SEQ_CLS', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=16, target_modules={'gate_proj', 'v_proj', 'o_proj', 'q_proj', 'down_proj', 'up_proj', 'k_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['classifier', 'score'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PEFTãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†ã€‚\n",
            "trainable params: 20,780,544 || all params: 2,635,136,256 || trainable%: 0.7886\n",
            "PEFTãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•: cuda:0\n",
            "\n",
            "============================================================\n",
            "6åˆ†é¡ãƒ¢ãƒ‡ãƒ« ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹\n",
            "============================================================\n",
            "ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«:\n",
            "  0: False_Correct\n",
            "  1: False_Misconception\n",
            "  2: False_Neither\n",
            "  3: True_Correct\n",
            "  4: True_Misconception\n",
            "  5: True_Neither\n",
            "Stratified splité©ç”¨\n",
            "è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 29356\n",
            "æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 7340\n",
            "\n",
            "TrainingArguments:\n",
            "TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=False,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=300,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=4,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_revision=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-05,\n",
            "length_column_name=length,\n",
            "liger_kernel_config=None,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/drive/MyDrive/kaggleCompe_MAP-math/logs_gemma_classification,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=map3,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=/content/drive/MyDrive/kaggleCompe_MAP-math/gemma_classification_model,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=[],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/drive/MyDrive/kaggleCompe_MAP-math/gemma_classification_model,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=300,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=200,\n",
            "weight_decay=0.01,\n",
            ")\n",
            "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1544' max='4590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1544/4590 1:16:42 < 2:31:32, 0.34 it/s, Epoch 1.68/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Map3</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.283447</td>\n",
              "      <td>0.006131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.283447</td>\n",
              "      <td>0.006131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.283447</td>\n",
              "      <td>0.006131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.283447</td>\n",
              "      <td>0.006131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.283447</td>\n",
              "      <td>0.006131</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Entry point print for the cell\n",
        "print(\"--- Start of cell 1oP9oFn73d9k ---\")\n",
        "\n",
        "import sys # Moved import sys to the very top\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM, # Keep import for potential future use, but not used in this flow\n",
        "    AutoModelForSequenceClassification, # Import AutoModelForSequenceClassification\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding, # Keep import for reference, but use custom collator\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from peft import LoraConfig, get_peft_model # Ensure PEFT is imported and used\n",
        "import warnings\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Re-define the Dataset class to ensure it's available\n",
        "class MathMisconceptionDataset(Dataset):\n",
        "    \"\"\"Math Misconception Dataset for PyTorch\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Return tensors with an explicit batch dimension of 1\n",
        "        # This is important for custom collation, and generally good practice\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"], # Shape [1, max_length]\n",
        "            \"attention_mask\": encoding[\"attention_mask\"], # Shape [1, max_length]\n",
        "            \"labels\": torch.tensor([label], dtype=torch.long), # Shape [1]\n",
        "        }\n",
        "print(\"MathMisconceptionDataset class defined.\")\n",
        "\n",
        "# Custom data collator function (Modified slightly for clarity, but logic is similar)\n",
        "# This collator will stack the tensors, resulting in shapes [batch_size, ...]\n",
        "def custom_data_collator(features):\n",
        "    \"\"\"Custom data collator to handle batching.\"\"\"\n",
        "    if not isinstance(features, list):\n",
        "        features = [features]\n",
        "\n",
        "    # Stack tensors from the list along the batch dimension (dim=0)\n",
        "    input_ids = torch.cat([f[\"input_ids\"] for f in features], dim=0)\n",
        "    attention_mask = torch.cat([f[\"attention_mask\"] for f in features], dim=0)\n",
        "    labels = torch.cat([f[\"labels\"] for f in features], dim=0)\n",
        "\n",
        "    # Ensure labels are 1D for CrossEntropyLoss (shape [batch_size])\n",
        "    if labels.ndim > 1:\n",
        "         labels = labels.squeeze(1) # Remove dimension if it's [batch_size, 1]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "print(\"custom_data_collator function defined.\")\n",
        "\n",
        "\n",
        "# Re-define load_and_prepare_data\n",
        "def load_and_prepare_data():\n",
        "    \"\"\"ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ï¼ˆ6åˆ†é¡å½¢å¼ï¼‰\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ï¼ˆ6åˆ†é¡å½¢å¼ï¼‰\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Define the data paths on Google Drive\n",
        "    train_data_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/map_data/train.csv\"\n",
        "    test_data_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/map_data/test.csv\"\n",
        "\n",
        "    try:\n",
        "        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
        "        train_df = pd.read_csv(train_data_path)\n",
        "        test_df = pd.read_csv(test_data_path)\n",
        "\n",
        "        print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train_df.shape}\")\n",
        "        print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test_df.shape}\")\n",
        "\n",
        "        # ã‚³ãƒ³ãƒšå½¢å¼ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆï¼ˆ6åˆ†é¡ï¼‰\n",
        "        print(\"Categoryåˆ†å¸ƒ:\")\n",
        "        print(train_df[\"Category\"].value_counts())\n",
        "\n",
        "        # NaNå€¤é™¤å»\n",
        "        before_len = len(train_df)\n",
        "        train_df = train_df.dropna(subset=[\"Category\", \"StudentExplanation\"])\n",
        "        after_len = len(train_df)\n",
        "        print(f\"NaNé™¤å»: {before_len} -> {after_len} ({before_len - after_len}è¡Œå‰Šé™¤)\")\n",
        "\n",
        "        # ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ä½œæˆ\n",
        "        def create_enhanced_text(row):\n",
        "            \"\"\"å¼·åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã‚’ä½œæˆ\"\"\"\n",
        "            question = str(row[\"QuestionText\"]) if pd.notna(row[\"QuestionText\"]) else \"\"\n",
        "            mc_answer = str(row[\"MC_Answer\"]) if pd.notna(row[\"MC_Answer\"]) else \"\"\n",
        "            explanation = (\n",
        "                str(row[\"StudentExplanation\"])\n",
        "                if pd.notna(row[\"StudentExplanation\"])\n",
        "                else \"\"\n",
        "            )\n",
        "\n",
        "            # è³ªå•ã€é¸æŠã•ã‚ŒãŸç­”ãˆã€èª¬æ˜ã‚’çµåˆ\n",
        "            enhanced_text = f\"Question: {question} Selected Answer: {mc_answer} Explanation: {explanation}\"\n",
        "            return enhanced_text\n",
        "\n",
        "        train_df[\"enhanced_text\"] = train_df.apply(create_enhanced_text, axis=1)\n",
        "        test_df[\"enhanced_text\"] = test_df.apply(create_enhanced_text, axis=1)\n",
        "\n",
        "        # 6ã¤ã®ã‚«ãƒ†ã‚´ãƒªã®ç¢ºèª\n",
        "        unique_categories = sorted(train_df[\"Category\"].unique())\n",
        "        print(f\"\\nãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚«ãƒ†ã‚´ãƒªæ•°: {len(unique_categories)}\")\n",
        "        print(\"ã‚«ãƒ†ã‚´ãƒªä¸€è¦§:\")\n",
        "        for i, cat in enumerate(unique_categories):\n",
        "            count = (train_df[\"Category\"] == cat).sum()\n",
        "            print(f\"  {i}: {cat} ({count}ä»¶)\")\n",
        "\n",
        "        return train_df, test_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¾ãŸã¯å‰å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\", file=sys.stderr)\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# Remove the custom GemmaForSequenceClassification class definition\n",
        "# class GemmaForSequenceClassification(nn.Module):\n",
        "#     def __init__(self, base_model, num_labels):\n",
        "#         ...\n",
        "#     def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "#         ...\n",
        "# Removed the entire class definition\n",
        "\n",
        "\n",
        "# New prepare_model function using AutoModelForSequenceClassification and applying PEFT\n",
        "def prepare_model_for_classification(num_labels=6, model_name=\"google/gemma-2-2b-it\"): # Changed model name to google/gemma-2-2b-it\n",
        "    \"\"\"AutoModelForSequenceClassification ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰ã€PEFTé©ç”¨ã€ãƒ‡ãƒã‚¤ã‚¹é…ç½®\"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 60)\n",
        "    print(f\"ãƒ¢ãƒ‡ãƒ«æº–å‚™: {model_name} (SequenceClassification ãƒ­ãƒ¼ãƒ‰ã€PEFTé©ç”¨ã€ãƒ‡ãƒã‚¤ã‚¹é…ç½®)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"{model_name} ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "            print(f\"ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ '{tokenizer.pad_token}' ã«è¨­å®šã—ã¾ã—ãŸã€‚\")\n",
        "        # Add a classification token if the model doesn't have one (common for CausalLMs fine-tuned for classification)\n",
        "        # Note: Gemma typically uses EOS as a classification token, check tokenizer details if needed.\n",
        "\n",
        "\n",
        "        print(f\"{model_name} ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« (SequenceClassificationã¨ã—ã¦) èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "        # Use AutoModelForSequenceClassification directly\n",
        "        # Pass num_labels here, the model will automatically add a classification head\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=num_labels,\n",
        "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if device.type == \"cuda\" else None,\n",
        "            low_cpu_mem_usage=True,\n",
        "            # Add pad_token_id if necessary, usually matches eos_token_id\n",
        "            pad_token_id=tokenizer.eos_token_id if tokenizer.pad_token_id is None else tokenizer.pad_token_id,\n",
        "        )\n",
        "\n",
        "        print(f\"AutoModelForSequenceClassification ãƒ¢ãƒ‡ãƒ« ({model_name}) ãƒ­ãƒ¼ãƒ‰å®Œäº†\")\n",
        "        print(f\"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model.num_parameters():,}\")\n",
        "\n",
        "\n",
        "        # Configure PEFT (LoRA)\n",
        "        print(\"\\nPEFT (LoRA) è¨­å®šä¸­...\")\n",
        "        # Adjust LoRA config parameters as needed\n",
        "        lora_config = LoraConfig(\n",
        "            r=16, # LoRA attention dimension\n",
        "            lora_alpha=32, # Alpha parameter for LoRA scaling\n",
        "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # Added common linear layers for LoRA\n",
        "            lora_dropout=0.05, # Dropout probability for LoRA layers\n",
        "            bias=\"none\", # Bias type\n",
        "            task_type=\"SEQ_CLS\", # Specify task type for Sequence Classification\n",
        "            modules_to_save=[\"classifier\", \"score\"], # Save the classification head and potentially the scoring layer if it exists\n",
        "        )\n",
        "        print(\"LoRA Config:\")\n",
        "        print(lora_config)\n",
        "\n",
        "        # Apply PEFT to the base model\n",
        "        model = get_peft_model(model, lora_config)\n",
        "        print(\"PEFTãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†ã€‚\")\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "        # Move the PEFT model to the correct device\n",
        "        # device_map=\"auto\" in from_pretrained might handle this, but explicit to() is safer\n",
        "        model.to(device)\n",
        "        print(f\"PEFTãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•: {next(model.parameters()).device}\")\n",
        "\n",
        "\n",
        "        return model, tokenizer, device\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿ã€PEFTé©ç”¨ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\", file=sys.stderr)\n",
        "        print(\"\\nè€ƒãˆã‚‰ã‚Œã‚‹åŸå› :\", file=sys.stderr)\n",
        "        print(\"1. Hugging Face Hub ã¸ã®èªè¨¼ãŒå¿…è¦\", file=sys.stderr)\n",
        "        print(\"2. Internetæ¥ç¶šã®å•é¡Œ\", file=sys.stderr) # Corrected typo\n",
        "        print(\"3. GPU ãƒ¡ãƒ¢ãƒªä¸è¶³\", file=sys.stderr)\n",
        "        print(\"4. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å•é¡Œ (transformers, peft, torch)\", file=sys.stderr)\n",
        "        print(\"5. ãƒ¢ãƒ‡ãƒ«åã®èª¤ã‚Š\", file=sys.stderr)\n",
        "        print(\"\\nè§£æ±ºæ–¹æ³•:\", file=sys.stderr)\n",
        "        print(\"- huggingface-cli login ã§ãƒ­ã‚°ã‚¤ãƒ³\", file=sys.stderr)\n",
        "        print(\"- Internetæ¥ç¶šç¢ºèª\", file=sys.stderr) # Corrected typo\n",
        "        print(\"- GPU ãƒ¡ãƒ¢ãƒªç©ºãå®¹é‡ç¢ºèª\")\n",
        "        print(\"- ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ (pip install -U transformers peft accelerate)\")\n",
        "        print(\"- ãƒ¢ãƒ‡ãƒ«åã‚’ç¢ºèª (ä¾‹: google/gemma-2-2b-it)\", file=sys.stderr)\n",
        "        print(\"\\nãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚\", file=sys.stderr)\n",
        "        return None, None, None\n",
        "\n",
        "\n",
        "# Re-define compute_map3_metrics and other functions\n",
        "def compute_map3_metrics(eval_pred):\n",
        "    \"\"\"MAP@3ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = torch.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
        "\n",
        "    map_scores = []\n",
        "    for i, true_label in enumerate(labels):\n",
        "        top3_indices = np.argsort(predictions[i])[::-1][:3]\n",
        "        score = 0.0\n",
        "        for j, pred_idx in enumerate(top3_indices):\n",
        "            if pred_idx == true_label:\n",
        "                score = 1.0 / (j + 1)\n",
        "                break\n",
        "        map_scores.append(score)\n",
        "\n",
        "    map3_score = np.mean(map_scores)\n",
        "    accuracy = accuracy_score(labels, np.argmax(predictions, axis=1))\n",
        "\n",
        "    return {\"map3\": map3_score, \"accuracy\": accuracy}\n",
        "\n",
        "def fine_tune_model(train_df, model, tokenizer, device):\n",
        "    \"\"\"ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 60)\n",
        "    print(\"6åˆ†é¡ãƒ¢ãƒ‡ãƒ« ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹\")\n",
        "    print(\"=\" * 60)\n",
        "    # print(\"--- Inside fine_tune_model ---\") # Removed debug print\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    train_df[\"encoded_labels\"] = label_encoder.fit_transform(train_df[\"Category\"])\n",
        "\n",
        "    print(f\"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«:\")\n",
        "    for i, label in enumerate(label_encoder.classes_):\n",
        "        print(f\"  {i}: {label}\")\n",
        "\n",
        "    train_texts = train_df[\"enhanced_text\"].tolist()\n",
        "    train_labels = train_df[\"encoded_labels\"].tolist()\n",
        "\n",
        "    try:\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            train_texts,\n",
        "            train_labels,\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=train_labels,\n",
        "        )\n",
        "        print(\"Stratified splité©ç”¨\")\n",
        "    except ValueError:\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            train_texts, train_labels, test_size=0.2, random_state=42\n",
        "        )\n",
        "        print(\"Regular splité©ç”¨ï¼ˆå°‘æ•°ã‚¯ãƒ©ã‚¹ã®ãŸã‚ï¼‰\")\n",
        "\n",
        "    print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}\")\n",
        "    print(f\"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val)}\")\n",
        "\n",
        "    train_dataset = MathMisconceptionDataset(\n",
        "        X_train, y_train, tokenizer, max_length=512\n",
        "    )\n",
        "    val_dataset = MathMisconceptionDataset(X_val, y_val, tokenizer, max_length=512)\n",
        "    # print(\"train_dataset and val_dataset created.\") # Removed debug print\n",
        "\n",
        "    # Use the custom data collator\n",
        "    data_collator = custom_data_collator\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"/content/drive/MyDrive/kaggleCompe_MAP-math/gemma_classification_model\", # Updated output dir name\n",
        "        num_train_epochs=5,\n",
        "        per_device_train_batch_size=8, # Changed batch size back to 8 as requested\n",
        "        per_device_eval_batch_size=8, # Changed batch size back to 8\n",
        "        gradient_accumulation_steps=4, # Adjusted accumulation to 4 to keep effective batch size 32\n",
        "        warmup_steps=200,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"/content/drive/MyDrive/kaggleCompe_MAP-math/logs_gemma_classification\", # Updated log dir name\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=300,\n",
        "        save_strategy=\"steps\", # Match eval_strategy for load_best_model_at_end\n",
        "        save_steps=300, # Save checkpoints when evaluating\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"map3\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\",\n",
        "        dataloader_pin_memory=False,\n",
        "        fp16=False, # Keep FP16 disabled for potential stability\n",
        "        bf16=True, # Keep BF16 enabled (if supported)\n",
        "        optim=\"adamw_torch\",\n",
        "        learning_rate=1e-5,\n",
        "        save_total_limit=2,\n",
        "        gradient_checkpointing=False, # Keep gradient checkpointing disabled\n",
        "        # gradient_checkpointing_kwargs={'use_reentrant': False}, # Remove kwargs as gradient checkpointing is False\n",
        "    )\n",
        "\n",
        "    print(\"\\nTrainingArguments:\")\n",
        "    print(training_args)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator, # Use custom collator\n",
        "        compute_metrics=compute_map3_metrics,\n",
        "    )\n",
        "\n",
        "    print(\"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...\")\n",
        "    try:\n",
        "        trainer.train()\n",
        "        print(\"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ã€‚\")\n",
        "        # Explicitly save the model after training\n",
        "        save_path = training_args.output_dir\n",
        "        print(f\"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­: {save_path}\")\n",
        "        trainer.save_model(save_path)\n",
        "        print(\"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†ã€‚\")\n",
        "\n",
        "        if trainer is None:\n",
        "             print(\"Warning: trainer is None after trainer.train()\", file=sys.stderr)\n",
        "        else:\n",
        "             print(\"trainer object is valid after trainer.train().\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\", file=sys.stderr)\n",
        "        return None, None # Return None on failure\n",
        "\n",
        "    print(\"\\næœ€çµ‚è©•ä¾¡:\")\n",
        "    try:\n",
        "        eval_results = trainer.evaluate()\n",
        "        for key, value in eval_results.items():\n",
        "            print(f\"{key}: {value:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ æœ€çµ‚è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\", file=sys.stderr)\n",
        "        return None, None\n",
        "\n",
        "\n",
        "    return trainer, label_encoder\n",
        "\n",
        "def generate_submission(trainer, tokenizer, test_df, label_encoder):\n",
        "    \"\"\"ã‚³ãƒ³ãƒšå½¢å¼ã®æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ\"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 60)\n",
        "    print(\"ã‚³ãƒ³ãƒšå½¢å¼æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if trainer is None:\n",
        "         print(\"Error: Trainer object is None, cannot generate submission.\", file=sys.stderr)\n",
        "         return None\n",
        "\n",
        "    test_texts = test_df[\"enhanced_text\"].tolist()\n",
        "    test_dataset = MathMisconceptionDataset(\n",
        "        test_texts, [0] * len(test_texts), tokenizer, max_length=512\n",
        "    )\n",
        "\n",
        "    print(\"äºˆæ¸¬å®Ÿè¡Œä¸­...\")\n",
        "    try:\n",
        "        # Use the same data collator for prediction\n",
        "        data_collator = custom_data_collator\n",
        "        # Trainer's predict method automatically uses the data collator from TrainingArguments\n",
        "        # or can be passed explicitly, but it usually works fine with the one set during init.\n",
        "        # However, explicitly setting it here for clarity if needed, though Trainer should use self.data_collator\n",
        "        # predictions = trainer.predict(test_dataset, data_collator=data_collator)\n",
        "\n",
        "        predictions = trainer.predict(test_dataset) # Trainer should use the data_collator set during init\n",
        "\n",
        "\n",
        "        print(\"äºˆæ¸¬å®Œäº†ã€‚\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ äºˆæ¸¬ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "\n",
        "    print(\"ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é©ç”¨ä¸­...\")\n",
        "    try:\n",
        "        # predictions.predictions contains the raw logits\n",
        "        probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1).numpy()\n",
        "        print(\"ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é©ç”¨å®Œäº†ã€‚\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é©ç”¨ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "\n",
        "    # å„ã‚µãƒ³ãƒ—ãƒ«ã§ä¸Šä½3ã¤ã®äºˆæ¸¬ã‚’å–å¾—\n",
        "    submission_predictions = []\n",
        "    print(\"ä¸Šä½3ã¤ã®äºˆæ¸¬ã‚’å–å¾—ä¸­...\")\n",
        "    try:\n",
        "        for prob in probs:\n",
        "            top3_indices = np.argsort(prob)[::-1][:3]\n",
        "            top3_labels = [label_encoder.classes_[idx] for idx in top3_indices]\n",
        "            submission_predictions.append(\" \".join(top3_labels))\n",
        "        print(\"ä¸Šä½3ã¤ã®äºˆæ¸¬å–å¾—å®Œäº†ã€‚\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ä¸Šä½3ã¤ã®äºˆæ¸¬å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "\n",
        "    # æå‡ºå½¢å¼ã®DataFrameä½œæˆ\n",
        "    print(\"æå‡ºå½¢å¼DataFrameä½œæˆä¸­...\")\n",
        "    try:\n",
        "        submission_df = pd.DataFrame(\n",
        "            {\n",
        "                \"row_id\": range(len(test_df)),\n",
        "                \"Category:Misconception\": submission_predictions,\n",
        "            }\n",
        "        )\n",
        "        print(\"æå‡ºå½¢å¼DataFrameä½œæˆå®Œäº†ã€‚\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ æå‡ºå½¢å¼DataFrameä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "\n",
        "    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\n",
        "    print(\"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚µãƒ³ãƒ—ãƒ«:\")\n",
        "    print(submission_df.head(10))\n",
        "\n",
        "    print(f\"äºˆæ¸¬å®Œäº†: {len(submission_df)}è¡Œ\")\n",
        "    return submission_df\n",
        "\n",
        "\n",
        "# Update the main function to run the full pipeline and check return values\n",
        "def main():\n",
        "    \"\"\"ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°\"\"\"\n",
        "    print(\"ğŸ¯ MAPç«¶æŠ€ - Gemma åˆ†é¡NLPãƒ¢ãƒ‡ãƒ«ï¼ˆã‚³ãƒ³ãƒšå½¢å¼æº–æ‹ ï¼‰\") # Updated title\n",
        "    print(\"=\" * 60)\n",
        "    # print(\"--- Inside main ---\") # Removed debug print\n",
        "\n",
        "    train_df, test_df = load_and_prepare_data()\n",
        "    if train_df is None or test_df is None:\n",
        "         print(\"ãƒ‡ãƒ¼ã‚¿æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚\", file=sys.stderr)\n",
        "         return None, None, None\n",
        "\n",
        "    # Use the new prepare_model_for_classification function\n",
        "    # We don't need the global peft_model, tokenizer, device from the old cell 1713c1f2 anymore\n",
        "    # The new function loads everything needed internally\n",
        "    # global peft_model, tokenizer, device # Remove access to old global variables\n",
        "\n",
        "\n",
        "    # Prepare the model using the new function\n",
        "    # We pass the number of labels and the desired model name\n",
        "    # model, tokenizer, device = prepare_model(num_labels=6, model_name=\"google/gemma-3-1b-it\") # Removed old call\n",
        "    model, tokenizer, device = prepare_model_for_classification(num_labels=6, model_name=\"google/gemma-2-2b-it\") # Call the new function\n",
        "\n",
        "\n",
        "    if model is None or tokenizer is None or device is None:\n",
        "        print(\"ãƒ¢ãƒ‡ãƒ«æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚\", file=sys.stderr) # Simplified error message\n",
        "        return None, None, None\n",
        "\n",
        "\n",
        "    trainer, label_encoder = fine_tune_model(train_df, model, tokenizer, device)\n",
        "    # Check if fine_tune_model returned a valid trainer\n",
        "    if trainer is None and label_encoder is None: # Check for both None\n",
        "         print(\"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚\", file=sys.stderr)\n",
        "         return None, None, None\n",
        "\n",
        "\n",
        "    submission_df = generate_submission(trainer, tokenizer, test_df, label_encoder)\n",
        "    # Check if generate_submission returned a valid DataFrame\n",
        "    if submission_df is None:\n",
        "         print(\"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚\", file=sys.stderr)\n",
        "         return None, None, None\n",
        "\n",
        "\n",
        "    submission_file_path = \"/content/drive/MyDrive/kaggleCompe_MAP-math/gemma_classification_submission.csv\" # Updated submission file name\n",
        "    print(f\"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {submission_file_path}\")\n",
        "    try:\n",
        "        submission_df.to_csv(submission_file_path, index=False)\n",
        "        print(f\"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {submission_file_path}\")\n",
        "    except Exception as e:\n",
        "         print(f\"âŒ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\", file=sys.stderr)\n",
        "         return None, None, None\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"6åˆ†é¡ãƒ¢ãƒ‡ãƒ«é–‹ç™ºå®Œäº†ï¼\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    return trainer, label_encoder, submission_df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ğŸš€ MAPç«¶æŠ€ - Gemma åˆ†é¡ãƒ¢ãƒ‡ãƒ«é–‹å§‹\") # Updated title\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(\"ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ãƒã‚§ãƒƒã‚¯:\")\n",
        "    print(f\"PyTorch: {torch.__version__}\")\n",
        "    print(f\"CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(\n",
        "            f\"GPU ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        # Execute the full main pipeline\n",
        "        trainer, label_encoder, submission_df = main()\n",
        "        if trainer is not None and label_encoder is not None and submission_df is not None:\n",
        "             print(\"\\nğŸ‰ Gemmaãƒ¢ãƒ‡ãƒ«é–‹ç™ºå®Œäº†ï¼\")\n",
        "        else:\n",
        "             print(\"\\nâŒ ãƒ¢ãƒ‡ãƒ«é–‹ç™ºãƒ—ãƒ­ã‚»ã‚¹ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\")\n",
        "\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nâ¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ\")\n",
        "        pass\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\", file=sys.stderr)\n",
        "        print(\"\\nGemmaãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œã«ã¯ä»¥ä¸‹ãŒå¿…è¦ã§ã™:\", file=sys.stderr)\n",
        "        print(\"1. ååˆ†ãªGPUãƒ¡ãƒ¢ãƒª (æ¨å¥¨: 8GBä»¥ä¸Š)\", file=sys.stderr)\n",
        "        print(\"2. Hugging Face Hubèªè¨¼ (huggingface-cli login)\", file=sys.stderr)\n",
        "        print(\"3. å®‰å®šã—ãŸInternetæ¥ç¶š\", file=sys.stderr)\n",
        "        # Removed specific Gemma 3N E4B compatibility note as we are switching models\n",
        "        print(\"\\nãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚\", file=sys.stderr)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "æœ€åˆã«ã‚„ã‚‹ã‚„ã¤"
      ],
      "metadata": {
        "id": "XNC2ARUSW5pa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "58cebf22"
      },
      "source": [
        "!huggingface-cli login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "09024185"
      },
      "source": [
        "!pip install peft accelerate -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03a216d9"
      },
      "source": [
        "!pip install timm -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fbb5455d"
      },
      "source": [
        "try:\n",
        "    import timm\n",
        "    print(\"timm library imported successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Error: timm library could not be imported.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemma3nã§ã‚„ã‚ã†ã¨ã—ã¦ãƒ¡ãƒ¢ãƒªãŒè¶³ã‚Šãªãã¦å¤±æ•—ã—ãŸã‚‚ã®"
      ],
      "metadata": {
        "id": "vcksfKyw5Q1J"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37164cfb"
      },
      "source": [
        "# --- Test Cell for GemmaForSequenceClassification ---\n",
        "print(\"--- Start of GemmaForSequenceClassification Test Cell ---\")\n",
        "\n",
        "import sys # Moved import sys to the very top\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM # Need CausalLM to load the base model\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset/DataLoader just in case needed, though dummy data is used\n",
        "\n",
        "# Define the GemmaForSequenceClassification class (copied from 1oP9oFn73d9k)\n",
        "# Add print statements inside the forward method for debugging shapes\n",
        "class GemmaForSequenceClassification(nn.Module):\n",
        "    def __init__(self, base_model, num_labels):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        # Access the hidden size from the text_config attribute\n",
        "        hidden_size = None\n",
        "        config = base_model.config # Start with the model's config\n",
        "\n",
        "        # If it's a PEFT model, try accessing the base model's config\n",
        "        if hasattr(base_model, 'base_model') and hasattr(base_model.base_model, 'config'):\n",
        "             config = base_model.base_model.config\n",
        "             print(f\"TEST: Accessing config from base_model.base_model.config ({type(config)})\")\n",
        "        else:\n",
        "             print(f\"TEST: Accessing config from base_model.config ({type(config)})\")\n",
        "\n",
        "        try:\n",
        "            # Try common attribute names for hidden size sequentially\n",
        "            print(\"TEST: Attempting to find hidden size using common attribute names...\")\n",
        "            if hasattr(config, 'hidden_size') and config.hidden_size is not None:\n",
        "                 hidden_size = config.hidden_size\n",
        "                 print(f\"TEST: Attempted config.hidden_size, Found: {hidden_size}\")\n",
        "            elif hasattr(config, 'd_model') and config.d_model is not None:\n",
        "                 hidden_size = config.d_model\n",
        "                 print(f\"TEST: Attempted config.d_model, Found: {hidden_size}\")\n",
        "            elif hasattr(config, 'n_embd') and config.n_embd is not None:\n",
        "                 hidden_size = config.n_embd\n",
        "                 print(f\"TEST: Attempted config.n_embd, Found: {hidden_size}\")\n",
        "            elif hasattr(config, 'text_config') and hasattr(config.text_config, 'hidden_size') and config.text_config.hidden_size is not None:\n",
        "                 hidden_size = config.text_config.hidden_size\n",
        "                 print(f\"TEST: Attempted config.text_config.hidden_size, Found: {hidden_size}\")\n",
        "            else:\n",
        "                 checked_attributes = ['hidden_size', 'd_model', 'n_embd', 'text_config.hidden_size']\n",
        "                 print(f\"TEST: Checked attributes: {', '.join(checked_attributes)} - None found.\")\n",
        "                 raise AttributeError(f\"Could not find hidden size attribute using common names in {type(config)}. Checked: {', '.join(checked_attributes)}\")\n",
        "\n",
        "            if hidden_size is None:\n",
        "                 raise AttributeError(\"Hidden size attribute found but its value is None.\")\n",
        "\n",
        "        except AttributeError as ae:\n",
        "            print(f\"TEST: âŒ Error accessing hidden size in GemmaForSequenceClassification init: {ae}\")\n",
        "            raise ae\n",
        "\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "        print(f\"TEST: Classification head created with input size: {hidden_size}\")\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        print(\"-\" * 20 + \" TEST: Forward Pass Debug (Start)\" + \"-\" * 20)\n",
        "        print(f\"TEST: input_ids shape (start): {input_ids.shape if input_ids is not None else 'None'}\")\n",
        "        print(f\"TEST: attention_mask shape (start): {attention_mask.shape if attention_mask is not None else 'None'}\")\n",
        "        print(f\"TEST: labels shape (start): {labels.shape if labels is not None else 'None'}\")\n",
        "\n",
        "        # Pass input through the PEFT model\n",
        "        # Use **kwargs to pass other arguments like labels if the base_model accepts them\n",
        "        # although for classification, we primarily care about hidden states/logits\n",
        "        model_inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"output_hidden_states\": True}\n",
        "        # Add labels if they exist and the base model's forward accepts them (GemmaForCausalLM might not directly use labels in its forward, but PEFT wrapper might pass it)\n",
        "        if labels is not None:\n",
        "             model_inputs['labels'] = labels # Pass labels to the base model forward call if it's configured to handle them\n",
        "\n",
        "        # Ensure inputs are on the correct device\n",
        "        device = next(self.parameters()).device\n",
        "        model_inputs = {k: v.to(device) for k, v in model_inputs.items() if isinstance(v, torch.Tensor)}\n",
        "        print(f\"TEST: Model inputs moved to device: {device}\")\n",
        "\n",
        "\n",
        "        outputs = self.base_model(**model_inputs)\n",
        "\n",
        "        print(\"-\" * 20 + \" TEST: Forward Pass Debug (After Base Model)\" + \"-\" * 20)\n",
        "        print(f\"TEST: Base model outputs type: {type(outputs)}\")\n",
        "        # Print keys if outputs is a dictionary or object with keys\n",
        "        if hasattr(outputs, 'keys'):\n",
        "             print(f\"TEST: Base model outputs keys: {outputs.keys()}\")\n",
        "\n",
        "\n",
        "        # Access the last hidden state from the outputs\n",
        "        last_hidden_state = None\n",
        "        if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
        "             # outputs.hidden_states is a tuple, the last element is the hidden states from the last layer\n",
        "             last_hidden_state = outputs.hidden_states[-1] # Shape: (batch_size, sequence_length, hidden_size)\n",
        "             print(f\"TEST: last_hidden_state shape (from hidden_states): {last_hidden_state.shape}\")\n",
        "        elif hasattr(outputs, 'last_hidden_state') and outputs.last_hidden_state is not None: # Some models use last_hidden_state directly\n",
        "             last_hidden_state = outputs.last_hidden_state\n",
        "             print(f\"TEST: last_hidden_state shape (from last_hidden_state): {last_hidden_state.shape}\")\n",
        "        else:\n",
        "             # If hidden_states is not available, try accessing the last layer output directly if possible\n",
        "             # This is less standard for CausalLM outputs, but as a fallback\n",
        "             print(\"TEST: Warning: hidden_states or last_hidden_state not found in base model output. Attempting direct access.\")\n",
        "             try:\n",
        "                 # This might vary depending on the output structure\n",
        "                 if isinstance(outputs, torch.Tensor) and outputs.ndim == 3:\n",
        "                      last_hidden_state = outputs # Assuming the output itself is the last hidden state\n",
        "                      print(f\"TEST: Accessed last hidden state directly from outputs (Tensor): {last_hidden_state.shape}\")\n",
        "                 elif isinstance(outputs, (list, tuple)) and len(outputs) > 0 and isinstance(outputs[0], torch.Tensor) and outputs[0].ndim == 3:\n",
        "                     last_hidden_state = outputs[0] # Assuming the first element is the last hidden state\n",
        "                     print(f\"TEST: Accessed last hidden state from outputs[0] (Tensor): {last_hidden_state.shape}\")\n",
        "                 else:\n",
        "                      print(f\"TEST: Could not find a 3D tensor in outputs: {outputs}\")\n",
        "                      raise AttributeError(\"Could not find a suitable tensor for last hidden state.\")\n",
        "             except Exception as e:\n",
        "                 print(f\"TEST: âŒ Error accessing last hidden state from base model output: {e}\")\n",
        "                 # Still raise the error if we can't get the hidden state\n",
        "                 raise AttributeError(\"Could not get last hidden state from base model output.\")\n",
        "\n",
        "\n",
        "        if last_hidden_state is None:\n",
        "             raise RuntimeError(\"Failed to obtain last hidden state from the base model output.\")\n",
        "\n",
        "\n",
        "        # Take the hidden state corresponding to the last token in the sequence\n",
        "        # Note: For classification, it's common to use the [CLS] token output or the last token output.\n",
        "        # For CausalLM fine-tuning for classification, the last token is used.\n",
        "        print(f\"TEST: last_hidden_state device before slicing: {last_hidden_state.device}\")\n",
        "        sequence_output = last_hidden_state[:, -1, :] # Shape: (batch_size, hidden_size)\n",
        "        print(f\"TEST: sequence_output shape (last token): {sequence_output.shape}\")\n",
        "\n",
        "\n",
        "        # Pass the hidden states through the classifier\n",
        "        print(f\"TEST: sequence_output device before classifier: {sequence_output.device}\")\n",
        "        print(f\"TEST: Classifier layer device: {self.classifier.weight.device}\")\n",
        "        logits = self.classifier(sequence_output) # Shape: (batch_size, num_labels)\n",
        "        print(f\"TEST: logits shape (after classifier): {logits.shape}\")\n",
        "\n",
        "\n",
        "        # Calculate loss internally (as before)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            # Ensure logits and labels are on the same device before loss calculation\n",
        "            # This is a potential source of subtle bugs\n",
        "            if logits.device != labels.device:\n",
        "                 print(f\"TEST: Warning: logits ({logits.device}) and labels ({labels.device}) are on different devices. Moving logits to labels device.\")\n",
        "                 logits = logits.to(labels.device)\n",
        "\n",
        "            print(\"-\" * 20 + \" TEST: Forward Pass Debug (Before Loss)\" + \"-\" * 20)\n",
        "            print(f\"TEST: logits shape (before loss call): {logits.shape}\")\n",
        "            print(f\"TEST: labels shape (before loss call): {labels.shape if labels is not None else 'None'}\")\n",
        "            if labels is not None:\n",
        "                 print(f\"TEST: logits device (before loss call): {logits.device}, labels device (before loss call): {labels.device}\")\n",
        "            print(\"-\" * 20 + \" TEST: Forward Pass Debug (End)\" + \"-\" * 20)\n",
        "\n",
        "\n",
        "            loss = loss_fct(logits, labels) # This is where the error might happen\n",
        "            print(f\"TEST: Loss calculated: {loss.item()}\")\n",
        "\n",
        "\n",
        "        # Return logits and loss for the Trainer\n",
        "        return {\"logits\": logits, \"loss\": loss}\n",
        "\n",
        "\n",
        "# --- Model Loading and PEFT Application ---\n",
        "print(\"\\n--- Loading Model and Applying PEFT ---\")\n",
        "model_name = \"google/gemma-3n-E4B-it\" # Use the model that was causing issues\n",
        "num_labels = 6 # Assuming 6 classes\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"TEST: Using device: {device}\")\n",
        "\n",
        "try:\n",
        "    print(f\"TEST: Loading tokenizer for {model_name}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(f\"TEST: Setting padding token to '{tokenizer.pad_token}'.\")\n",
        "\n",
        "    print(f\"TEST: Loading base model (CausalLM) {model_name}...\")\n",
        "    # Use torch_dtype=torch.float16 and device_map=\"auto\" to handle potentially large model\n",
        "    # The meta tensor error suggests the model is still too big even with auto device map\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16, # Keep float16 for memory efficiency\n",
        "        device_map=\"auto\", # Attempt auto device mapping\n",
        "        low_cpu_mem_usage=True,\n",
        "    )\n",
        "    # Ensure the base model is on the correct device if device_map was not 'auto' or if running on CPU\n",
        "    # This explicit .to(device) might be the source of the meta tensor error if device_map failed\n",
        "    # Let's REMOVE this explicit .to(device) after from_pretrained with device_map=\"auto\"\n",
        "    # if device.type != 'cuda' or base_model.device.type != device.type:\n",
        "    #      base_model.to(device)\n",
        "    #      print(f\"TEST: Base model moved to device: {base_model.device}\")\n",
        "\n",
        "\n",
        "    # PEFT (LoRA) configuration\n",
        "    print(\"\\nTEST: Configuring PEFT (LoRA)...\")\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\", # Keep CAUSAL_LM task type for base model PEFT\n",
        "        # modules_to_save should NOT include classifier here, as it's in the wrapper\n",
        "    )\n",
        "    print(\"TEST: LoRA Config:\")\n",
        "    print(lora_config)\n",
        "\n",
        "    # Apply PEFT to the base model\n",
        "    peft_model = get_peft_model(base_model, lora_config)\n",
        "    print(\"TEST: PEFT model created.\")\n",
        "    peft_model.print_trainable_parameters()\n",
        "\n",
        "    # Ensure PEFT model is on the correct device\n",
        "    # This explicit .to(device) might also cause issues if the PEFT model is still too big\n",
        "    # Let's REMOVE this explicit .to(device) as device_map=\"auto\" should handle device placement\n",
        "    # if device.type != 'cuda' or peft_model.device.type != device.type:\n",
        "    #      peft_model.to(device)\n",
        "    #      print(f\"TEST: PEFT model moved to device: {peft_model.device}\")\n",
        "\n",
        "    # Check the device of the PEFT model after get_peft_model\n",
        "    print(f\"TEST: PEFT model device after get_peft_model: {next(peft_model.parameters()).device}\")\n",
        "\n",
        "\n",
        "    # Wrap the PEFT model with the custom classification head\n",
        "    print(\"\\nTEST: Wrapping PEFT model with GemmaForSequenceClassification...\")\n",
        "    classification_model = GemmaForSequenceClassification(peft_model, num_labels=num_labels)\n",
        "    print(\"TEST: GemmaForSequenceClassification instance created.\")\n",
        "\n",
        "    # Ensure the final classification model is on the correct device\n",
        "    # This is where the meta tensor error is likely happening during the .to(device) call\n",
        "    print(f\"TEST: Classification model device BEFORE .to(device): {next(classification_model.parameters()).device}\")\n",
        "    classification_model.to(device) # This line is the likely source of the meta tensor error\n",
        "    print(f\"TEST: Classification model moved to device AFTER .to(device): {next(classification_model.parameters()).device}\")\n",
        "\n",
        "\n",
        "    # --- Create a Dummy Input Batch (Batch Size 1) ---\n",
        "    print(\"\\n--- Creating Dummy Input Batch ---\")\n",
        "    dummy_text = \"This is a test sentence.\"\n",
        "    # Tokenize the dummy text\n",
        "    dummy_encoding = tokenizer(\n",
        "        dummy_text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512, # Use the same max_length as the Dataset\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    # Mimic DataLoader output format with batch size 1\n",
        "    # Ensure dummy data is created on CPU first if model loading fails to avoid cuda error\n",
        "    dummy_input_ids = dummy_encoding[\"input_ids\"] # Start on CPU\n",
        "    dummy_attention_mask = dummy_encoding[\"attention_mask\"] # Start on CPU\n",
        "    dummy_labels = torch.tensor([0], dtype=torch.long) # Start on CPU\n",
        "\n",
        "    # The .to(device) call for dummy data should happen just before passing to the model's forward pass\n",
        "    # Inside the forward method of GemmaForSequenceClassification, we added device handling for inputs.\n",
        "\n",
        "    print(f\"TEST: Dummy input_ids shape (CPU): {dummy_input_ids.shape}\")\n",
        "    print(f\"TEST: Dummy attention_mask shape (CPU): {dummy_attention_mask.shape}\")\n",
        "    print(f\"TEST: Dummy labels shape (CPU): {dummy_labels.shape}\")\n",
        "\n",
        "\n",
        "    # --- Pass Dummy Batch Through the Model's Forward Method ---\n",
        "    print(\"\\n--- Running Forward Pass with Dummy Data ---\")\n",
        "    # The forward method now handles moving inputs to the correct device\n",
        "    try:\n",
        "        # Call the forward method with the dummy batch\n",
        "        # Pass all relevant inputs\n",
        "        output_dict = classification_model(\n",
        "            input_ids=dummy_input_ids,\n",
        "            attention_mask=dummy_attention_mask,\n",
        "            labels=dummy_labels # Include labels to test loss calculation path\n",
        "        )\n",
        "\n",
        "        print(\"\\n--- Forward Pass Output ---\")\n",
        "        print(f\"TEST: Output dictionary keys: {output_dict.keys()}\")\n",
        "        if 'logits' in output_dict:\n",
        "             print(f\"TEST: Logits shape: {output_dict['logits'].shape}, device: {output_dict['logits'].device}\")\n",
        "        if 'loss' in output_dict:\n",
        "             print(f\"TEST: Loss value: {output_dict['loss'].item()}, device: {output_dict['loss'].device}\")\n",
        "\n",
        "        print(\"\\n--- GemmaForSequenceClassification Test Complete ---\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"TEST: âŒ Error during forward pass test: {e}\", file=sys.stderr)\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"TEST: âŒ Error during model/PEFT loading or setup: {e}\", file=sys.stderr)\n",
        "\n",
        "print(\"--- End of GemmaForSequenceClassification Test Cell ---\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
